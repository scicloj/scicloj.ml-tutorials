<!DOCTYPE html>
<html lang="en-US">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link href="https://stackpath.bootstrapcdn.com/bootswatch/4.5.0/sandstone/bootstrap.min.css" rel="stylesheet" type="text/css">
        <link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.6.0/styles/solarized-light.min.css" rel="stylesheet" type="text/css">
        <link href="https://cdnjs.cloudflare.com/ajax/libs/ag-grid/24.0.0/styles/ag-grid.min.css" rel="stylesheet" type="text/css">
        <link href="https://cdnjs.cloudflare.com/ajax/libs/ag-grid/24.0.0/styles/ag-theme-balham.min.css" rel="stylesheet" type="text/css">
        <link href="https://unpkg.com/leaflet@1.6.0/dist/leaflet.css" rel="stylesheet" type="text/css">

        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
        <!-- The loading of KaTeX is deferred to speed up page rendering -->
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
    </head>
    <body>
        <p id="loading">Loading ...</p>
        <div id="app"></div>
    </body>
    <script id="state" type="text">"{:options {:reverse-notes? false, :header? false, :notes-in-cards? false, :initially-collapse? false, :auto-scroll? false, :port 5678, :custom-header [:div {:style {:font-style \"italic\", :font-family \"\\\"Lucida Console\\\", Courier, monospace\"}} \"(notespace)\" [:p \"Sun Dec 18 17:58:02 CET 2022\"] nil [:hr]], :custom-footer [:div [:hr] [:hr]]}, :ids [\"1483\" \"1485\" \"1487\" \"1489\" \"1491\" \"1493\" \"1495\" \"1497\" \"1499\" \"1501\" \"1503\" \"1505\" \"1507\" \"1509\" \"1511\" \"1513\" \"1515\" \"1517\" \"1519\" \"1521\" \"1523\" \"1525\" \"1527\" \"1529\" \"1531\"], :id->content {\"1483\" [:div [:p] [:div [:p/code {:code \"(comment\\n  (note/init-with-browser)\\n  (note/eval-this-notespace)\\n  (note/reread-this-notespace)\\n  (note/render-static-html \\\"docs/userguide-sklearnclj.html\\\")\\n  (note/init))\", :bg-class \"bg-light\"}]] nil [:p/code {:code \"nil\\n\"}]], \"1505\" [:div [:p] nil nil [:p/markdown \"Predict on new data\"]], \"1501\" [:div [:p] nil nil [:p/markdown \"Train model\"]], \"1519\" [:div [:p] nil nil [:p/markdown \"# Models\"]], \"1491\" [:div [:p] [:div [:p/code {:code \"(require '[scicloj.ml.core :as ml]\\n         '[scicloj.ml.metamorph :as mm]\\n         '[scicloj.ml.dataset :as ds]\\n         '[tech.v3.dataset.tensor :as dst]\\n         '[scicloj.sklearn-clj :as sklearn-clj]\\n         '[scicloj.sklearn-clj.ml]\\n         '[scicloj.metamorph.ml.toydata :as toydata]\\n         '[libpython-clj2.python :refer [py.-] :as py])\", :bg-class \"bg-light\"}]] nil nil], \"1497\" [:div [:p] nil nil [:p/markdown \"Make pipe with sklearn model 'logistic-regression'\"]], \"1507\" [:div [:p] [:div [:p/code {:code \"(->\\n (ml/transform-pipe\\n  (dst/tensor->dataset [[3 4 5]])\\n  pipe\\n  fitted-ctx)\\n :metamorph/data)\", :bg-class \"bg-light\"}]] nil [:p/code {:code \":_unnamed [1 3]:\\n\\n| 0 | 1 |   2 |\\n|--:|--:|----:|\\n| 3 | 4 | 2.0 |\\n\\n\"}]], \"1499\" [:div [:p] [:div [:p/code {:code \"(def pipe\\n  (ml/pipeline\\n   (mm/set-inference-target 2)\\n   {:metamorph/id :model}\\n   (mm/model {:model-type :sklearn.classification/logistic-regression\\n              :max-iter 100})))\", :bg-class \"bg-light\"}]] nil nil], \"1515\" [:div [:p] [:div [:p/code {:code \"(def model-attributes\\n  (-> fitted-ctx :model :model-data :attributes))\", :bg-class \"bg-light\"}]] nil nil], \"1511\" [:div [:p] [:div [:p/code {:code \"(-> fitted-ctx :model :model-data :model\\n    (py.- coef_)\\n    (py/->jvm))\", :bg-class \"bg-light\"}]] nil [:p/code {:code \"#tech.v3.tensor<float64>[3 2]\\n[[   -0.4807    -0.4807]\\n [-2.519E-06 -2.519E-06]\\n [    0.4807     0.4807]]\\n\"}]], \"1487\" [:div [:p] nil nil [:p/markdown \"The [scicloj.ml](https://github.com/scicloj/scicloj.ml) plugin [sklearn-clj](https://github.com/scicloj/sklearn-clj)\\n gives easy access to all models from [scikit-learn](https://scikit-learn.org/stable/)\"]], \"1521\" [:div [:p] nil nil [:p/markdown \"Below all models are listed with their parameters and the original documentation.\\n\\nThe parameters are given as Clojure keys in kebap-case. As the document texts are imported from python\\nthey refer to the python spelling of the parameter. But the translation between the two should be obvious.\"]], \"1489\" [:div [:p] nil nil [:p/markdown \"After [libpython.clj](https://github.com/clj-python/libpython-clj)\\n has been setup with the python package sklearn installed,\\nthe following lines show how to use any sklearn model in a usual `scicloj.ml` pipeline:\"]], \"1495\" [:div [:p] [:div [:p/code {:code \"(def ds (dst/tensor->dataset [[0 0 0 ] [1 1 1 ] [2 2 2]]))\", :bg-class \"bg-light\"}]] nil nil], \"1531\" [:div [:p] nil nil ([:div [:h3 {:id \":sklearn.regression/ada-boost-regressor\"} \":sklearn.regression/ada-boost-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [5 2]:\\n\\n|           :name | :default |\\n|-----------------|----------|\\n| :base-estimator |          |\\n|  :learning-rate |    1.000 |\\n|           :loss |   linear |\\n|   :n-estimators |       50 |\\n|   :random-state |          |\\n\"]]] [:span [:p/markdown \"An AdaBoost regressor.\\n\\n    An AdaBoost [1] regressor is a meta-estimator that begins by fitting a\\n    regressor on the original dataset and then fits additional copies of the\\n    regressor on the same dataset but where the weights of instances are\\n    adjusted according to the error of the current prediction. As such,\\n    subsequent regressors focus more on difficult cases.\\n\\n    This class implements the algorithm known as AdaBoost.R2 [2].\\n\\n    Read more in the User Guide: `adaboost`.\\n\\n    *Added in 0.14*\\n\\n    Parameters\\n    ----------\\n    - `base_estimator`: object, default=None\\n        The base estimator from which the boosted ensemble is built.\\n        If ``None``, then the base estimator is\\n        `~sklearn.tree.DecisionTreeRegressor` initialized with\\n        `max_depth=3`.\\n\\n    - `n_estimators`: int, default=50\\n        The maximum number of estimators at which boosting is terminated.\\n        In case of perfect fit, the learning procedure is stopped early.\\n        Values must be in the range `[1, inf)`.\\n\\n    - `learning_rate`: float, default=1.0\\n        Weight applied to each regressor at each boosting iteration. A higher\\n        learning rate increases the contribution of each regressor. There is\\n        a trade-off between the `learning_rate` and `n_estimators` parameters.\\n        Values must be in the range `(0.0, inf)`.\\n\\n    - `loss`: {'linear', 'square', 'exponential'}, default='linear'\\n        The loss function to use when updating the weights after each\\n        boosting iteration.\\n\\n    - `random_state`: int, RandomState instance or None, default=None\\n        Controls the random seed given at each `base_estimator` at each\\n        boosting iteration.\\n        Thus, it is only used when `base_estimator` exposes a `random_state`.\\n        In addition, it controls the bootstrap of the weights used to train the\\n        `base_estimator` at each boosting iteration.\\n        Pass an int for reproducible output across multiple function calls.\\n        See `Glossary <random_state>`.\\n\\n    Attributes\\n    ----------\\n    - `base_estimator_`: estimator\\n        The base estimator from which the ensemble is grown.\\n\\n    - `estimators_`: list of regressors\\n        The collection of fitted sub-estimators.\\n\\n    - `estimator_weights_`: ndarray of floats\\n        Weights for each estimator in the boosted ensemble.\\n\\n    - `estimator_errors_`: ndarray of floats\\n        Regression error for each estimator in the boosted ensemble.\\n\\n    - `feature_importances_`: ndarray of shape (n_features,)\\n        The impurity-based feature importances if supported by the\\n        ``base_estimator`` (when based on decision trees).\\n\\n        Warning: impurity-based feature importances can be misleading for\\n        high cardinality features (many unique values). See\\n        `sklearn.inspection.permutation_importance` as an alternative.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    See Also\\n    --------\\n    - `AdaBoostClassifier`: An AdaBoost classifier.\\n    - `GradientBoostingRegressor`: Gradient Boosting Classification Tree.\\n    - `sklearn.tree.DecisionTreeRegressor`: A decision tree regressor.\\n\\n    References\\n    ----------\\n - [1] Y. Freund, R. Schapire, \\\"A Decision-Theoretic Generalization of\\n           on-Line Learning and an Application to Boosting\\\", 1995.\\n\\n - [2] H. Drucker, \\\"Improving Regressors using Boosting Techniques\\\", 1997.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.ensemble import AdaBoostRegressor\\n    >>> from sklearn.datasets import make_regression\\n    >>> X, y = make_regression(n_features=4, n_informative=2,\\n    ...                        random_state=0, shuffle=False)\\n    >>> regr = AdaBoostRegressor(random_state=0, n_estimators=100)\\n    >>> regr.fit(X, y)\\n    AdaBoostRegressor(n_estimators=100, random_state=0)\\n    >>> regr.predict([[0, 0, 0, 0]])\\n    array([4.7972...])\\n    >>> regr.score(X, y)\\n    0.9771...\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/ard-regression\"} \":sklearn.regression/ard-regression\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [12 2]:\\n\\n|             :name |   :default |\\n|-------------------|------------|\\n|        :normalize | deprecated |\\n|              :tol |   0.001000 |\\n|          :alpha-2 |  1.000E-06 |\\n| :threshold-lambda |  1.000E+04 |\\n|         :lambda-1 |  1.000E-06 |\\n|           :copy-x |       true |\\n|         :lambda-2 |  1.000E-06 |\\n|    :fit-intercept |       true |\\n|          :alpha-1 |  1.000E-06 |\\n|           :n-iter |        300 |\\n|          :verbose |      false |\\n|    :compute-score |      false |\\n\"]]] [:span [:p/markdown \"Bayesian ARD regression.\\n\\n    Fit the weights of a regression model, using an ARD prior. The weights of\\n    the regression model are assumed to be in Gaussian distributions.\\n    Also estimate the parameters lambda (precisions of the distributions of the\\n    weights) and alpha (precision of the distribution of the noise).\\n    The estimation is done by an iterative procedures (Evidence Maximization)\\n\\n    Read more in the User Guide: `bayesian_regression`.\\n\\n    Parameters\\n    ----------\\n    - `n_iter`: int, default=300\\n        Maximum number of iterations.\\n\\n    - `tol`: float, default=1e-3\\n        Stop the algorithm if w has converged.\\n\\n    - `alpha_1`: float, default=1e-6\\n        - `Hyper-parameter`: shape parameter for the Gamma distribution prior\\n        over the alpha parameter.\\n\\n    - `alpha_2`: float, default=1e-6\\n        - `Hyper-parameter`: inverse scale parameter (rate parameter) for the\\n        Gamma distribution prior over the alpha parameter.\\n\\n    - `lambda_1`: float, default=1e-6\\n        - `Hyper-parameter`: shape parameter for the Gamma distribution prior\\n        over the lambda parameter.\\n\\n    - `lambda_2`: float, default=1e-6\\n        - `Hyper-parameter`: inverse scale parameter (rate parameter) for the\\n        Gamma distribution prior over the lambda parameter.\\n\\n    - `compute_score`: bool, default=False\\n        If True, compute the objective function at each step of the model.\\n\\n    - `threshold_lambda`: float, default=10 000\\n        Threshold for removing (pruning) weights with high precision from\\n        the computation.\\n\\n    - `fit_intercept`: bool, default=True\\n        Whether to calculate the intercept for this model. If set\\n        to false, no intercept will be used in calculations\\n        (i.e. data is expected to be centered).\\n\\n    - `normalize`: bool, default=False\\n        This parameter is ignored when ``fit_intercept`` is set to False.\\n        If True, the regressors X will be normalized before regression by\\n        subtracting the mean and dividing by the l2-norm.\\n        If you wish to standardize, please use\\n        `~sklearn.preprocessing.StandardScaler` before calling ``fit``\\n        on an estimator with ``normalize=False``.\\n\\n        *Deprecated since 1.0*\\n            ``normalize`` was deprecated in version 1.0 and will be removed in\\n            1.2.\\n\\n    - `copy_X`: bool, default=True\\n        If True, X will be copied; else, it may be overwritten.\\n\\n    - `verbose`: bool, default=False\\n        Verbose mode when fitting the model.\\n\\n    Attributes\\n    ----------\\n    - `coef_`: array-like of shape (n_features,)\\n        Coefficients of the regression model (mean of distribution)\\n\\n    - `alpha_`: float\\n       estimated precision of the noise.\\n\\n    - `lambda_`: array-like of shape (n_features,)\\n       estimated precisions of the weights.\\n\\n    - `sigma_`: array-like of shape (n_features, n_features)\\n        estimated variance-covariance matrix of the weights\\n\\n    - `scores_`: float\\n        if computed, value of the objective function (to be maximized)\\n\\n    - `intercept_`: float\\n        Independent term in decision function. Set to 0.0 if\\n        ``fit_intercept = False``.\\n\\n    - `X_offset_`: float\\n        If `normalize=True`, offset subtracted for centering data to a\\n        zero mean.\\n\\n    - `X_scale_`: float\\n        If `normalize=True`, parameter used to scale data to a unit\\n        standard deviation.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    See Also\\n    --------\\n    - `BayesianRidge`: Bayesian ridge regression.\\n\\n    Notes\\n    -----\\n    For an example, see examples/linear_model/plot_ard.py: `sphx_glr_auto_examples_linear_model_plot_ard.py`.\\n\\n    References\\n    ----------\\n    D. J. C. MacKay, Bayesian nonlinear modeling for the prediction\\n    competition, ASHRAE Transactions, 1994.\\n\\n    R. Salakhutdinov, Lecture notes on Statistical Machine Learning,\\n    http://www.utstat.toronto.edu/~rsalakhu/sta4273/notes/Lecture2.pdf#page=15\\n    Their beta is our ``self.alpha_``\\n    Their alpha is our ``self.lambda_``\\n    ARD is a little different than the slide: only dimensions/features for\\n    which ``self.lambda_ < self.threshold_lambda`` are kept and the rest are\\n    discarded.\\n\\n    Examples\\n    --------\\n    >>> from sklearn import linear_model\\n    >>> clf = linear_model.ARDRegression()\\n    >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\\n    ARDRegression()\\n    >>> clf.predict([[1, 1]])\\n    array([1.])\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/bagging-regressor\"} \":sklearn.regression/bagging-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [11 2]:\\n\\n|               :name | :default |\\n|---------------------|----------|\\n|          :bootstrap |     true |\\n| :bootstrap-features |    false |\\n|             :n-jobs |          |\\n|       :random-state |          |\\n|          :oob-score |    false |\\n|     :base-estimator |          |\\n|       :max-features |    1.000 |\\n|         :warm-start |    false |\\n|       :n-estimators |       10 |\\n|        :max-samples |    1.000 |\\n|            :verbose |        0 |\\n\"]]] [:span [:p/markdown \"A Bagging regressor.\\n\\n    A Bagging regressor is an ensemble meta-estimator that fits base\\n    regressors each on random subsets of the original dataset and then\\n    aggregate their individual predictions (either by voting or by averaging)\\n    to form a final prediction. Such a meta-estimator can typically be used as\\n    a way to reduce the variance of a black-box estimator (e.g., a decision\\n    tree), by introducing randomization into its construction procedure and\\n    then making an ensemble out of it.\\n\\n    This algorithm encompasses several works from the literature. When random\\n    subsets of the dataset are drawn as random subsets of the samples, then\\n    this algorithm is known as Pasting [1]_. If samples are drawn with\\n    replacement, then the method is known as Bagging [2]_. When random subsets\\n    of the dataset are drawn as random subsets of the features, then the method\\n    is known as Random Subspaces [3]_. Finally, when base estimators are built\\n    on subsets of both samples and features, then the method is known as\\n    Random Patches [4]_.\\n\\n    Read more in the User Guide: `bagging`.\\n\\n    *Added in 0.15*\\n\\n    Parameters\\n    ----------\\n    - `base_estimator`: object, default=None\\n        The base estimator to fit on random subsets of the dataset.\\n        If None, then the base estimator is a\\n        `~sklearn.tree.DecisionTreeRegressor`.\\n\\n    - `n_estimators`: int, default=10\\n        The number of base estimators in the ensemble.\\n\\n    - `max_samples`: int or float, default=1.0\\n        The number of samples to draw from X to train each base estimator (with\\n        replacement by default, see `bootstrap` for more details).\\n\\n        - If int, then draw `max_samples` samples.\\n        - If float, then draw `max_samples * X.shape[0]` samples.\\n\\n    - `max_features`: int or float, default=1.0\\n        The number of features to draw from X to train each base estimator (\\n        without replacement by default, see `bootstrap_features` for more\\n        details).\\n\\n        - If int, then draw `max_features` features.\\n        - If float, then draw `max_features * X.shape[1]` features.\\n\\n    - `bootstrap`: bool, default=True\\n        Whether samples are drawn with replacement. If False, sampling\\n        without replacement is performed.\\n\\n    - `bootstrap_features`: bool, default=False\\n        Whether features are drawn with replacement.\\n\\n    - `oob_score`: bool, default=False\\n        Whether to use out-of-bag samples to estimate\\n        the generalization error. Only available if bootstrap=True.\\n\\n    - `warm_start`: bool, default=False\\n        When set to True, reuse the solution of the previous call to fit\\n        and add more estimators to the ensemble, otherwise, just fit\\n        a whole new ensemble. See `the Glossary <warm_start>`.\\n\\n    - `n_jobs`: int, default=None\\n        The number of jobs to run in parallel for both `fit` and\\n        `predict`. ``None`` means 1 unless in a\\n        `joblib.parallel_backend` context. ``-1`` means using all\\n        processors. See `Glossary <n_jobs>` for more details.\\n\\n    - `random_state`: int, RandomState instance or None, default=None\\n        Controls the random resampling of the original dataset\\n        (sample wise and feature wise).\\n        If the base estimator accepts a `random_state` attribute, a different\\n        seed is generated for each instance in the ensemble.\\n        Pass an int for reproducible output across multiple function calls.\\n        See `Glossary <random_state>`.\\n\\n    - `verbose`: int, default=0\\n        Controls the verbosity when fitting and predicting.\\n\\n    Attributes\\n    ----------\\n    - `base_estimator_`: estimator\\n        The base estimator from which the ensemble is grown.\\n\\n    - `n_features_`: int\\n        The number of features when `fit` is performed.\\n\\n        *Deprecated since 1.0*\\n            Attribute `n_features_` was deprecated in version 1.0 and will be\\n            removed in 1.2. Use `n_features_in_` instead.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    - `estimators_`: list of estimators\\n        The collection of fitted sub-estimators.\\n\\n    - `estimators_samples_`: list of arrays\\n        The subset of drawn samples (i.e., the in-bag samples) for each base\\n        estimator. Each subset is defined by an array of the indices selected.\\n\\n    - `estimators_features_`: list of arrays\\n        The subset of drawn features for each base estimator.\\n\\n    - `oob_score_`: float\\n        Score of the training dataset obtained using an out-of-bag estimate.\\n        This attribute exists only when ``oob_score`` is True.\\n\\n    - `oob_prediction_`: ndarray of shape (n_samples,)\\n        Prediction computed with out-of-bag estimate on the training\\n        set. If n_estimators is small it might be possible that a data point\\n        was never left out during the bootstrap. In this case,\\n        `oob_prediction_` might contain NaN. This attribute exists only\\n        when ``oob_score`` is True.\\n\\n    See Also\\n    --------\\n    - `BaggingClassifier`: A Bagging classifier.\\n\\n    References\\n    ----------\\n\\n - [1] L. Breiman, \\\"Pasting small votes for classification in large\\n           databases and on-line\\\", Machine Learning, 36(1), 85-103, 1999.\\n\\n - [2] L. Breiman, \\\"Bagging predictors\\\", Machine Learning, 24(2), 123-140,\\n           1996.\\n\\n - [3] T. Ho, \\\"The random subspace method for constructing decision\\n           forests\\\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\\n           1998.\\n\\n - [4] G. Louppe and P. Geurts, \\\"Ensembles on Random Patches\\\", Machine\\n           Learning and Knowledge Discovery in Databases, 346-361, 2012.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.svm import SVR\\n    >>> from sklearn.ensemble import BaggingRegressor\\n    >>> from sklearn.datasets import make_regression\\n    >>> X, y = make_regression(n_samples=100, n_features=4,\\n    ...                        n_informative=2, n_targets=1,\\n    ...                        random_state=0, shuffle=False)\\n    >>> regr = BaggingRegressor(base_estimator=SVR(),\\n    ...                         n_estimators=10, random_state=0).fit(X, y)\\n    >>> regr.predict([[0, 0, 0, 0]])\\n    array([-2.8720...])\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/bayesian-ridge\"} \":sklearn.regression/bayesian-ridge\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [13 2]:\\n\\n|          :name |   :default |\\n|----------------|------------|\\n|     :normalize | deprecated |\\n|           :tol |   0.001000 |\\n|       :alpha-2 |  1.000E-06 |\\n|      :lambda-1 |  1.000E-06 |\\n|        :copy-x |       true |\\n|      :lambda-2 |  1.000E-06 |\\n|    :alpha-init |            |\\n| :fit-intercept |       true |\\n|       :alpha-1 |  1.000E-06 |\\n|   :lambda-init |            |\\n|        :n-iter |        300 |\\n|       :verbose |      false |\\n| :compute-score |      false |\\n\"]]] [:span [:p/markdown \"Bayesian ridge regression.\\n\\n    Fit a Bayesian ridge model. See the Notes section for details on this\\n    implementation and the optimization of the regularization parameters\\n    lambda (precision of the weights) and alpha (precision of the noise).\\n\\n    Read more in the User Guide: `bayesian_regression`.\\n\\n    Parameters\\n    ----------\\n    - `n_iter`: int, default=300\\n        Maximum number of iterations. Should be greater than or equal to 1.\\n\\n    - `tol`: float, default=1e-3\\n        Stop the algorithm if w has converged.\\n\\n    - `alpha_1`: float, default=1e-6\\n        - `Hyper-parameter`: shape parameter for the Gamma distribution prior\\n        over the alpha parameter.\\n\\n    - `alpha_2`: float, default=1e-6\\n        - `Hyper-parameter`: inverse scale parameter (rate parameter) for the\\n        Gamma distribution prior over the alpha parameter.\\n\\n    - `lambda_1`: float, default=1e-6\\n        - `Hyper-parameter`: shape parameter for the Gamma distribution prior\\n        over the lambda parameter.\\n\\n    - `lambda_2`: float, default=1e-6\\n        - `Hyper-parameter`: inverse scale parameter (rate parameter) for the\\n        Gamma distribution prior over the lambda parameter.\\n\\n    - `alpha_init`: float, default=None\\n        Initial value for alpha (precision of the noise).\\n        If not set, alpha_init is 1/Var(y).\\n\\n            *Added in 0.22*\\n\\n    - `lambda_init`: float, default=None\\n        Initial value for lambda (precision of the weights).\\n        If not set, lambda_init is 1.\\n\\n            *Added in 0.22*\\n\\n    - `compute_score`: bool, default=False\\n        If True, compute the log marginal likelihood at each iteration of the\\n        optimization.\\n\\n    - `fit_intercept`: bool, default=True\\n        Whether to calculate the intercept for this model.\\n        The intercept is not treated as a probabilistic parameter\\n        and thus has no associated variance. If set\\n        to False, no intercept will be used in calculations\\n        (i.e. data is expected to be centered).\\n\\n    - `normalize`: bool, default=False\\n        This parameter is ignored when ``fit_intercept`` is set to False.\\n        If True, the regressors X will be normalized before regression by\\n        subtracting the mean and dividing by the l2-norm.\\n        If you wish to standardize, please use\\n        `~sklearn.preprocessing.StandardScaler` before calling ``fit``\\n        on an estimator with ``normalize=False``.\\n\\n        *Deprecated since 1.0*\\n            ``normalize`` was deprecated in version 1.0 and will be removed in\\n            1.2.\\n\\n    - `copy_X`: bool, default=True\\n        If True, X will be copied; else, it may be overwritten.\\n\\n    - `verbose`: bool, default=False\\n        Verbose mode when fitting the model.\\n\\n    Attributes\\n    ----------\\n    - `coef_`: array-like of shape (n_features,)\\n        Coefficients of the regression model (mean of distribution)\\n\\n    - `intercept_`: float\\n        Independent term in decision function. Set to 0.0 if\\n        ``fit_intercept = False``.\\n\\n    - `alpha_`: float\\n       Estimated precision of the noise.\\n\\n    - `lambda_`: float\\n       Estimated precision of the weights.\\n\\n    - `sigma_`: array-like of shape (n_features, n_features)\\n        Estimated variance-covariance matrix of the weights\\n\\n    - `scores_`: array-like of shape (n_iter_+1,)\\n        If computed_score is True, value of the log marginal likelihood (to be\\n        maximized) at each iteration of the optimization. The array starts\\n        with the value of the log marginal likelihood obtained for the initial\\n        values of alpha and lambda and ends with the value obtained for the\\n        estimated alpha and lambda.\\n\\n    - `n_iter_`: int\\n        The actual number of iterations to reach the stopping criterion.\\n\\n    - `X_offset_`: float\\n        If `normalize=True`, offset subtracted for centering data to a\\n        zero mean.\\n\\n    - `X_scale_`: float\\n        If `normalize=True`, parameter used to scale data to a unit\\n        standard deviation.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    See Also\\n    --------\\n    - `ARDRegression`: Bayesian ARD regression.\\n\\n    Notes\\n    -----\\n    There exist several strategies to perform Bayesian ridge regression. This\\n    implementation is based on the algorithm described in Appendix A of\\n    (Tipping, 2001) where updates of the regularization parameters are done as\\n    suggested in (MacKay, 1992). Note that according to A New\\n    View of Automatic Relevance Determination (Wipf and Nagarajan, 2008) these\\n    update rules do not guarantee that the marginal likelihood is increasing\\n    between two consecutive iterations of the optimization.\\n\\n    References\\n    ----------\\n    D. J. C. MacKay, Bayesian Interpolation, Computation and Neural Systems,\\n    Vol. 4, No. 3, 1992.\\n\\n    M. E. Tipping, Sparse Bayesian Learning and the Relevance Vector Machine,\\n    Journal of Machine Learning Research, Vol. 1, 2001.\\n\\n    Examples\\n    --------\\n    >>> from sklearn import linear_model\\n    >>> clf = linear_model.BayesianRidge()\\n    >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\\n    BayesianRidge()\\n    >>> clf.predict([[1, 1]])\\n    array([1.])\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/cca\"} \":sklearn.regression/cca\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [5 2]:\\n\\n|         :name |  :default |\\n|---------------|-----------|\\n|         :copy |      true |\\n|     :max-iter |       500 |\\n| :n-components |         2 |\\n|        :scale |      true |\\n|          :tol | 1.000E-06 |\\n\"]]] [:span [:p/markdown \"Canonical Correlation Analysis, also known as \\\"Mode B\\\" PLS.\\n\\n    Read more in the User Guide: `cross_decomposition`.\\n\\n    Parameters\\n    ----------\\n    - `n_components`: int, default=2\\n        Number of components to keep. Should be in `[1, min(n_samples,\\n        n_features, n_targets)]`.\\n\\n    - `scale`: bool, default=True\\n        Whether to scale `X` and `Y`.\\n\\n    - `max_iter`: int, default=500\\n        The maximum number of iterations of the power method.\\n\\n    - `tol`: float, default=1e-06\\n        The tolerance used as convergence criteria in the power method: the\\n        algorithm stops whenever the squared norm of `u_i - u_{i-1}` is less\\n        than `tol`, where `u` corresponds to the left singular vector.\\n\\n    - `copy`: bool, default=True\\n        Whether to copy `X` and `Y` in fit before applying centering, and\\n        potentially scaling. If False, these operations will be done inplace,\\n        modifying both arrays.\\n\\n    Attributes\\n    ----------\\n    - `x_weights_`: ndarray of shape (n_features, n_components)\\n        The left singular vectors of the cross-covariance matrices of each\\n        iteration.\\n\\n    - `y_weights_`: ndarray of shape (n_targets, n_components)\\n        The right singular vectors of the cross-covariance matrices of each\\n        iteration.\\n\\n    - `x_loadings_`: ndarray of shape (n_features, n_components)\\n        The loadings of `X`.\\n\\n    - `y_loadings_`: ndarray of shape (n_targets, n_components)\\n        The loadings of `Y`.\\n\\n    - `x_rotations_`: ndarray of shape (n_features, n_components)\\n        The projection matrix used to transform `X`.\\n\\n    - `y_rotations_`: ndarray of shape (n_features, n_components)\\n        The projection matrix used to transform `Y`.\\n\\n    - `coef_`: ndarray of shape (n_features, n_targets)\\n        The coefficients of the linear model such that `Y` is approximated as\\n        `Y = X @ coef_ + intercept_`.\\n\\n    - `intercept_`: ndarray of shape (n_targets,)\\n        The intercepts of the linear model such that `Y` is approximated as\\n        `Y = X @ coef_ + intercept_`.\\n\\n        *Added in 1.1*\\n\\n    - `n_iter_`: list of shape (n_components,)\\n        Number of iterations of the power method, for each\\n        component.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    See Also\\n    --------\\n    - `PLSCanonical`: Partial Least Squares transformer and regressor.\\n    - `PLSSVD`: Partial Least Square SVD.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.cross_decomposition import CCA\\n    >>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [3.,5.,4.]]\\n    >>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\\n    >>> cca = CCA(n_components=1)\\n    >>> cca.fit(X, Y)\\n    CCA(n_components=1)\\n    >>> X_c, Y_c = cca.transform(X, Y)\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/decision-tree-regressor\"} \":sklearn.regression/decision-tree-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [11 2]:\\n\\n|                     :name |      :default |\\n|---------------------------|---------------|\\n| :min-weight-fraction-leaf |         0.000 |\\n|           :max-leaf-nodes |               |\\n|    :min-impurity-decrease |         0.000 |\\n|        :min-samples-split |         2.000 |\\n|                :ccp-alpha |         0.000 |\\n|                 :splitter |          best |\\n|             :random-state |               |\\n|         :min-samples-leaf |             1 |\\n|             :max-features |               |\\n|                :max-depth |               |\\n|                :criterion | squared_error |\\n\"]]] [:span [:p/markdown \"A decision tree regressor.\\n\\n    Read more in the User Guide: `tree`.\\n\\n    Parameters\\n    ----------\\n    - `criterion`: {\\\"squared_error\\\", \\\"friedman_mse\\\", \\\"absolute_error\\\",             \\\"poisson\\\"}, default=\\\"squared_error\\\"\\n        The function to measure the quality of a split. Supported criteria\\n        are \\\"squared_error\\\" for the mean squared error, which is equal to\\n        variance reduction as feature selection criterion and minimizes the L2\\n        loss using the mean of each terminal node, \\\"friedman_mse\\\", which uses\\n        mean squared error with Friedman's improvement score for potential\\n        splits, \\\"absolute_error\\\" for the mean absolute error, which minimizes\\n        the L1 loss using the median of each terminal node, and \\\"poisson\\\" which\\n        uses reduction in Poisson deviance to find splits.\\n\\n        *Added in 0.18*\\n           Mean Absolute Error (MAE) criterion.\\n\\n        *Added in 0.24*\\n            Poisson deviance criterion.\\n\\n        *Deprecated since 1.0*\\n            Criterion \\\"mse\\\" was deprecated in v1.0 and will be removed in\\n            version 1.2. Use `criterion=\\\"squared_error\\\"` which is equivalent.\\n\\n        *Deprecated since 1.0*\\n            Criterion \\\"mae\\\" was deprecated in v1.0 and will be removed in\\n            version 1.2. Use `criterion=\\\"absolute_error\\\"` which is equivalent.\\n\\n    - `splitter`: {\\\"best\\\", \\\"random\\\"}, default=\\\"best\\\"\\n        The strategy used to choose the split at each node. Supported\\n        strategies are \\\"best\\\" to choose the best split and \\\"random\\\" to choose\\n        the best random split.\\n\\n    - `max_depth`: int, default=None\\n        The maximum depth of the tree. If None, then nodes are expanded until\\n        all leaves are pure or until all leaves contain less than\\n        min_samples_split samples.\\n\\n    - `min_samples_split`: int or float, default=2\\n        The minimum number of samples required to split an internal node:\\n\\n        - If int, then consider `min_samples_split` as the minimum number.\\n        - If float, then `min_samples_split` is a fraction and\\n          `ceil(min_samples_split * n_samples)` are the minimum\\n          number of samples for each split.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n    - `min_samples_leaf`: int or float, default=1\\n        The minimum number of samples required to be at a leaf node.\\n        A split point at any depth will only be considered if it leaves at\\n        least ``min_samples_leaf`` training samples in each of the left and\\n        right branches.  This may have the effect of smoothing the model,\\n        especially in regression.\\n\\n        - If int, then consider `min_samples_leaf` as the minimum number.\\n        - If float, then `min_samples_leaf` is a fraction and\\n          `ceil(min_samples_leaf * n_samples)` are the minimum\\n          number of samples for each node.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n    - `min_weight_fraction_leaf`: float, default=0.0\\n        The minimum weighted fraction of the sum total of weights (of all\\n        the input samples) required to be at a leaf node. Samples have\\n        equal weight when sample_weight is not provided.\\n\\n    - `max_features`: int, float or {\\\"auto\\\", \\\"sqrt\\\", \\\"log2\\\"}, default=None\\n        The number of features to consider when looking for the best split:\\n\\n        - If int, then consider `max_features` features at each split.\\n        - If float, then `max_features` is a fraction and\\n          `int(max_features * n_features)` features are considered at each\\n          split.\\n        - If \\\"auto\\\", then `max_features=n_features`.\\n        - If \\\"sqrt\\\", then `max_features=sqrt(n_features)`.\\n        - If \\\"log2\\\", then `max_features=log2(n_features)`.\\n        - If None, then `max_features=n_features`.\\n\\n        *Deprecated since 1.1*\\n            The `\\\"auto\\\"` option was deprecated in 1.1 and will be removed\\n            in 1.3.\\n\\n        Note: the search for a split does not stop until at least one\\n        valid partition of the node samples is found, even if it requires to\\n        effectively inspect more than ``max_features`` features.\\n\\n    - `random_state`: int, RandomState instance or None, default=None\\n        Controls the randomness of the estimator. The features are always\\n        randomly permuted at each split, even if ``splitter`` is set to\\n        ``\\\"best\\\"``. When ``max_features < n_features``, the algorithm will\\n        select ``max_features`` at random at each split before finding the best\\n        split among them. But the best found split may vary across different\\n        runs, even if ``max_features=n_features``. That is the case, if the\\n        improvement of the criterion is identical for several splits and one\\n        split has to be selected at random. To obtain a deterministic behaviour\\n        during fitting, ``random_state`` has to be fixed to an integer.\\n        See `Glossary <random_state>` for details.\\n\\n    - `max_leaf_nodes`: int, default=None\\n        Grow a tree with ``max_leaf_nodes`` in best-first fashion.\\n        Best nodes are defined as relative reduction in impurity.\\n        If None then unlimited number of leaf nodes.\\n\\n    - `min_impurity_decrease`: float, default=0.0\\n        A node will be split if this split induces a decrease of the impurity\\n        greater than or equal to this value.\\n\\n        The weighted impurity decrease equation is the following\\n\\n```python\\nN_t / N * (impurity - N_t_R / N_t * right_impurity\\n                    - N_t_L / N_t * left_impurity)\\n\\ne ``N`` is the total number of samples, ``N_t`` is the number of\\nles at the current node, ``N_t_L`` is the number of samples in the\\n child, and ``N_t_R`` is the number of samples in the right child.\\n\\n`, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\\n`sample_weight`` is passed.\\n\\nersionadded:: 0.19\\n\\na : non-negative float, default=0.0\\nlexity parameter used for Minimal Cost-Complexity Pruning. The\\nree with the largest cost complexity that is smaller than\\np_alpha`` will be chosen. By default, no pruning is performed. See\\n:`minimal_cost_complexity_pruning` for details.\\n\\nersionadded:: 0.22\\n\\nes\\n--\\nimportances_ : ndarray of shape (n_features,)\\nfeature importances.\\nhigher, the more important the feature.\\nimportance of a feature is computed as the\\nmalized) total reduction of the criterion brought\\nhat feature. It is also known as the Gini importance [4]_.\\n\\ning: impurity-based feature importances can be misleading for\\n cardinality features (many unique values). See\\nc:`sklearn.inspection.permutation_importance` as an alternative.\\n\\nures_ : int\\ninferred value of max_features.\\n\\nes_ : int\\nnumber of features when ``fit`` is performed.\\n\\neprecated:: 1.0\\nn_features_` is deprecated in 1.0 and will be removed in\\n.2. Use `n_features_in_` instead.\\n\\nes_in_ : int\\ner of features seen during :term:`fit`.\\n\\nersionadded:: 0.24\\n\\nnames_in_ : ndarray of shape (`n_features_in_`,)\\ns of features seen during :term:`fit`. Defined only when `X`\\nfeature names that are all strings.\\n\\nersionadded:: 1.0\\n\\ns_ : int\\nnumber of outputs when ``fit`` is performed.\\n\\nTree instance\\nunderlying Tree object. Please refer to\\nlp(sklearn.tree._tree.Tree)`` for attributes of Tree object and\\n:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\\nbasic usage of these attributes.\\n\\n\\n\\nTreeClassifier : A decision tree classifier.\\n\\n\\n\\nult values for the parameters controlling the size of the trees\\nmax_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\\n trees which can potentially be very large on some data sets. To\\nemory consumption, the complexity and size of the trees should be\\ned by setting those parameter values.\\n\\nes\\n--\\n\\nttps://en.wikipedia.org/wiki/Decision_tree_learning\\n\\n. Breiman, J. Friedman, R. Olshen, and C. Stone, \\\"Classification\\nnd Regression Trees\\\", Wadsworth, Belmont, CA, 1984.\\n\\n. Hastie, R. Tibshirani and J. Friedman. \\\"Elements of Statistical\\nearning\\\", Springer, 2009.\\n\\n. Breiman, and A. Cutler, \\\"Random Forests\\\",\\nttps://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\\n\\n\\n\\n sklearn.datasets import load_diabetes\\n sklearn.model_selection import cross_val_score\\n sklearn.tree import DecisionTreeRegressor\\n = load_diabetes(return_X_y=True)\\nessor = DecisionTreeRegressor(random_state=0)\\ns_val_score(regressor, X, y, cv=10)\\n               # doctest: +SKIP\\n\\n0.39..., -0.46...,  0.02...,  0.06..., -0.50...,\\n.16...,  0.11..., -0.73..., -0.30..., -0.00...])\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/dummy-regressor\"} \":sklearn.regression/dummy-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [3 2]:\\n\\n|     :name | :default |\\n|-----------|----------|\\n| :constant |          |\\n| :quantile |          |\\n| :strategy |     mean |\\n\"]]] [:span [:p/markdown \"Regressor that makes predictions using simple rules.\\n\\n    This regressor is useful as a simple baseline to compare with other\\n    (real) regressors. Do not use it for real problems.\\n\\n    Read more in the User Guide: `dummy_estimators`.\\n\\n    *Added in 0.13*\\n\\n    Parameters\\n    ----------\\n    - `strategy`: {\\\"mean\\\", \\\"median\\\", \\\"quantile\\\", \\\"constant\\\"}, default=\\\"mean\\\"\\n        Strategy to use to generate predictions.\\n\\n        * \\\"mean\\\": always predicts the mean of the training set\\n        * \\\"median\\\": always predicts the median of the training set\\n        * \\\"quantile\\\": always predicts a specified quantile of the training set,\\n          provided with the quantile parameter.\\n        * \\\"constant\\\": always predicts a constant value that is provided by\\n          the user.\\n\\n    - `constant`: int or float or array-like of shape (n_outputs,), default=None\\n        The explicit constant as predicted by the \\\"constant\\\" strategy. This\\n        parameter is useful only for the \\\"constant\\\" strategy.\\n\\n    - `quantile`: float in [0.0, 1.0], default=None\\n        The quantile to predict using the \\\"quantile\\\" strategy. A quantile of\\n        0.5 corresponds to the median, while 0.0 to the minimum and 1.0 to the\\n        maximum.\\n\\n    Attributes\\n    ----------\\n    - `constant_`: ndarray of shape (1, n_outputs)\\n        Mean or median or quantile of the training targets or constant value\\n        given by the user.\\n\\n    - `n_features_in_`: `None`\\n        Always set to `None`.\\n\\n        *Added in 0.24*\\n        *Deprecated since 1.0*\\n            Will be removed in 1.0\\n\\n    - `n_outputs_`: int\\n        Number of outputs.\\n\\n    See Also\\n    --------\\n    DummyClassifier: Classifier that makes predictions using simple rules.\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> from sklearn.dummy import DummyRegressor\\n    >>> X = np.array([1.0, 2.0, 3.0, 4.0])\\n    >>> y = np.array([2.0, 3.0, 5.0, 10.0])\\n    >>> dummy_regr = DummyRegressor(strategy=\\\"mean\\\")\\n    >>> dummy_regr.fit(X, y)\\n    DummyRegressor()\\n    >>> dummy_regr.predict(X)\\n    array([5., 5., 5., 5.])\\n    >>> dummy_regr.score(X, y)\\n    0.0\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/elastic-net\"} \":sklearn.regression/elastic-net\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [12 2]:\\n\\n|          :name |   :default |\\n|----------------|------------|\\n|     :normalize | deprecated |\\n|      :positive |      false |\\n|           :tol |  0.0001000 |\\n|      :max-iter |       1000 |\\n|  :random-state |            |\\n|        :copy-x |       true |\\n|    :precompute |      false |\\n| :fit-intercept |       true |\\n|         :alpha |      1.000 |\\n|    :warm-start |      false |\\n|     :selection |     cyclic |\\n|     :l-1-ratio |     0.5000 |\\n\"]]] [:span [:p/markdown \"Linear regression with combined L1 and L2 priors as regularizer.\\n\\n    Minimizes the objective function\\n\\n```python\\n1 / (2 * n_samples) * ||y - Xw||^2_2\\n+ alpha * l1_ratio * ||w||_1\\n+ 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\\n\\nre interested in controlling the L1 and L2 penalty\\nly, keep in mind that this is equivalent to::\\n\\na * ||w||_1 + 0.5 * b * ||w||_2^2\\n\\n\\n\\nalpha = a + b and l1_ratio = a / (a + b)\\n\\nmeter l1_ratio corresponds to alpha in the glmnet R package while\\nrresponds to the lambda parameter in glmnet. Specifically, l1_ratio\\nhe lasso penalty. Currently, l1_ratio <= 0.01 is not reliable,\\nou supply your own sequence of alpha.\\n\\ne in the :ref:`User Guide <elastic_net>`.\\n\\nrs\\n--\\nfloat, default=1.0\\ntant that multiplies the penalty terms. Defaults to 1.0.\\nthe notes for the exact mathematical meaning of this\\nmeter. ``alpha = 0`` is equivalent to an ordinary least square,\\ned by the :class:`LinearRegression` object. For numerical\\nons, using ``alpha = 0`` with the ``Lasso`` object is not advised.\\nn this, you should use the :class:`LinearRegression` object.\\n\\n : float, default=0.5\\nElasticNet mixing parameter, with ``0 <= l1_ratio <= 1``. For\\n_ratio = 0`` the penalty is an L2 penalty. ``For l1_ratio = 1`` it\\nn L1 penalty.  For ``0 < l1_ratio < 1``, the penalty is a\\nination of L1 and L2.\\n\\nrcept : bool, default=True\\nher the intercept should be estimated or not. If ``False``, the\\n is assumed to be already centered.\\n\\ne : bool, default=False\\n parameter is ignored when ``fit_intercept`` is set to False.\\nrue, the regressors X will be normalized before regression by\\nracting the mean and dividing by the l2-norm.\\nou wish to standardize, please use\\nss:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\\nn estimator with ``normalize=False``.\\n\\neprecated:: 1.0\\n``normalize`` was deprecated in version 1.0 and will be removed in\\n1.2.\\n\\nte : bool or array-like of shape (n_features, n_features),                 default=False\\nher to use a precomputed Gram matrix to speed up\\nulations. The Gram matrix can also be passed as argument.\\nsparse input this option is always ``False`` to preserve sparsity.\\n\\n : int, default=1000\\nmaximum number of iterations.\\n\\n bool, default=True\\n`True``, X will be copied; else, it may be overwritten.\\n\\noat, default=1e-4\\ntolerance for the optimization: if the updates are\\nler than ``tol``, the optimization code checks the\\n gap for optimality and continues until it is smaller\\n ``tol``, see Notes below.\\n\\nrt : bool, default=False\\n set to ``True``, reuse the solution of the previous call to fit as\\nialization, otherwise, just erase the previous solution.\\n:term:`the Glossary <warm_start>`.\\n\\n : bool, default=False\\n set to ``True``, forces the coefficients to be positive.\\n\\ntate : int, RandomState instance, default=None\\nseed of the pseudo random number generator that selects a random\\nure to update. Used when ``selection`` == 'random'.\\n an int for reproducible output across multiple function calls.\\n:term:`Glossary <random_state>`.\\n\\nn : {'cyclic', 'random'}, default='cyclic'\\net to 'random', a random coefficient is updated every iteration\\ner than looping over features sequentially by default. This\\nting to 'random') often leads to significantly faster convergence\\ncially when tol is higher than 1e-4.\\n\\nes\\n--\\nndarray of shape (n_features,) or (n_targets, n_features)\\nmeter vector (w in the cost function formula).\\n\\noef_ : sparse matrix of shape (n_features,) or             (n_targets, n_features)\\nse representation of the `coef_`.\\n\\nt_ : float or ndarray of shape (n_targets,)\\npendent term in decision function.\\n\\n: list of int\\ner of iterations run by the coordinate descent solver to reach\\nspecified tolerance.\\n\\n_ : float or ndarray of shape (n_targets,)\\nn param alpha, the dual gaps at the end of the optimization,\\n shape as each observation of y.\\n\\nes_in_ : int\\ner of features seen during :term:`fit`.\\n\\nersionadded:: 0.24\\n\\nnames_in_ : ndarray of shape (`n_features_in_`,)\\ns of features seen during :term:`fit`. Defined only when `X`\\nfeature names that are all strings.\\n\\nersionadded:: 1.0\\n\\n\\n\\netCV : Elastic net model with best model selection by\\ns-validation.\\nssor : Implements elastic net regression with incremental training.\\nifier : Implements logistic regression with elastic net penalty\\nGDClassifier(loss=\\\"log_loss\\\", penalty=\\\"elasticnet\\\")``).\\n\\n\\n\\n unnecessary memory duplication the X argument of the fit method\\ne directly passed as a Fortran-contiguous numpy array.\\n\\nise stopping criteria based on `tol` are the following: First, check that\\nimum coordinate update, i.e. :math:`\\\\max_j |w_j^{new} - w_j^{old}|`\\ner than `tol` times the maximum absolute coefficient, :math:`\\\\max_j |w_j|`.\\nhen additionally check whether the dual gap is smaller than `tol` times\\n|y||_2^2 / n_{\\text{samples}}`.\\n\\n\\n\\n sklearn.linear_model import ElasticNet\\n sklearn.datasets import make_regression\\n\\n = make_regression(n_features=2, random_state=0)\\n = ElasticNet(random_state=0)\\n.fit(X, y)\\net(random_state=0)\\nt(regr.coef_)\\n6048 64.55968825]\\nt(regr.intercept_)\\n\\nt(regr.predict([[0, 0]]))\\n.]\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/elastic-net-cv\"} \":sklearn.regression/elastic-net-cv\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [16 2]:\\n\\n|          :name |   :default |\\n|----------------|------------|\\n|     :normalize | deprecated |\\n|      :positive |      false |\\n|           :tol |  0.0001000 |\\n|      :n-alphas |        100 |\\n|           :eps |   0.001000 |\\n|        :alphas |            |\\n|      :max-iter |       1000 |\\n|        :n-jobs |            |\\n|  :random-state |            |\\n|        :copy-x |       true |\\n|    :precompute |       auto |\\n| :fit-intercept |       true |\\n|            :cv |            |\\n|     :selection |     cyclic |\\n|     :l-1-ratio |     0.5000 |\\n|       :verbose |          0 |\\n\"]]] [:span [:p/markdown \"Elastic Net model with iterative fitting along a regularization path.\\n\\n    See glossary entry for `cross-validation estimator`.\\n\\n    Read more in the User Guide: `elastic_net`.\\n\\n    Parameters\\n    ----------\\n    - `l1_ratio`: float or list of float, default=0.5\\n        Float between 0 and 1 passed to ElasticNet (scaling between\\n        l1 and l2 penalties). For ``l1_ratio = 0``\\n        the penalty is an L2 penalty. For ``l1_ratio = 1`` it is an L1 penalty.\\n        For ``0 < l1_ratio < 1``, the penalty is a combination of L1 and L2\\n        This parameter can be a list, in which case the different\\n        values are tested by cross-validation and the one giving the best\\n        prediction score is used. Note that a good choice of list of\\n        values for l1_ratio is often to put more values close to 1\\n        (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7,\\n        .9, .95, .99, 1]``.\\n\\n    - `eps`: float, default=1e-3\\n        Length of the path. ``eps=1e-3`` means that\\n        ``alpha_min / alpha_max = 1e-3``.\\n\\n    - `n_alphas`: int, default=100\\n        Number of alphas along the regularization path, used for each l1_ratio.\\n\\n    - `alphas`: ndarray, default=None\\n        List of alphas where to compute the models.\\n        If None alphas are set automatically.\\n\\n    - `fit_intercept`: bool, default=True\\n        Whether to calculate the intercept for this model. If set\\n        to false, no intercept will be used in calculations\\n        (i.e. data is expected to be centered).\\n\\n    - `normalize`: bool, default=False\\n        This parameter is ignored when ``fit_intercept`` is set to False.\\n        If True, the regressors X will be normalized before regression by\\n        subtracting the mean and dividing by the l2-norm.\\n        If you wish to standardize, please use\\n        `~sklearn.preprocessing.StandardScaler` before calling ``fit``\\n        on an estimator with ``normalize=False``.\\n\\n        *Deprecated since 1.0*\\n            ``normalize`` was deprecated in version 1.0 and will be removed in\\n            1.2.\\n\\n    - `precompute`: 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\\n        Whether to use a precomputed Gram matrix to speed up\\n        calculations. If set to ``'auto'`` let us decide. The Gram\\n        matrix can also be passed as argument.\\n\\n    - `max_iter`: int, default=1000\\n        The maximum number of iterations.\\n\\n    - `tol`: float, default=1e-4\\n        The tolerance for the optimization: if the updates are\\n        smaller than ``tol``, the optimization code checks the\\n        dual gap for optimality and continues until it is smaller\\n        than ``tol``.\\n\\n    - `cv`: int, cross-validation generator or iterable, default=None\\n        Determines the cross-validation splitting strategy.\\n        Possible inputs for cv are:\\n\\n        - None, to use the default 5-fold cross-validation,\\n        - int, to specify the number of folds.\\n        - `CV splitter`,\\n        - An iterable yielding (train, test) splits as arrays of indices.\\n\\n        For int/None inputs, `KFold` is used.\\n\\n        Refer User Guide: `cross_validation` for the various\\n        cross-validation strategies that can be used here.\\n\\n        *Changed in 0.22*\\n            ``cv`` default value if None changed from 3-fold to 5-fold.\\n\\n    - `copy_X`: bool, default=True\\n        If ``True``, X will be copied; else, it may be overwritten.\\n\\n    - `verbose`: bool or int, default=0\\n        Amount of verbosity.\\n\\n    - `n_jobs`: int, default=None\\n        Number of CPUs to use during the cross validation.\\n        ``None`` means 1 unless in a `joblib.parallel_backend` context.\\n        ``-1`` means using all processors. See `Glossary <n_jobs>`\\n        for more details.\\n\\n    - `positive`: bool, default=False\\n        When set to ``True``, forces the coefficients to be positive.\\n\\n    - `random_state`: int, RandomState instance, default=None\\n        The seed of the pseudo random number generator that selects a random\\n        feature to update. Used when ``selection`` == 'random'.\\n        Pass an int for reproducible output across multiple function calls.\\n        See `Glossary <random_state>`.\\n\\n    - `selection`: {'cyclic', 'random'}, default='cyclic'\\n        If set to 'random', a random coefficient is updated every iteration\\n        rather than looping over features sequentially by default. This\\n        (setting to 'random') often leads to significantly faster convergence\\n        especially when tol is higher than 1e-4.\\n\\n    Attributes\\n    ----------\\n    - `alpha_`: float\\n        The amount of penalization chosen by cross validation.\\n\\n    - `l1_ratio_`: float\\n        The compromise between l1 and l2 penalization chosen by\\n        cross validation.\\n\\n    - `coef_`: ndarray of shape (n_features,) or (n_targets, n_features)\\n        Parameter vector (w in the cost function formula).\\n\\n    - `intercept_`: float or ndarray of shape (n_targets, n_features)\\n        Independent term in the decision function.\\n\\n    - `mse_path_`: ndarray of shape (n_l1_ratio, n_alpha, n_folds)\\n        Mean square error for the test set on each fold, varying l1_ratio and\\n        alpha.\\n\\n    - `alphas_`: ndarray of shape (n_alphas,) or (n_l1_ratio, n_alphas)\\n        The grid of alphas used for fitting, for each l1_ratio.\\n\\n    - `dual_gap_`: float\\n        The dual gaps at the end of the optimization for the optimal alpha.\\n\\n    - `n_iter_`: int\\n        Number of iterations run by the coordinate descent solver to reach\\n        the specified tolerance for the optimal alpha.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    See Also\\n    --------\\n    - `enet_path`: Compute elastic net path with coordinate descent.\\n    - `ElasticNet`: Linear regression with combined L1 and L2 priors as regularizer.\\n\\n    Notes\\n    -----\\n    In `fit`, once the best parameters `l1_ratio` and `alpha` are found through\\n    cross-validation, the model is fit again using the entire training set.\\n\\n    To avoid unnecessary memory duplication the `X` argument of the `fit`\\n    method should be directly passed as a Fortran-contiguous numpy array.\\n\\n    The parameter `l1_ratio` corresponds to alpha in the glmnet R package\\n    while alpha corresponds to the lambda parameter in glmnet.\\n    More specifically, the optimization objective is\\n\\n```python\\n1 / (2 * n_samples) * ||y - Xw||^2_2\\n+ alpha * l1_ratio * ||w||_1\\n+ 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\\n\\nou are interested in controlling the L1 and L2 penalty\\nrately, keep in mind that this is equivalent to::\\n\\na * L1 + b * L2\\n\\n:\\n\\nalpha = a + b and l1_ratio = a / (a + b).\\n\\nan example, see\\n:`examples/linear_model/plot_lasso_model_selection.py\\nx_glr_auto_examples_linear_model_plot_lasso_model_selection.py>`.\\n\\nples\\n----\\nfrom sklearn.linear_model import ElasticNetCV\\nfrom sklearn.datasets import make_regression\\n\\nX, y = make_regression(n_features=2, random_state=0)\\nregr = ElasticNetCV(cv=5, random_state=0)\\nregr.fit(X, y)\\nticNetCV(cv=5, random_state=0)\\nprint(regr.alpha_)\\n9...\\nprint(regr.intercept_)\\n8...\\nprint(regr.predict([[0, 0]]))\\n98...]\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/extra-tree-regressor\"} \":sklearn.regression/extra-tree-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [11 2]:\\n\\n|                     :name |      :default |\\n|---------------------------|---------------|\\n| :min-weight-fraction-leaf |         0.000 |\\n|           :max-leaf-nodes |               |\\n|    :min-impurity-decrease |         0.000 |\\n|        :min-samples-split |         2.000 |\\n|                :ccp-alpha |         0.000 |\\n|                 :splitter |        random |\\n|             :random-state |               |\\n|         :min-samples-leaf |             1 |\\n|             :max-features |         1.000 |\\n|                :max-depth |               |\\n|                :criterion | squared_error |\\n\"]]] [:span [:p/markdown \"An extremely randomized tree regressor.\\n\\n    Extra-trees differ from classic decision trees in the way they are built.\\n    When looking for the best split to separate the samples of a node into two\\n    groups, random splits are drawn for each of the `max_features` randomly\\n    selected features and the best split among those is chosen. When\\n    `max_features` is set 1, this amounts to building a totally random\\n    decision tree.\\n\\n    Warning: Extra-trees should only be used within ensemble methods.\\n\\n    Read more in the User Guide: `tree`.\\n\\n    Parameters\\n    ----------\\n    - `criterion`: {\\\"squared_error\\\", \\\"friedman_mse\\\"}, default=\\\"squared_error\\\"\\n        The function to measure the quality of a split. Supported criteria\\n        are \\\"squared_error\\\" for the mean squared error, which is equal to\\n        variance reduction as feature selection criterion and \\\"mae\\\" for the\\n        mean absolute error.\\n\\n        *Added in 0.18*\\n           Mean Absolute Error (MAE) criterion.\\n\\n        *Added in 0.24*\\n            Poisson deviance criterion.\\n\\n        *Deprecated since 1.0*\\n            Criterion \\\"mse\\\" was deprecated in v1.0 and will be removed in\\n            version 1.2. Use `criterion=\\\"squared_error\\\"` which is equivalent.\\n\\n        *Deprecated since 1.0*\\n            Criterion \\\"mae\\\" was deprecated in v1.0 and will be removed in\\n            version 1.2. Use `criterion=\\\"absolute_error\\\"` which is equivalent.\\n\\n    - `splitter`: {\\\"random\\\", \\\"best\\\"}, default=\\\"random\\\"\\n        The strategy used to choose the split at each node. Supported\\n        strategies are \\\"best\\\" to choose the best split and \\\"random\\\" to choose\\n        the best random split.\\n\\n    - `max_depth`: int, default=None\\n        The maximum depth of the tree. If None, then nodes are expanded until\\n        all leaves are pure or until all leaves contain less than\\n        min_samples_split samples.\\n\\n    - `min_samples_split`: int or float, default=2\\n        The minimum number of samples required to split an internal node:\\n\\n        - If int, then consider `min_samples_split` as the minimum number.\\n        - If float, then `min_samples_split` is a fraction and\\n          `ceil(min_samples_split * n_samples)` are the minimum\\n          number of samples for each split.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n    - `min_samples_leaf`: int or float, default=1\\n        The minimum number of samples required to be at a leaf node.\\n        A split point at any depth will only be considered if it leaves at\\n        least ``min_samples_leaf`` training samples in each of the left and\\n        right branches.  This may have the effect of smoothing the model,\\n        especially in regression.\\n\\n        - If int, then consider `min_samples_leaf` as the minimum number.\\n        - If float, then `min_samples_leaf` is a fraction and\\n          `ceil(min_samples_leaf * n_samples)` are the minimum\\n          number of samples for each node.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n    - `min_weight_fraction_leaf`: float, default=0.0\\n        The minimum weighted fraction of the sum total of weights (of all\\n        the input samples) required to be at a leaf node. Samples have\\n        equal weight when sample_weight is not provided.\\n\\n    - `max_features`: int, float, {\\\"auto\\\", \\\"sqrt\\\", \\\"log2\\\"} or None, default=1.0\\n        The number of features to consider when looking for the best split:\\n\\n        - If int, then consider `max_features` features at each split.\\n        - If float, then `max_features` is a fraction and\\n          `int(max_features * n_features)` features are considered at each\\n          split.\\n        - If \\\"auto\\\", then `max_features=n_features`.\\n        - If \\\"sqrt\\\", then `max_features=sqrt(n_features)`.\\n        - If \\\"log2\\\", then `max_features=log2(n_features)`.\\n        - If None, then `max_features=n_features`.\\n\\n        *Changed in 1.1*\\n            The default of `max_features` changed from `\\\"auto\\\"` to `1.0`.\\n\\n        *Deprecated since 1.1*\\n            The `\\\"auto\\\"` option was deprecated in 1.1 and will be removed\\n            in 1.3.\\n\\n        Note: the search for a split does not stop until at least one\\n        valid partition of the node samples is found, even if it requires to\\n        effectively inspect more than ``max_features`` features.\\n\\n    - `random_state`: int, RandomState instance or None, default=None\\n        Used to pick randomly the `max_features` used at each split.\\n        See `Glossary <random_state>` for details.\\n\\n    - `min_impurity_decrease`: float, default=0.0\\n        A node will be split if this split induces a decrease of the impurity\\n        greater than or equal to this value.\\n\\n        The weighted impurity decrease equation is the following\\n\\n```python\\nN_t / N * (impurity - N_t_R / N_t * right_impurity\\n                    - N_t_L / N_t * left_impurity)\\n\\ne ``N`` is the total number of samples, ``N_t`` is the number of\\nles at the current node, ``N_t_L`` is the number of samples in the\\n child, and ``N_t_R`` is the number of samples in the right child.\\n\\n`, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\\n`sample_weight`` is passed.\\n\\nersionadded:: 0.19\\n\\n_nodes : int, default=None\\n a tree with ``max_leaf_nodes`` in best-first fashion.\\n nodes are defined as relative reduction in impurity.\\none then unlimited number of leaf nodes.\\n\\na : non-negative float, default=0.0\\nlexity parameter used for Minimal Cost-Complexity Pruning. The\\nree with the largest cost complexity that is smaller than\\np_alpha`` will be chosen. By default, no pruning is performed. See\\n:`minimal_cost_complexity_pruning` for details.\\n\\nersionadded:: 0.22\\n\\nes\\n--\\nures_ : int\\ninferred value of max_features.\\n\\nes_ : int\\nnumber of features when ``fit`` is performed.\\n\\neprecated:: 1.0\\nn_features_` is deprecated in 1.0 and will be removed in\\n.2. Use `n_features_in_` instead.\\n\\nes_in_ : int\\ner of features seen during :term:`fit`.\\n\\nersionadded:: 0.24\\n\\nnames_in_ : ndarray of shape (`n_features_in_`,)\\ns of features seen during :term:`fit`. Defined only when `X`\\nfeature names that are all strings.\\n\\nersionadded:: 1.0\\n\\nimportances_ : ndarray of shape (n_features,)\\nrn impurity-based feature importances (the higher, the more\\nrtant the feature).\\n\\ning: impurity-based feature importances can be misleading for\\n cardinality features (many unique values). See\\nc:`sklearn.inspection.permutation_importance` as an alternative.\\n\\ns_ : int\\nnumber of outputs when ``fit`` is performed.\\n\\nTree instance\\nunderlying Tree object. Please refer to\\nlp(sklearn.tree._tree.Tree)`` for attributes of Tree object and\\n:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\\nbasic usage of these attributes.\\n\\n\\n\\neClassifier : An extremely randomized tree classifier.\\nensemble.ExtraTreesClassifier : An extra-trees classifier.\\nensemble.ExtraTreesRegressor : An extra-trees regressor.\\n\\n\\n\\nult values for the parameters controlling the size of the trees\\nmax_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\\n trees which can potentially be very large on some data sets. To\\nemory consumption, the complexity and size of the trees should be\\ned by setting those parameter values.\\n\\nes\\n--\\n\\n. Geurts, D. Ernst., and L. Wehenkel, \\\"Extremely randomized trees\\\",\\nachine Learning, 63(1), 3-42, 2006.\\n\\n\\n\\n sklearn.datasets import load_diabetes\\n sklearn.model_selection import train_test_split\\n sklearn.ensemble import BaggingRegressor\\n sklearn.tree import ExtraTreeRegressor\\n = load_diabetes(return_X_y=True)\\nain, X_test, y_train, y_test = train_test_split(\\nX, y, random_state=0)\\na_tree = ExtraTreeRegressor(random_state=0)\\n= BaggingRegressor(extra_tree, random_state=0).fit(\\nX_train, y_train)\\nscore(X_test, y_test)\\n\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/extra-trees-regressor\"} \":sklearn.regression/extra-trees-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [17 2]:\\n\\n|                     :name |      :default |\\n|---------------------------|---------------|\\n| :min-weight-fraction-leaf |         0.000 |\\n|           :max-leaf-nodes |               |\\n|    :min-impurity-decrease |         0.000 |\\n|        :min-samples-split |         2.000 |\\n|                :bootstrap |         false |\\n|                :ccp-alpha |         0.000 |\\n|                   :n-jobs |               |\\n|             :random-state |               |\\n|                :oob-score |         false |\\n|         :min-samples-leaf |             1 |\\n|             :max-features |         1.000 |\\n|               :warm-start |         false |\\n|                :max-depth |               |\\n|             :n-estimators |           100 |\\n|              :max-samples |               |\\n|                :criterion | squared_error |\\n|                  :verbose |             0 |\\n\"]]] [:span [:p/markdown \"\\n    An extra-trees regressor.\\n\\n    This class implements a meta estimator that fits a number of\\n    randomized decision trees (a.k.a. extra-trees) on various sub-samples\\n    of the dataset and uses averaging to improve the predictive accuracy\\n    and control over-fitting.\\n\\n    Read more in the User Guide: `forest`.\\n\\n    Parameters\\n    ----------\\n    - `n_estimators`: int, default=100\\n        The number of trees in the forest.\\n\\n        *Changed in 0.22*\\n           The default value of ``n_estimators`` changed from 10 to 100\\n           in 0.22.\\n\\n    - `criterion`: {\\\"squared_error\\\", \\\"absolute_error\\\"}, default=\\\"squared_error\\\"\\n        The function to measure the quality of a split. Supported criteria\\n        are \\\"squared_error\\\" for the mean squared error, which is equal to\\n        variance reduction as feature selection criterion, and \\\"absolute_error\\\"\\n        for the mean absolute error.\\n\\n        *Added in 0.18*\\n           Mean Absolute Error (MAE) criterion.\\n\\n        *Deprecated since 1.0*\\n            Criterion \\\"mse\\\" was deprecated in v1.0 and will be removed in\\n            version 1.2. Use `criterion=\\\"squared_error\\\"` which is equivalent.\\n\\n        *Deprecated since 1.0*\\n            Criterion \\\"mae\\\" was deprecated in v1.0 and will be removed in\\n            version 1.2. Use `criterion=\\\"absolute_error\\\"` which is equivalent.\\n\\n    - `max_depth`: int, default=None\\n        The maximum depth of the tree. If None, then nodes are expanded until\\n        all leaves are pure or until all leaves contain less than\\n        min_samples_split samples.\\n\\n    - `min_samples_split`: int or float, default=2\\n        The minimum number of samples required to split an internal node:\\n\\n        - If int, then consider `min_samples_split` as the minimum number.\\n        - If float, then `min_samples_split` is a fraction and\\n          `ceil(min_samples_split * n_samples)` are the minimum\\n          number of samples for each split.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n    - `min_samples_leaf`: int or float, default=1\\n        The minimum number of samples required to be at a leaf node.\\n        A split point at any depth will only be considered if it leaves at\\n        least ``min_samples_leaf`` training samples in each of the left and\\n        right branches.  This may have the effect of smoothing the model,\\n        especially in regression.\\n\\n        - If int, then consider `min_samples_leaf` as the minimum number.\\n        - If float, then `min_samples_leaf` is a fraction and\\n          `ceil(min_samples_leaf * n_samples)` are the minimum\\n          number of samples for each node.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n    - `min_weight_fraction_leaf`: float, default=0.0\\n        The minimum weighted fraction of the sum total of weights (of all\\n        the input samples) required to be at a leaf node. Samples have\\n        equal weight when sample_weight is not provided.\\n\\n    - `max_features`: {\\\"sqrt\\\", \\\"log2\\\", None}, int or float, default=1.0\\n        The number of features to consider when looking for the best split:\\n\\n        - If int, then consider `max_features` features at each split.\\n        - If float, then `max_features` is a fraction and\\n          `round(max_features * n_features)` features are considered at each\\n          split.\\n        - If \\\"auto\\\", then `max_features=n_features`.\\n        - If \\\"sqrt\\\", then `max_features=sqrt(n_features)`.\\n        - If \\\"log2\\\", then `max_features=log2(n_features)`.\\n        - If None or 1.0, then `max_features=n_features`.\\n\\n\\n---\\n**Note**\\n\\nThe default of 1.0 is equivalent to bagged trees and more\\nrandomness can be achieved by setting smaller values, e.g. 0.3.\\n\\nersionchanged:: 1.1\\nThe default of `max_features` changed from `\\\"auto\\\"` to 1.0.\\n\\neprecated:: 1.1\\nThe `\\\"auto\\\"` option was deprecated in 1.1 and will be removed\\nin 1.3.\\n\\n: the search for a split does not stop until at least one\\nd partition of the node samples is found, even if it requires to\\nctively inspect more than ``max_features`` features.\\n\\n_nodes : int, default=None\\n trees with ``max_leaf_nodes`` in best-first fashion.\\n nodes are defined as relative reduction in impurity.\\none then unlimited number of leaf nodes.\\n\\nrity_decrease : float, default=0.0\\nde will be split if this split induces a decrease of the impurity\\nter than or equal to this value.\\n\\nweighted impurity decrease equation is the following::\\n\\nN_t / N * (impurity - N_t_R / N_t * right_impurity\\n                    - N_t_L / N_t * left_impurity)\\n\\ne ``N`` is the total number of samples, ``N_t`` is the number of\\nles at the current node, ``N_t_L`` is the number of samples in the\\n child, and ``N_t_R`` is the number of samples in the right child.\\n\\n`, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\\n`sample_weight`` is passed.\\n\\nersionadded:: 0.19\\n\\np : bool, default=False\\nher bootstrap samples are used when building trees. If False, the\\ne dataset is used to build each tree.\\n\\ne : bool, default=False\\nher to use out-of-bag samples to estimate the generalization score.\\n available if bootstrap=True.\\n\\n int, default=None\\nnumber of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\\nh:`decision_path` and :meth:`apply` are all parallelized over the\\ns. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\\next. ``-1`` means using all processors. See :term:`Glossary\\nobs>` for more details.\\n\\ntate : int, RandomState instance or None, default=None\\nrols 3 sources of randomness:\\n\\ne bootstrapping of the samples used when building trees\\nf ``bootstrap=True``)\\ne sampling of the features to consider when looking for the best\\nlit at each node (if ``max_features < n_features``)\\ne draw of the splits for each of the `max_features`\\n\\n:term:`Glossary <random_state>` for details.\\n\\n: int, default=0\\nrols the verbosity when fitting and predicting.\\n\\nrt : bool, default=False\\n set to ``True``, reuse the solution of the previous call to fit\\nadd more estimators to the ensemble, otherwise, just fit a whole\\nforest. See :term:`the Glossary <warm_start>`.\\n\\na : non-negative float, default=0.0\\nlexity parameter used for Minimal Cost-Complexity Pruning. The\\nree with the largest cost complexity that is smaller than\\np_alpha`` will be chosen. By default, no pruning is performed. See\\n:`minimal_cost_complexity_pruning` for details.\\n\\nersionadded:: 0.22\\n\\nles : int or float, default=None\\nootstrap is True, the number of samples to draw from X\\nrain each base estimator.\\n\\n None (default), then draw `X.shape[0]` samples.\\n int, then draw `max_samples` samples.\\n float, then draw `max_samples * X.shape[0]` samples. Thus,\\nax_samples` should be in the interval `(0.0, 1.0]`.\\n\\nersionadded:: 0.22\\n\\nes\\n--\\nimator_ : ExtraTreeRegressor\\nchild estimator template used to create the collection of fitted\\nestimators.\\n\\nrs_ : list of DecisionTreeRegressor\\ncollection of fitted sub-estimators.\\n\\nimportances_ : ndarray of shape (n_features,)\\nimpurity-based feature importances.\\nhigher, the more important the feature.\\nimportance of a feature is computed as the (normalized)\\nl reduction of the criterion brought by that feature.  It is also\\nn as the Gini importance.\\n\\ning: impurity-based feature importances can be misleading for\\n cardinality features (many unique values). See\\nc:`sklearn.inspection.permutation_importance` as an alternative.\\n\\nes_ : int\\nnumber of features.\\n\\neprecated:: 1.0\\nAttribute `n_features_` was deprecated in version 1.0 and will be\\nremoved in 1.2. Use `n_features_in_` instead.\\n\\nes_in_ : int\\ner of features seen during :term:`fit`.\\n\\nersionadded:: 0.24\\n\\nnames_in_ : ndarray of shape (`n_features_in_`,)\\ns of features seen during :term:`fit`. Defined only when `X`\\nfeature names that are all strings.\\n\\nersionadded:: 1.0\\n\\ns_ : int\\nnumber of outputs.\\n\\ne_ : float\\ne of the training dataset obtained using an out-of-bag estimate.\\n attribute exists only when ``oob_score`` is True.\\n\\niction_ : ndarray of shape (n_samples,) or (n_samples, n_outputs)\\niction computed with out-of-bag estimate on the training set.\\n attribute exists only when ``oob_score`` is True.\\n\\n\\n\\nesClassifier : An extra-trees classifier with random splits.\\nrestClassifier : A random forest classifier with optimal splits.\\nrestRegressor : Ensemble regressor using trees with optimal splits.\\n\\n\\n\\nult values for the parameters controlling the size of the trees\\nmax_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\\n trees which can potentially be very large on some data sets. To\\nemory consumption, the complexity and size of the trees should be\\ned by setting those parameter values.\\n\\nes\\n--\\n. Geurts, D. Ernst., and L. Wehenkel, \\\"Extremely randomized trees\\\",\\nachine Learning, 63(1), 3-42, 2006.\\n\\n\\n\\n sklearn.datasets import load_diabetes\\n sklearn.model_selection import train_test_split\\n sklearn.ensemble import ExtraTreesRegressor\\n = load_diabetes(return_X_y=True)\\nain, X_test, y_train, y_test = train_test_split(\\nX, y, random_state=0)\\n= ExtraTreesRegressor(n_estimators=100, random_state=0).fit(\\n_train, y_train)\\nscore(X_test, y_test)\\n.\\n\\n---\\n\"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/gamma-regressor\"} \":sklearn.regression/gamma-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [6 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|         :alpha |     1.000 |\\n| :fit-intercept |      true |\\n|      :max-iter |       100 |\\n|           :tol | 0.0001000 |\\n|       :verbose |         0 |\\n|    :warm-start |     false |\\n\"]]] [:span [:p/markdown \"Generalized Linear Model with a Gamma distribution.\\n\\n    This regressor uses the 'log' link function.\\n\\n    Read more in the User Guide: `Generalized_linear_regression`.\\n\\n    *Added in 0.23*\\n\\n    Parameters\\n    ----------\\n    - `alpha`: float, default=1\\n        Constant that multiplies the penalty term and thus determines the\\n        regularization strength. ``alpha = 0`` is equivalent to unpenalized\\n        GLMs. In this case, the design matrix `X` must have full column rank\\n        (no collinearities).\\n        Values must be in the range `[0.0, inf)`.\\n\\n    - `fit_intercept`: bool, default=True\\n        Specifies if a constant (a.k.a. bias or intercept) should be\\n        added to the linear predictor (X @ coef + intercept).\\n\\n    - `max_iter`: int, default=100\\n        The maximal number of iterations for the solver.\\n        Values must be in the range `[1, inf)`.\\n\\n    - `tol`: float, default=1e-4\\n        Stopping criterion. For the lbfgs solver,\\n        the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol``\\n        where ``g_j`` is the j-th component of the gradient (derivative) of\\n        the objective function.\\n        Values must be in the range `(0.0, inf)`.\\n\\n    - `warm_start`: bool, default=False\\n        If set to ``True``, reuse the solution of the previous call to ``fit``\\n        as initialization for ``coef_`` and ``intercept_`` .\\n\\n    - `verbose`: int, default=0\\n        For the lbfgs solver set verbose to any positive number for verbosity.\\n        Values must be in the range `[0, inf)`.\\n\\n    Attributes\\n    ----------\\n    - `coef_`: array of shape (n_features,)\\n        Estimated coefficients for the linear predictor (`X * coef_ +\\n        intercept_`) in the GLM.\\n\\n    - `intercept_`: float\\n        Intercept (a.k.a. bias) added to linear predictor.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `n_iter_`: int\\n        Actual number of iterations used in the solver.\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    See Also\\n    --------\\n    - `PoissonRegressor`: Generalized Linear Model with a Poisson distribution.\\n    - `TweedieRegressor`: Generalized Linear Model with a Tweedie distribution.\\n\\n    Examples\\n    --------\\n    >>> from sklearn import linear_model\\n    >>> clf = linear_model.GammaRegressor()\\n    >>> X = [[1, 2], [2, 3], [3, 4], [4, 3]]\\n    >>> y = [19, 26, 33, 30]\\n    >>> clf.fit(X, y)\\n    GammaRegressor()\\n    >>> clf.score(X, y)\\n    0.773...\\n    >>> clf.coef_\\n    array([0.072..., 0.066...])\\n    >>> clf.intercept_\\n    2.896...\\n    >>> clf.predict([[1, 0], [2, 8]])\\n    array([19.483..., 35.795...])\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/gaussian-process-regressor\"} \":sklearn.regression/gaussian-process-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [7 2]:\\n\\n|                 :name |      :default |\\n|-----------------------|---------------|\\n|                :alpha |     1.000E-10 |\\n|         :copy-x-train |          true |\\n|               :kernel |               |\\n| :n-restarts-optimizer |             0 |\\n|          :normalize-y |         false |\\n|            :optimizer | fmin_l_bfgs_b |\\n|         :random-state |               |\\n\"]]] [:span [:p/markdown \"Gaussian process regression (GPR).\\n\\n    The implementation is based on Algorithm 2.1 of [1]_.\\n\\n    In addition to standard scikit-learn estimator API,\\n    `GaussianProcessRegressor`:\\n\\n       * allows prediction without prior fitting (based on the GP prior)\\n       * provides an additional method `sample_y(X)`, which evaluates samples\\n         drawn from the GPR (prior or posterior) at given inputs\\n       * exposes a method `log_marginal_likelihood(theta)`, which can be used\\n         externally for other ways of selecting hyperparameters, e.g., via\\n         Markov chain Monte Carlo.\\n\\n    Read more in the User Guide: `gaussian_process`.\\n\\n    *Added in 0.18*\\n\\n    Parameters\\n    ----------\\n    - `kernel`: kernel instance, default=None\\n        The kernel specifying the covariance function of the GP. If None is\\n        passed, the kernel ``ConstantKernel(1.0, constant_value_bounds=\\\"fixed\\\")\\n        * RBF(1.0, length_scale_bounds=\\\"fixed\\\")`` is used as default. Note that\\n        the kernel hyperparameters are optimized during fitting unless the\\n        bounds are marked as \\\"fixed\\\".\\n\\n    - `alpha`: float or ndarray of shape (n_samples,), default=1e-10\\n        Value added to the diagonal of the kernel matrix during fitting.\\n        This can prevent a potential numerical issue during fitting, by\\n        ensuring that the calculated values form a positive definite matrix.\\n        It can also be interpreted as the variance of additional Gaussian\\n        measurement noise on the training observations. Note that this is\\n        different from using a `WhiteKernel`. If an array is passed, it must\\n        have the same number of entries as the data used for fitting and is\\n        used as datapoint-dependent noise level. Allowing to specify the\\n        noise level directly as a parameter is mainly for convenience and\\n        for consistency with `~sklearn.linear_model.Ridge`.\\n\\n    - `optimizer`: \\\"fmin_l_bfgs_b\\\" or callable, default=\\\"fmin_l_bfgs_b\\\"\\n        Can either be one of the internally supported optimizers for optimizing\\n        the kernel's parameters, specified by a string, or an externally\\n        defined optimizer passed as a callable. If a callable is passed, it\\n        must have the signature\\n\\n```python\\ndef optimizer(obj_func, initial_theta, bounds):\\n    # * 'obj_func': the objective function to be minimized, which\\n    #   takes the hyperparameters theta as a parameter and an\\n    #   optional flag eval_gradient, which determines if the\\n    #   gradient is returned additionally to the function value\\n    # * 'initial_theta': the initial value for theta, which can be\\n    #   used by local optimizers\\n    # * 'bounds': the bounds on the values of theta\\n    ....\\n    # Returned are the best found hyperparameters theta and\\n    # the corresponding value of the target function.\\n    return theta_opt, func_min\\n\\ndefault, the L-BFGS-B algorithm from `scipy.optimize.minimize`\\nsed. If None is passed, the kernel's parameters are kept fixed.\\nlable internal optimizers are: `{'fmin_l_bfgs_b'}`.\\n\\nts_optimizer : int, default=0\\nnumber of restarts of the optimizer for finding the kernel's\\nmeters which maximize the log-marginal likelihood. The first run\\nhe optimizer is performed from the kernel's initial parameters,\\nremaining ones (if any) from thetas sampled log-uniform randomly\\n the space of allowed theta-values. If greater than 0, all bounds\\n be finite. Note that `n_restarts_optimizer == 0` implies that one\\nis performed.\\n\\ne_y : bool, default=False\\nher or not to normalize the target values `y` by removing the mean\\nscaling to unit-variance. This is recommended for cases where\\n-mean, unit-variance priors are used. Note that, in this\\nementation, the normalisation is reversed before the GP predictions\\nreported.\\n\\nersionchanged:: 0.23\\n\\nrain : bool, default=True\\nrue, a persistent copy of the training data is stored in the\\nct. Otherwise, just a reference to the training data is stored,\\nh might cause predictions to change if the data is modified\\nrnally.\\n\\ntate : int, RandomState instance or None, default=None\\nrmines random number generation used to initialize the centers.\\n an int for reproducible results across multiple function calls.\\n:term:`Glossary <random_state>`.\\n\\nes\\n--\\n : array-like of shape (n_samples, n_features) or list of object\\nure vectors or other representations of training data (also\\nired for prediction).\\n\\n : array-like of shape (n_samples,) or (n_samples, n_targets)\\net values in training data (also required for prediction).\\n\\n: kernel instance\\nkernel used for prediction. The structure of the kernel is the\\n as the one passed as parameter but with optimized hyperparameters.\\n\\nay-like of shape (n_samples, n_samples)\\nr-triangular Cholesky decomposition of the kernel in ``X_train_``.\\n\\n array-like of shape (n_samples,)\\n coefficients of training data points in kernel space.\\n\\ninal_likelihood_value_ : float\\nlog-marginal-likelihood of ``self.kernel_.theta``.\\n\\nes_in_ : int\\ner of features seen during :term:`fit`.\\n\\nersionadded:: 0.24\\n\\nnames_in_ : ndarray of shape (`n_features_in_`,)\\ns of features seen during :term:`fit`. Defined only when `X`\\nfeature names that are all strings.\\n\\nersionadded:: 1.0\\n\\n\\n\\nProcessClassifier : Gaussian process classification (GPC)\\nd on Laplace approximation.\\n\\nes\\n--\\nRasmussen, Carl Edward.\\nsian processes in machine learning.\\\"\\nr school on machine learning. Springer, Berlin, Heidelberg, 2003\\n://www.gaussianprocess.org/gpml/chapters/RW.pdf>`_.\\n\\n\\n\\n sklearn.datasets import make_friedman2\\n sklearn.gaussian_process import GaussianProcessRegressor\\n sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\\n = make_friedman2(n_samples=500, noise=0, random_state=0)\\nel = DotProduct() + WhiteKernel()\\n= GaussianProcessRegressor(kernel=kernel,\\n    random_state=0).fit(X, y)\\nscore(X, y)\\n.\\npredict(X[:2,:], return_std=True)\\n653.0..., 592.1...]), array([316.6..., 316.6...]))\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/gradient-boosting-regressor\"} \":sklearn.regression/gradient-boosting-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [21 2]:\\n\\n|                     :name |      :default |\\n|---------------------------|---------------|\\n|         :n-iter-no-change |               |\\n|            :learning-rate |        0.1000 |\\n| :min-weight-fraction-leaf |         0.000 |\\n|           :max-leaf-nodes |               |\\n|    :min-impurity-decrease |         0.000 |\\n|        :min-samples-split |         2.000 |\\n|                      :tol |     0.0001000 |\\n|                :subsample |         1.000 |\\n|                :ccp-alpha |         0.000 |\\n|             :random-state |               |\\n|                       ... |           ... |\\n|         :min-samples-leaf |         1.000 |\\n|             :max-features |               |\\n|                     :init |               |\\n|                    :alpha |        0.9000 |\\n|               :warm-start |         false |\\n|                :max-depth |             3 |\\n|      :validation-fraction |        0.1000 |\\n|             :n-estimators |           100 |\\n|                :criterion |  friedman_mse |\\n|                     :loss | squared_error |\\n|                  :verbose |             0 |\\n\"]]] [:span [:p/markdown \"Gradient Boosting for regression.\\n\\n    GB builds an additive model in a forward stage-wise fashion;\\n    it allows for the optimization of arbitrary differentiable loss functions.\\n    In each stage a regression tree is fit on the negative gradient of the\\n    given loss function.\\n\\n    Read more in the User Guide: `gradient_boosting`.\\n\\n    Parameters\\n    ----------\\n    - `loss`: {'squared_error', 'absolute_error', 'huber', 'quantile'},             default='squared_error'\\n        Loss function to be optimized. 'squared_error' refers to the squared\\n        error for regression. 'absolute_error' refers to the absolute error of\\n        regression and is a robust loss function. 'huber' is a\\n        combination of the two. 'quantile' allows quantile regression (use\\n        `alpha` to specify the quantile).\\n\\n        *Deprecated since 1.0*\\n            The loss 'ls' was deprecated in v1.0 and will be removed in\\n            version 1.2. Use `loss='squared_error'` which is equivalent.\\n\\n        *Deprecated since 1.0*\\n            The loss 'lad' was deprecated in v1.0 and will be removed in\\n            version 1.2. Use `loss='absolute_error'` which is equivalent.\\n\\n    - `learning_rate`: float, default=0.1\\n        Learning rate shrinks the contribution of each tree by `learning_rate`.\\n        There is a trade-off between learning_rate and n_estimators.\\n        Values must be in the range `(0.0, inf)`.\\n\\n    - `n_estimators`: int, default=100\\n        The number of boosting stages to perform. Gradient boosting\\n        is fairly robust to over-fitting so a large number usually\\n        results in better performance.\\n        Values must be in the range `[1, inf)`.\\n\\n    - `subsample`: float, default=1.0\\n        The fraction of samples to be used for fitting the individual base\\n        learners. If smaller than 1.0 this results in Stochastic Gradient\\n        Boosting. `subsample` interacts with the parameter `n_estimators`.\\n        Choosing `subsample < 1.0` leads to a reduction of variance\\n        and an increase in bias.\\n        Values must be in the range `(0.0, 1.0]`.\\n\\n    - `criterion`: {'friedman_mse', 'squared_error', 'mse'},             default='friedman_mse'\\n        The function to measure the quality of a split. Supported criteria are\\n        \\\"friedman_mse\\\" for the mean squared error with improvement score by\\n        Friedman, \\\"squared_error\\\" for mean squared error. The default value of\\n        \\\"friedman_mse\\\" is generally the best as it can provide a better\\n        approximation in some cases.\\n\\n        *Added in 0.18*\\n\\n        *Deprecated since 1.0*\\n            Criterion 'mse' was deprecated in v1.0 and will be removed in\\n            version 1.2. Use `criterion='squared_error'` which is equivalent.\\n\\n    - `min_samples_split`: int or float, default=2\\n        The minimum number of samples required to split an internal node:\\n\\n        - If int, values must be in the range `[2, inf)`.\\n        - If float, values must be in the range `(0.0, 1.0]` and `min_samples_split`\\n          will be `ceil(min_samples_split * n_samples)`.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n    - `min_samples_leaf`: int or float, default=1\\n        The minimum number of samples required to be at a leaf node.\\n        A split point at any depth will only be considered if it leaves at\\n        least ``min_samples_leaf`` training samples in each of the left and\\n        right branches.  This may have the effect of smoothing the model,\\n        especially in regression.\\n\\n        - If int, values must be in the range `[1, inf)`.\\n        - If float, values must be in the range `(0.0, 1.0]` and `min_samples_leaf`\\n          will be `ceil(min_samples_leaf * n_samples)`.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n    - `min_weight_fraction_leaf`: float, default=0.0\\n        The minimum weighted fraction of the sum total of weights (of all\\n        the input samples) required to be at a leaf node. Samples have\\n        equal weight when sample_weight is not provided.\\n        Values must be in the range `[0.0, 0.5]`.\\n\\n    - `max_depth`: int, default=3\\n        Maximum depth of the individual regression estimators. The maximum\\n        depth limits the number of nodes in the tree. Tune this parameter\\n        for best performance; the best value depends on the interaction\\n        of the input variables.\\n        Values must be in the range `[1, inf)`.\\n\\n    - `min_impurity_decrease`: float, default=0.0\\n        A node will be split if this split induces a decrease of the impurity\\n        greater than or equal to this value.\\n        Values must be in the range `[0.0, inf)`.\\n\\n        The weighted impurity decrease equation is the following\\n\\n```python\\nN_t / N * (impurity - N_t_R / N_t * right_impurity\\n                    - N_t_L / N_t * left_impurity)\\n\\ne ``N`` is the total number of samples, ``N_t`` is the number of\\nles at the current node, ``N_t_L`` is the number of samples in the\\n child, and ``N_t_R`` is the number of samples in the right child.\\n\\n`, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\\n`sample_weight`` is passed.\\n\\nersionadded:: 0.19\\n\\nstimator or 'zero', default=None\\nstimator object that is used to compute the initial predictions.\\nit`` has to provide :term:`fit` and :term:`predict`. If 'zero', the\\nial raw predictions are set to zero. By default a\\nmmyEstimator`` is used, predicting either the average target value\\n loss='squared_error'), or a quantile for the other losses.\\n\\ntate : int, RandomState instance or None, default=None\\nrols the random seed given to each Tree estimator at each\\nting iteration.\\nddition, it controls the random permutation of the features at\\n split (see Notes for more details).\\nlso controls the random splitting of the training data to obtain a\\ndation set if `n_iter_no_change` is not None.\\n an int for reproducible output across multiple function calls.\\n:term:`Glossary <random_state>`.\\n\\nures : {'auto', 'sqrt', 'log2'}, int or float, default=None\\nnumber of features to consider when looking for the best split:\\n\\n int, values must be in the range `[1, inf)`.\\n float, values must be in the range `(0.0, 1.0]` and the features\\nnsidered at each split will be `int(max_features * n_features)`.\\n \\\"auto\\\", then `max_features=n_features`.\\n \\\"sqrt\\\", then `max_features=sqrt(n_features)`.\\n \\\"log2\\\", then `max_features=log2(n_features)`.\\n None, then `max_features=n_features`.\\n\\nsing `max_features < n_features` leads to a reduction of variance\\nan increase in bias.\\n\\n: the search for a split does not stop until at least one\\nd partition of the node samples is found, even if it requires to\\nctively inspect more than ``max_features`` features.\\n\\nfloat, default=0.9\\nalpha-quantile of the huber loss function and the quantile\\n function. Only if ``loss='huber'`` or ``loss='quantile'``.\\nes must be in the range `(0.0, 1.0)`.\\n\\n: int, default=0\\nle verbose output. If 1 then it prints progress and performance\\n in a while (the more trees the lower the frequency). If greater\\n 1 then it prints progress and performance for every tree.\\nes must be in the range `[0, inf)`.\\n\\n_nodes : int, default=None\\n trees with ``max_leaf_nodes`` in best-first fashion.\\n nodes are defined as relative reduction in impurity.\\nes must be in the range `[2, inf)`.\\none, then unlimited number of leaf nodes.\\n\\nrt : bool, default=False\\n set to ``True``, reuse the solution of the previous call to fit\\nadd more estimators to the ensemble, otherwise, just erase the\\nious solution. See :term:`the Glossary <warm_start>`.\\n\\non_fraction : float, default=0.1\\nproportion of training data to set aside as validation set for\\ny stopping. Values must be in the range `(0.0, 1.0)`.\\n used if ``n_iter_no_change`` is set to an integer.\\n\\nersionadded:: 0.20\\n\\no_change : int, default=None\\niter_no_change`` is used to decide if early stopping will be used\\nerminate training when validation score is not improving. By\\nult it is set to None to disable early stopping. If set to a\\ner, it will set aside ``validation_fraction`` size of the training\\n as validation and terminate training when validation score is not\\noving in all of the previous ``n_iter_no_change`` numbers of\\nations.\\nes must be in the range `[1, inf)`.\\n\\nersionadded:: 0.20\\n\\noat, default=1e-4\\nrance for the early stopping. When the loss is not improving\\nt least tol for ``n_iter_no_change`` iterations (if set to a\\ner), the training stops.\\nes must be in the range `(0.0, inf)`.\\n\\nersionadded:: 0.20\\n\\na : non-negative float, default=0.0\\nlexity parameter used for Minimal Cost-Complexity Pruning. The\\nree with the largest cost complexity that is smaller than\\np_alpha`` will be chosen. By default, no pruning is performed.\\nes must be in the range `[0.0, inf)`.\\n:ref:`minimal_cost_complexity_pruning` for details.\\n\\nersionadded:: 0.22\\n\\nes\\n--\\nimportances_ : ndarray of shape (n_features,)\\nimpurity-based feature importances.\\nhigher, the more important the feature.\\nimportance of a feature is computed as the (normalized)\\nl reduction of the criterion brought by that feature.  It is also\\nn as the Gini importance.\\n\\ning: impurity-based feature importances can be misleading for\\n cardinality features (many unique values). See\\nc:`sklearn.inspection.permutation_importance` as an alternative.\\n\\novement_ : ndarray of shape (n_estimators,)\\nimprovement in loss (= deviance) on the out-of-bag samples\\ntive to the previous iteration.\\nb_improvement_[0]`` is the improvement in\\n of the first stage over the ``init`` estimator.\\n available if ``subsample < 1.0``\\n\\nore_ : ndarray of shape (n_estimators,)\\ni-th score ``train_score_[i]`` is the deviance (= loss) of the\\nl at iteration ``i`` on the in-bag sample.\\n`subsample == 1`` this is the deviance on the training data.\\n\\nLossFunction\\nconcrete ``LossFunction`` object.\\n\\neprecated:: 1.1\\n Attribute `loss_` was deprecated in version 1.1 and will be\\nremoved in 1.3.\\n\\nestimator\\nestimator that provides the initial predictions.\\nvia the ``init`` argument or ``loss.init_estimator``.\\n\\nrs_ : ndarray of DecisionTreeRegressor of shape (n_estimators, 1)\\ncollection of fitted sub-estimators.\\n\\ntors_ : int\\nnumber of estimators as selected by early stopping (if\\niter_no_change`` is specified). Otherwise it is set to\\nestimators``.\\n\\nes_ : int\\nnumber of data features.\\n\\neprecated:: 1.0\\nAttribute `n_features_` was deprecated in version 1.0 and will be\\nremoved in 1.2. Use `n_features_in_` instead.\\n\\nes_in_ : int\\ner of features seen during :term:`fit`.\\n\\nersionadded:: 0.24\\n\\nnames_in_ : ndarray of shape (`n_features_in_`,)\\ns of features seen during :term:`fit`. Defined only when `X`\\nfeature names that are all strings.\\n\\nersionadded:: 1.0\\n\\nures_ : int\\ninferred value of max_features.\\n\\n\\n\\nientBoostingRegressor : Histogram-based Gradient Boosting\\nsification Tree.\\ntree.DecisionTreeRegressor : A decision tree regressor.\\nensemble.RandomForestRegressor : A random forest regressor.\\n\\n\\n\\nures are always randomly permuted at each split. Therefore,\\n found split may vary, even with the same training data and\\natures=n_features``, if the improvement of the criterion is\\nl for several splits enumerated during the search of the best\\no obtain a deterministic behaviour during fitting,\\n_state`` has to be fixed.\\n\\nes\\n--\\nman, Greedy Function Approximation: A Gradient Boosting\\n The Annals of Statistics, Vol. 29, No. 5, 2001.\\n\\nman, Stochastic Gradient Boosting, 1999\\n\\ne, R. Tibshirani and J. Friedman.\\n of Statistical Learning Ed. 2, Springer, 2009.\\n\\n\\n\\n sklearn.datasets import make_regression\\n sklearn.ensemble import GradientBoostingRegressor\\n sklearn.model_selection import train_test_split\\n = make_regression(random_state=0)\\nain, X_test, y_train, y_test = train_test_split(\\nX, y, random_state=0)\\n= GradientBoostingRegressor(random_state=0)\\nfit(X_train, y_train)\\nBoostingRegressor(random_state=0)\\npredict(X_test[1:2])\\n61...])\\nscore(X_test, y_test)\\n\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/hist-gradient-boosting-regressor\"} \":sklearn.regression/hist-gradient-boosting-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [19 2]:\\n\\n|                 :name |      :default |\\n|-----------------------|---------------|\\n|     :n-iter-no-change |         10.00 |\\n|        :learning-rate |        0.1000 |\\n|       :max-leaf-nodes |         31.00 |\\n|              :scoring |          loss |\\n|                  :tol |     1.000E-07 |\\n|       :early-stopping |          auto |\\n|             :quantile |               |\\n|             :max-iter |           100 |\\n|         :random-state |               |\\n|             :max-bins |           255 |\\n|     :min-samples-leaf |            20 |\\n|        :monotonic-cst |               |\\n|           :warm-start |         false |\\n|            :max-depth |               |\\n|  :validation-fraction |        0.1000 |\\n|                 :loss | squared_error |\\n|              :verbose |             0 |\\n| :categorical-features |               |\\n|   :l-2-regularization |         0.000 |\\n\"]]] [:span [:p/markdown \"Histogram-based Gradient Boosting Regression Tree.\\n\\n    This estimator is much faster than\\n    `GradientBoostingRegressor<sklearn.ensemble.GradientBoostingRegressor>`\\n    for big datasets (n_samples >= 10 000).\\n\\n    This estimator has native support for missing values (NaNs). During\\n    training, the tree grower learns at each split point whether samples\\n    with missing values should go to the left or right child, based on the\\n    potential gain. When predicting, samples with missing values are\\n    assigned to the left or right child consequently. If no missing values\\n    were encountered for a given feature during training, then samples with\\n    missing values are mapped to whichever child has the most samples.\\n\\n    This implementation is inspired by\\n    [LightGBM ](https://github.com/Microsoft/LightGBM).\\n\\n    Read more in the User Guide: `histogram_based_gradient_boosting`.\\n\\n    *Added in 0.21*\\n\\n    Parameters\\n    ----------\\n    - `loss`: {'squared_error', 'absolute_error', 'poisson', 'quantile'},             default='squared_error'\\n        The loss function to use in the boosting process. Note that the\\n        \\\"squared error\\\" and \\\"poisson\\\" losses actually implement\\n        \\\"half least squares loss\\\" and \\\"half poisson deviance\\\" to simplify the\\n        computation of the gradient. Furthermore, \\\"poisson\\\" loss internally\\n        uses a log-link and requires ``y >= 0``.\\n        \\\"quantile\\\" uses the pinball loss.\\n\\n        *Changed in 0.23*\\n           Added option 'poisson'.\\n\\n        *Changed in 1.1*\\n           Added option 'quantile'.\\n\\n        *Deprecated since 1.0*\\n            The loss 'least_squares' was deprecated in v1.0 and will be removed\\n            in version 1.2. Use `loss='squared_error'` which is equivalent.\\n\\n        *Deprecated since 1.0*\\n            The loss 'least_absolute_deviation' was deprecated in v1.0 and will\\n            be removed in version 1.2. Use `loss='absolute_error'` which is\\n            equivalent.\\n\\n    - `quantile`: float, default=None\\n        If loss is \\\"quantile\\\", this parameter specifies which quantile to be estimated\\n        and must be between 0 and 1.\\n    - `learning_rate`: float, default=0.1\\n        The learning rate, also known as *shrinkage*. This is used as a\\n        multiplicative factor for the leaves values. Use ``1`` for no\\n        shrinkage.\\n    - `max_iter`: int, default=100\\n        The maximum number of iterations of the boosting process, i.e. the\\n        maximum number of trees.\\n    - `max_leaf_nodes`: int or None, default=31\\n        The maximum number of leaves for each tree. Must be strictly greater\\n        than 1. If None, there is no maximum limit.\\n    - `max_depth`: int or None, default=None\\n        The maximum depth of each tree. The depth of a tree is the number of\\n        edges to go from the root to the deepest leaf.\\n        Depth isn't constrained by default.\\n    - `min_samples_leaf`: int, default=20\\n        The minimum number of samples per leaf. For small datasets with less\\n        than a few hundred samples, it is recommended to lower this value\\n        since only very shallow trees would be built.\\n    - `l2_regularization`: float, default=0\\n        The L2 regularization parameter. Use ``0`` for no regularization\\n        (default).\\n    - `max_bins`: int, default=255\\n        The maximum number of bins to use for non-missing values. Before\\n        training, each feature of the input array `X` is binned into\\n        integer-valued bins, which allows for a much faster training stage.\\n        Features with a small number of unique values may use less than\\n        ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin\\n        is always reserved for missing values. Must be no larger than 255.\\n    - `categorical_features`: array-like of {bool, int} of shape (n_features)             or shape (n_categorical_features,), default=None\\n        Indicates the categorical features.\\n\\n        - None : no feature will be considered categorical.\\n        - boolean array-like : boolean mask indicating categorical features.\\n        - integer array-like : integer indices indicating categorical\\n          features.\\n\\n        For each categorical feature, there must be at most `max_bins` unique\\n        categories, and each categorical value must be in [0, max_bins -1].\\n\\n        Read more in the User Guide: `categorical_support_gbdt`.\\n\\n        *Added in 0.24*\\n\\n    - `monotonic_cst`: array-like of int of shape (n_features), default=None\\n        Indicates the monotonic constraint to enforce on each feature. -1, 1\\n        and 0 respectively correspond to a negative constraint, positive\\n        constraint and no constraint. Read more in the User Guide: `monotonic_cst_gbdt`.\\n\\n        *Added in 0.23*\\n\\n    - `warm_start`: bool, default=False\\n        When set to ``True``, reuse the solution of the previous call to fit\\n        and add more estimators to the ensemble. For results to be valid, the\\n        estimator should be re-trained on the same data only.\\n        See `the Glossary <warm_start>`.\\n    - `early_stopping`: 'auto' or bool, default='auto'\\n        If 'auto', early stopping is enabled if the sample size is larger than\\n        10000. If True, early stopping is enabled, otherwise early stopping is\\n        disabled.\\n\\n        *Added in 0.23*\\n\\n    - `scoring`: str or callable or None, default='loss'\\n        Scoring parameter to use for early stopping. It can be a single\\n        string (see :ref:`scoring_parameter`) or a callable (see\\n        :ref:`scoring`). If None, the estimator's default scorer is used. If\\n        ``scoring='loss'``, early stopping is checked w.r.t the loss value.\\n        Only used if early stopping is performed.\\n    - `validation_fraction`: int or float or None, default=0.1\\n        Proportion (or absolute size) of training data to set aside as\\n        validation data for early stopping. If None, early stopping is done on\\n        the training data. Only used if early stopping is performed.\\n    - `n_iter_no_change`: int, default=10\\n        Used to determine when to \\\"early stop\\\". The fitting process is\\n        stopped when none of the last ``n_iter_no_change`` scores are better\\n        than the ``n_iter_no_change - 1`` -th-to-last one, up to some\\n        tolerance. Only used if early stopping is performed.\\n    - `tol`: float, default=1e-7\\n        The absolute tolerance to use when comparing scores during early\\n        stopping. The higher the tolerance, the more likely we are to early\\n        stop: higher tolerance means that it will be harder for subsequent\\n        iterations to be considered an improvement upon the reference score.\\n    - `verbose`: int, default=0\\n        The verbosity level. If not zero, print some information about the\\n        fitting process.\\n    - `random_state`: int, RandomState instance or None, default=None\\n        Pseudo-random number generator to control the subsampling in the\\n        binning process, and the train/validation data split if early stopping\\n        is enabled.\\n        Pass an int for reproducible output across multiple function calls.\\n        See `Glossary <random_state>`.\\n\\n    Attributes\\n    ----------\\n    - `do_early_stopping_`: bool\\n        Indicates whether early stopping is used during training.\\n    - `n_iter_`: int\\n        The number of iterations as selected by early stopping, depending on\\n        the `early_stopping` parameter. Otherwise it corresponds to max_iter.\\n    - `n_trees_per_iteration_`: int\\n        The number of tree that are built at each iteration. For regressors,\\n        this is always 1.\\n    - `train_score_`: ndarray, shape (n_iter_+1,)\\n        The scores at each iteration on the training data. The first entry\\n        is the score of the ensemble before the first iteration. Scores are\\n        computed according to the ``scoring`` parameter. If ``scoring`` is\\n        not 'loss', scores are computed on a subset of at most 10 000\\n        samples. Empty if no early stopping.\\n    - `validation_score_`: ndarray, shape (n_iter_+1,)\\n        The scores at each iteration on the held-out validation data. The\\n        first entry is the score of the ensemble before the first iteration.\\n        Scores are computed according to the ``scoring`` parameter. Empty if\\n        no early stopping or if ``validation_fraction`` is None.\\n    - `is_categorical_`: ndarray, shape (n_features, ) or None\\n        Boolean mask for the categorical features. ``None`` if there are no\\n        categorical features.\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    See Also\\n    --------\\n    - `GradientBoostingRegressor`: Exact gradient boosting method that does not\\n        scale as good on datasets with a large number of samples.\\n    - `sklearn.tree.DecisionTreeRegressor`: A decision tree regressor.\\n    - `RandomForestRegressor`: A meta-estimator that fits a number of decision\\n        tree regressors on various sub-samples of the dataset and uses\\n        averaging to improve the statistical performance and control\\n        over-fitting.\\n    - `AdaBoostRegressor`: A meta-estimator that begins by fitting a regressor\\n        on the original dataset and then fits additional copies of the\\n        regressor on the same dataset but where the weights of instances are\\n        adjusted according to the error of the current prediction. As such,\\n        subsequent regressors focus more on difficult cases.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.ensemble import HistGradientBoostingRegressor\\n    >>> from sklearn.datasets import load_diabetes\\n    >>> X, y = load_diabetes(return_X_y=True)\\n    >>> est = HistGradientBoostingRegressor().fit(X, y)\\n    >>> est.score(X, y)\\n    0.92...\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/huber-regressor\"} \":sklearn.regression/huber-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [6 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|         :alpha | 0.0001000 |\\n|       :epsilon |     1.350 |\\n| :fit-intercept |      true |\\n|      :max-iter |       100 |\\n|           :tol | 1.000E-05 |\\n|    :warm-start |     false |\\n\"]]] [:span [:p/markdown \"Linear regression model that is robust to outliers.\\n\\n    The Huber Regressor optimizes the squared loss for the samples where\\n    ``|(y - X'w) / sigma| < epsilon`` and the absolute loss for the samples\\n    where ``|(y - X'w) / sigma| > epsilon``, where w and sigma are parameters\\n    to be optimized. The parameter sigma makes sure that if y is scaled up\\n    or down by a certain factor, one does not need to rescale epsilon to\\n    achieve the same robustness. Note that this does not take into account\\n    the fact that the different features of X may be of different scales.\\n\\n    This makes sure that the loss function is not heavily influenced by the\\n    outliers while not completely ignoring their effect.\\n\\n    Read more in the User Guide: `huber_regression`\\n\\n    *Added in 0.18*\\n\\n    Parameters\\n    ----------\\n    - `epsilon`: float, greater than 1.0, default=1.35\\n        The parameter epsilon controls the number of samples that should be\\n        classified as outliers. The smaller the epsilon, the more robust it is\\n        to outliers.\\n\\n    - `max_iter`: int, default=100\\n        Maximum number of iterations that\\n        ``scipy.optimize.minimize(method=\\\"L-BFGS-B\\\")`` should run for.\\n\\n    - `alpha`: float, default=0.0001\\n        Regularization parameter.\\n\\n    - `warm_start`: bool, default=False\\n        This is useful if the stored attributes of a previously used model\\n        has to be reused. If set to False, then the coefficients will\\n        be rewritten for every call to fit.\\n        See `the Glossary <warm_start>`.\\n\\n    - `fit_intercept`: bool, default=True\\n        Whether or not to fit the intercept. This can be set to False\\n        if the data is already centered around the origin.\\n\\n    - `tol`: float, default=1e-05\\n        The iteration will stop when\\n        ``max{|proj g_i | i = 1, ..., n}`` <= ``tol``\\n        where pg_i is the i-th component of the projected gradient.\\n\\n    Attributes\\n    ----------\\n    - `coef_`: array, shape (n_features,)\\n        Features got by optimizing the Huber loss.\\n\\n    - `intercept_`: float\\n        Bias.\\n\\n    - `scale_`: float\\n        The value by which ``|y - X'w - c|`` is scaled down.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    - `n_iter_`: int\\n        Number of iterations that\\n        ``scipy.optimize.minimize(method=\\\"L-BFGS-B\\\")`` has run for.\\n\\n        *Changed in 0.20*\\n\\n            In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\\n            ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\\n\\n    - `outliers_`: array, shape (n_samples,)\\n        A boolean mask which is set to True where the samples are identified\\n        as outliers.\\n\\n    See Also\\n    --------\\n    - `RANSACRegressor`: RANSAC (RANdom SAmple Consensus) algorithm.\\n    - `TheilSenRegressor`: Theil-Sen Estimator robust multivariate regression model.\\n    - `SGDRegressor`: Fitted by minimizing a regularized empirical loss with SGD.\\n\\n    References\\n    ----------\\n - [1] Peter J. Huber, Elvezio M. Ronchetti, Robust Statistics\\n           Concomitant scale estimates, pg 172\\n - [2] Art B. Owen (2006), A robust hybrid of lasso and ridge regression.\\n           https://statweb.stanford.edu/~owen/reports/hhu.pdf\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> from sklearn.linear_model import HuberRegressor, LinearRegression\\n    >>> from sklearn.datasets import make_regression\\n    >>> rng = np.random.RandomState(0)\\n    >>> X, y, coef = make_regression(\\n    ...     n_samples=200, n_features=2, noise=4.0, coef=True, random_state=0)\\n    >>> X[:4] = rng.uniform(10, 20, (4, 2))\\n    >>> y[:4] = rng.uniform(10, 20, 4)\\n    >>> huber = HuberRegressor().fit(X, y)\\n    >>> huber.score(X, y)\\n    -7.284...\\n    >>> huber.predict(X[:1,])\\n    array([806.7200...])\\n    >>> linear = LinearRegression().fit(X, y)\\n    >>> print(\\\"True coefficients:\\\", coef)\\n    True coefficients: [20.4923...  34.1698...]\\n    >>> print(\\\"Huber coefficients:\\\", huber.coef_)\\n    Huber coefficients: [17.7906... 31.0106...]\\n    >>> print(\\\"Linear Regression coefficients:\\\", linear.coef_)\\n    Linear Regression coefficients: [-1.9221...  7.0226...]\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/isotonic-regression\"} \":sklearn.regression/isotonic-regression\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [4 2]:\\n\\n|          :name | :default |\\n|----------------|----------|\\n|    :increasing |     true |\\n| :out-of-bounds |      nan |\\n|         :y-max |          |\\n|         :y-min |          |\\n\"]]] [:span [:p/markdown \"Isotonic regression model.\\n\\n    Read more in the User Guide: `isotonic`.\\n\\n    *Added in 0.13*\\n\\n    Parameters\\n    ----------\\n    - `y_min`: float, default=None\\n        Lower bound on the lowest predicted value (the minimum value may\\n        still be higher). If not set, defaults to -inf.\\n\\n    - `y_max`: float, default=None\\n        Upper bound on the highest predicted value (the maximum may still be\\n        lower). If not set, defaults to +inf.\\n\\n    - `increasing`: bool or 'auto', default=True\\n        Determines whether the predictions should be constrained to increase\\n        or decrease with `X`. 'auto' will decide based on the Spearman\\n        correlation estimate's sign.\\n\\n    - `out_of_bounds`: {'nan', 'clip', 'raise'}, default='nan'\\n        Handles how `X` values outside of the training domain are handled\\n        during prediction.\\n\\n        - 'nan', predictions will be NaN.\\n        - 'clip', predictions will be set to the value corresponding to\\n          the nearest train interval endpoint.\\n        - 'raise', a `ValueError` is raised.\\n\\n    Attributes\\n    ----------\\n    - `X_min_`: float\\n        Minimum value of input array `X_` for left bound.\\n\\n    - `X_max_`: float\\n        Maximum value of input array `X_` for right bound.\\n\\n    - `X_thresholds_`: ndarray of shape (n_thresholds,)\\n        Unique ascending `X` values used to interpolate\\n        the y = f(X) monotonic function.\\n\\n        *Added in 0.24*\\n\\n    - `y_thresholds_`: ndarray of shape (n_thresholds,)\\n        De-duplicated `y` values suitable to interpolate the y = f(X)\\n        monotonic function.\\n\\n        *Added in 0.24*\\n\\n    - `f_`: function\\n        The stepwise interpolating function that covers the input domain ``X``.\\n\\n    - `increasing_`: bool\\n        Inferred value for ``increasing``.\\n\\n    See Also\\n    --------\\n    - `sklearn.linear_model.LinearRegression`: Ordinary least squares Linear\\n        Regression.\\n    - `sklearn.ensemble.HistGradientBoostingRegressor`: Gradient boosting that\\n        is a non-parametric model accepting monotonicity constraints.\\n    - `isotonic_regression`: Function to solve the isotonic regression model.\\n\\n    Notes\\n    -----\\n    Ties are broken using the secondary method from de Leeuw, 1977.\\n\\n    References\\n    ----------\\n    Isotonic Median Regression: A Linear Programming Approach\\n    Nilotpal Chakravarti\\n    Mathematics of Operations Research\\n    Vol. 14, No. 2 (May, 1989), pp. 303-308\\n\\n    Isotone Optimization in R : Pool-Adjacent-Violators\\n    Algorithm (PAVA) and Active Set Methods\\n    de Leeuw, Hornik, Mair\\n    Journal of Statistical Software 2009\\n\\n    Correctness of Kruskal's algorithms for monotone regression with ties\\n    de Leeuw, Psychometrica, 1977\\n\\n    Examples\\n    --------\\n    >>> from sklearn.datasets import make_regression\\n    >>> from sklearn.isotonic import IsotonicRegression\\n    >>> X, y = make_regression(n_samples=10, n_features=1, random_state=41)\\n    >>> iso_reg = IsotonicRegression().fit(X, y)\\n    >>> iso_reg.predict([.1, .2])\\n    array([1.8628..., 3.7256...])\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/k-neighbors-regressor\"} \":sklearn.regression/k-neighbors-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [8 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|     :algorithm |      auto |\\n|     :leaf-size |        30 |\\n|        :metric | minkowski |\\n| :metric-params |           |\\n|        :n-jobs |           |\\n|   :n-neighbors |         5 |\\n|             :p |         2 |\\n|       :weights |   uniform |\\n\"]]] [:span [:p/markdown \"Regression based on k-nearest neighbors.\\n\\n    The target is predicted by local interpolation of the targets\\n    associated of the nearest neighbors in the training set.\\n\\n    Read more in the User Guide: `regression`.\\n\\n    *Added in 0.9*\\n\\n    Parameters\\n    ----------\\n    - `n_neighbors`: int, default=5\\n        Number of neighbors to use by default for `kneighbors` queries.\\n\\n    - `weights`: {'uniform', 'distance'} or callable, default='uniform'\\n        Weight function used in prediction.  Possible values:\\n\\n        - 'uniform' : uniform weights.  All points in each neighborhood\\n          are weighted equally.\\n        - 'distance' : weight points by the inverse of their distance.\\n          in this case, closer neighbors of a query point will have a\\n          greater influence than neighbors which are further away.\\n        - [callable] : a user-defined function which accepts an\\n          array of distances, and returns an array of the same shape\\n          containing the weights.\\n\\n        Uniform weights are used by default.\\n\\n    - `algorithm`: {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\\n        Algorithm used to compute the nearest neighbors:\\n\\n        - 'ball_tree' will use `BallTree`\\n        - 'kd_tree' will use `KDTree`\\n        - 'brute' will use a brute-force search.\\n        - 'auto' will attempt to decide the most appropriate algorithm\\n          based on the values passed to `fit` method.\\n\\n        Note: fitting on sparse input will override the setting of\\n        this parameter, using brute force.\\n\\n    - `leaf_size`: int, default=30\\n        Leaf size passed to BallTree or KDTree.  This can affect the\\n        speed of the construction and query, as well as the memory\\n        required to store the tree.  The optimal value depends on the\\n        nature of the problem.\\n\\n    - `p`: int, default=2\\n        Power parameter for the Minkowski metric. When p = 1, this is\\n        equivalent to using manhattan_distance (l1), and euclidean_distance\\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\\n\\n    - `metric`: str or callable, default='minkowski'\\n        The distance metric to use for the tree.  The default metric is\\n        minkowski, and with p=2 is equivalent to the standard Euclidean\\n        metric. For a list of available metrics, see the documentation of\\n        `~sklearn.metrics.DistanceMetric` and the metrics listed in\\n        `sklearn.metrics.pairwise.PAIRWISE_DISTANCE_FUNCTIONS`. Note that the\\n        \\\"cosine\\\" metric uses `~sklearn.metrics.pairwise.cosine_distances`.\\n        If metric is \\\"precomputed\\\", X is assumed to be a distance matrix and\\n        must be square during fit. X may be a `sparse graph`,\\n        in which case only \\\"nonzero\\\" elements may be considered neighbors.\\n\\n    - `metric_params`: dict, default=None\\n        Additional keyword arguments for the metric function.\\n\\n    - `n_jobs`: int, default=None\\n        The number of parallel jobs to run for neighbors search.\\n        ``None`` means 1 unless in a `joblib.parallel_backend` context.\\n        ``-1`` means using all processors. See `Glossary <n_jobs>`\\n        for more details.\\n        Doesn't affect `fit` method.\\n\\n    Attributes\\n    ----------\\n    - `effective_metric_`: str or callable\\n        The distance metric to use. It will be same as the `metric` parameter\\n        or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\\n        'minkowski' and `p` parameter set to 2.\\n\\n    - `effective_metric_params_`: dict\\n        Additional keyword arguments for the metric function. For most metrics\\n        will be same with `metric_params` parameter, but may also contain the\\n        `p` parameter value if the `effective_metric_` attribute is set to\\n        'minkowski'.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    - `n_samples_fit_`: int\\n        Number of samples in the fitted data.\\n\\n    See Also\\n    --------\\n    - `NearestNeighbors`: Unsupervised learner for implementing neighbor searches.\\n    - `RadiusNeighborsRegressor`: Regression based on neighbors within a fixed radius.\\n    - `KNeighborsClassifier`: Classifier implementing the k-nearest neighbors vote.\\n    - `RadiusNeighborsClassifier`: Classifier implementing\\n        a vote among neighbors within a given radius.\\n\\n    Notes\\n    -----\\n    See Nearest Neighbors: `neighbors` in the online documentation\\n    for a discussion of the choice of ``algorithm`` and ``leaf_size``.\\n\\n\\n---\\n**Warning**\\n\\nRegarding the Nearest Neighbors algorithms, if it is found that two\\nneighbors, neighbor `k+1` and `k`, have identical distances but\\ndifferent labels, the results will depend on the ordering of the\\ntraining data.\\n\\nps://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\\n\\nmples\\n-----\\n X = [[0], [1], [2], [3]]\\n y = [0, 0, 1, 1]\\n from sklearn.neighbors import KNeighborsRegressor\\n neigh = KNeighborsRegressor(n_neighbors=2)\\n neigh.fit(X, y)\\nighborsRegressor(...)\\n print(neigh.predict([[1.5]]))\\n5]\\n\\n---\\n\"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/kernel-ridge\"} \":sklearn.regression/kernel-ridge\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [6 2]:\\n\\n|          :name | :default |\\n|----------------|----------|\\n|         :alpha |        1 |\\n|        :coef-0 |        1 |\\n|        :degree |        3 |\\n|         :gamma |          |\\n|        :kernel |   linear |\\n| :kernel-params |          |\\n\"]]] [:span [:p/markdown \"Kernel ridge regression.\\n\\n    Kernel ridge regression (KRR) combines ridge regression (linear least\\n    squares with l2-norm regularization) with the kernel trick. It thus\\n    learns a linear function in the space induced by the respective kernel and\\n    the data. For non-linear kernels, this corresponds to a non-linear\\n    function in the original space.\\n\\n    The form of the model learned by KRR is identical to support vector\\n    regression (SVR). However, different loss functions are used: KRR uses\\n    squared error loss while support vector regression uses epsilon-insensitive\\n    loss, both combined with l2 regularization. In contrast to SVR, fitting a\\n    KRR model can be done in closed-form and is typically faster for\\n    medium-sized datasets. On the other hand, the learned model is non-sparse\\n    and thus slower than SVR, which learns a sparse model for epsilon > 0, at\\n    prediction-time.\\n\\n    This estimator has built-in support for multi-variate regression\\n    (i.e., when y is a 2d-array of shape [n_samples, n_targets]).\\n\\n    Read more in the User Guide: `kernel_ridge`.\\n\\n    Parameters\\n    ----------\\n    - `alpha`: float or array-like of shape (n_targets,), default=1.0\\n        Regularization strength; must be a positive float. Regularization\\n        improves the conditioning of the problem and reduces the variance of\\n        the estimates. Larger values specify stronger regularization.\\n        Alpha corresponds to ``1 / (2C)`` in other linear models such as\\n        `~sklearn.linear_model.LogisticRegression` or\\n        `~sklearn.svm.LinearSVC`. If an array is passed, penalties are\\n        assumed to be specific to the targets. Hence they must correspond in\\n        number. See :ref:`ridge_regression` for formula.\\n\\n    - `kernel`: str or callable, default=\\\"linear\\\"\\n        Kernel mapping used internally. This parameter is directly passed to\\n        `~sklearn.metrics.pairwise.pairwise_kernel`.\\n        If `kernel` is a string, it must be one of the metrics\\n        in `pairwise.PAIRWISE_KERNEL_FUNCTIONS` or \\\"precomputed\\\".\\n        If `kernel` is \\\"precomputed\\\", X is assumed to be a kernel matrix.\\n        Alternatively, if `kernel` is a callable function, it is called on\\n        each pair of instances (rows) and the resulting value recorded. The\\n        callable should take two rows from X as input and return the\\n        corresponding kernel value as a single number. This means that\\n        callables from `sklearn.metrics.pairwise` are not allowed, as\\n        they operate on matrices, not single samples. Use the string\\n        identifying the kernel instead.\\n\\n    - `gamma`: float, default=None\\n        Gamma parameter for the RBF, laplacian, polynomial, exponential chi2\\n        and sigmoid kernels. Interpretation of the default value is left to\\n        the kernel; see the documentation for sklearn.metrics.pairwise.\\n        Ignored by other kernels.\\n\\n    - `degree`: float, default=3\\n        Degree of the polynomial kernel. Ignored by other kernels.\\n\\n    - `coef0`: float, default=1\\n        Zero coefficient for polynomial and sigmoid kernels.\\n        Ignored by other kernels.\\n\\n    - `kernel_params`: mapping of str to any, default=None\\n        Additional parameters (keyword arguments) for kernel function passed\\n        as callable object.\\n\\n    Attributes\\n    ----------\\n    - `dual_coef_`: ndarray of shape (n_samples,) or (n_samples, n_targets)\\n        Representation of weight vector(s) in kernel space\\n\\n    - `X_fit_`: {ndarray, sparse matrix} of shape (n_samples, n_features)\\n        Training data, which is also required for prediction. If\\n        kernel == \\\"precomputed\\\" this is instead the precomputed\\n        training matrix, of shape (n_samples, n_samples).\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    See Also\\n    --------\\n    - `sklearn.gaussian_process.GaussianProcessRegressor`: Gaussian\\n        Process regressor providing automatic kernel hyperparameters\\n        tuning and predictions uncertainty.\\n    - `sklearn.linear_model.Ridge`: Linear ridge regression.\\n    - `sklearn.linear_model.RidgeCV`: Ridge regression with built-in\\n        cross-validation.\\n    - `sklearn.svm.SVR`: Support Vector Regression accepting a large variety\\n        of kernels.\\n\\n    References\\n    ----------\\n    * Kevin P. Murphy\\n      \\\"Machine Learning: A Probabilistic Perspective\\\", The MIT Press\\n      chapter 14.4.3, pp. 492-493\\n\\n    Examples\\n    --------\\n    >>> from sklearn.kernel_ridge import KernelRidge\\n    >>> import numpy as np\\n    >>> n_samples, n_features = 10, 5\\n    >>> rng = np.random.RandomState(0)\\n    >>> y = rng.randn(n_samples)\\n    >>> X = rng.randn(n_samples, n_features)\\n    >>> krr = KernelRidge(alpha=1.0)\\n    >>> krr.fit(X, y)\\n    KernelRidge(alpha=1.0)\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/lars\"} \":sklearn.regression/lars\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [10 2]:\\n\\n|            :name |   :default |\\n|------------------|------------|\\n|       :normalize | deprecated |\\n|        :fit-path |       true |\\n|             :eps |  2.220E-16 |\\n|    :random-state |            |\\n|          :jitter |            |\\n|          :copy-x |       true |\\n|      :precompute |       auto |\\n|   :fit-intercept |       true |\\n| :n-nonzero-coefs |        500 |\\n|         :verbose |      false |\\n\"]]] [:span [:p/markdown \"Least Angle Regression model a.k.a. LAR.\\n\\n    Read more in the User Guide: `least_angle_regression`.\\n\\n    Parameters\\n    ----------\\n    - `fit_intercept`: bool, default=True\\n        Whether to calculate the intercept for this model. If set\\n        to false, no intercept will be used in calculations\\n        (i.e. data is expected to be centered).\\n\\n    - `verbose`: bool or int, default=False\\n        Sets the verbosity amount.\\n\\n    - `normalize`: bool, default=True\\n        This parameter is ignored when ``fit_intercept`` is set to False.\\n        If True, the regressors X will be normalized before regression by\\n        subtracting the mean and dividing by the l2-norm.\\n        If you wish to standardize, please use\\n        `~sklearn.preprocessing.StandardScaler` before calling ``fit``\\n        on an estimator with ``normalize=False``.\\n\\n        *Deprecated since 1.0*\\n            ``normalize`` was deprecated in version 1.0. It will default\\n            to False in 1.2 and be removed in 1.4.\\n\\n    - `precompute`: bool, 'auto' or array-like , default='auto'\\n        Whether to use a precomputed Gram matrix to speed up\\n        calculations. If set to ``'auto'`` let us decide. The Gram\\n        matrix can also be passed as argument.\\n\\n    - `n_nonzero_coefs`: int, default=500\\n        Target number of non-zero coefficients. Use ``np.inf`` for no limit.\\n\\n    - `eps`: float, default=np.finfo(float).eps\\n        The machine-precision regularization in the computation of the\\n        Cholesky diagonal factors. Increase this for very ill-conditioned\\n        systems. Unlike the ``tol`` parameter in some iterative\\n        optimization-based algorithms, this parameter does not control\\n        the tolerance of the optimization.\\n\\n    - `copy_X`: bool, default=True\\n        If ``True``, X will be copied; else, it may be overwritten.\\n\\n    - `fit_path`: bool, default=True\\n        If True the full path is stored in the ``coef_path_`` attribute.\\n        If you compute the solution for a large problem or many targets,\\n        setting ``fit_path`` to ``False`` will lead to a speedup, especially\\n        with a small alpha.\\n\\n    - `jitter`: float, default=None\\n        Upper bound on a uniform noise parameter to be added to the\\n        `y` values, to satisfy the model's assumption of\\n        one-at-a-time computations. Might help with stability.\\n\\n        *Added in 0.23*\\n\\n    - `random_state`: int, RandomState instance or None, default=None\\n        Determines random number generation for jittering. Pass an int\\n        for reproducible output across multiple function calls.\\n        See `Glossary <random_state>`. Ignored if `jitter` is None.\\n\\n        *Added in 0.23*\\n\\n    Attributes\\n    ----------\\n    - `alphas_`: array-like of shape (n_alphas + 1,) or list of such arrays\\n        Maximum of covariances (in absolute value) at each iteration.\\n        ``n_alphas`` is either ``max_iter``, ``n_features`` or the\\n        number of nodes in the path with ``alpha >= alpha_min``, whichever\\n        is smaller. If this is a list of array-like, the length of the outer\\n        list is `n_targets`.\\n\\n    - `active_`: list of shape (n_alphas,) or list of such lists\\n        Indices of active variables at the end of the path.\\n        If this is a list of list, the length of the outer list is `n_targets`.\\n\\n    - `coef_path_`: array-like of shape (n_features, n_alphas + 1) or list             of such arrays\\n        The varying values of the coefficients along the path. It is not\\n        present if the ``fit_path`` parameter is ``False``. If this is a list\\n        of array-like, the length of the outer list is `n_targets`.\\n\\n    - `coef_`: array-like of shape (n_features,) or (n_targets, n_features)\\n        Parameter vector (w in the formulation formula).\\n\\n    - `intercept_`: float or array-like of shape (n_targets,)\\n        Independent term in decision function.\\n\\n    - `n_iter_`: array-like or int\\n        The number of iterations taken by lars_path to find the\\n        grid of alphas for each target.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    See Also\\n    --------\\n    lars_path: Compute Least Angle Regression or Lasso\\n        path using LARS algorithm.\\n    - `LarsCV`: Cross-validated Least Angle Regression model.\\n    - `sklearn.decomposition.sparse_encode`: Sparse coding.\\n\\n    Examples\\n    --------\\n    >>> from sklearn import linear_model\\n    >>> reg = linear_model.Lars(n_nonzero_coefs=1, normalize=False)\\n    >>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1.1111, 0, -1.1111])\\n    Lars(n_nonzero_coefs=1, normalize=False)\\n    >>> print(reg.coef_)\\n    [ 0. -1.11...]\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/lars-cv\"} \":sklearn.regression/lars-cv\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [10 2]:\\n\\n|          :name |   :default |\\n|----------------|------------|\\n|     :normalize | deprecated |\\n|           :eps |  2.220E-16 |\\n|  :max-n-alphas |       1000 |\\n|      :max-iter |        500 |\\n|        :n-jobs |            |\\n|        :copy-x |       true |\\n|    :precompute |       auto |\\n| :fit-intercept |       true |\\n|            :cv |            |\\n|       :verbose |      false |\\n\"]]] [:span [:p/markdown \"Cross-validated Least Angle Regression model.\\n\\n    See glossary entry for `cross-validation estimator`.\\n\\n    Read more in the User Guide: `least_angle_regression`.\\n\\n    Parameters\\n    ----------\\n    - `fit_intercept`: bool, default=True\\n        Whether to calculate the intercept for this model. If set\\n        to false, no intercept will be used in calculations\\n        (i.e. data is expected to be centered).\\n\\n    - `verbose`: bool or int, default=False\\n        Sets the verbosity amount.\\n\\n    - `max_iter`: int, default=500\\n        Maximum number of iterations to perform.\\n\\n    - `normalize`: bool, default=True\\n        This parameter is ignored when ``fit_intercept`` is set to False.\\n        If True, the regressors X will be normalized before regression by\\n        subtracting the mean and dividing by the l2-norm.\\n        If you wish to standardize, please use\\n        `~sklearn.preprocessing.StandardScaler` before calling ``fit``\\n        on an estimator with ``normalize=False``.\\n\\n        *Deprecated since 1.0*\\n            ``normalize`` was deprecated in version 1.0. It will default\\n            to False in 1.2 and be removed in 1.4.\\n\\n    - `precompute`: bool, 'auto' or array-like , default='auto'\\n        Whether to use a precomputed Gram matrix to speed up\\n        calculations. If set to ``'auto'`` let us decide. The Gram matrix\\n        cannot be passed as argument since we will use only subsets of X.\\n\\n    - `cv`: int, cross-validation generator or an iterable, default=None\\n        Determines the cross-validation splitting strategy.\\n        Possible inputs for cv are:\\n\\n        - None, to use the default 5-fold cross-validation,\\n        - integer, to specify the number of folds.\\n        - `CV splitter`,\\n        - An iterable yielding (train, test) splits as arrays of indices.\\n\\n        For integer/None inputs, `KFold` is used.\\n\\n        Refer User Guide: `cross_validation` for the various\\n        cross-validation strategies that can be used here.\\n\\n        *Changed in 0.22*\\n            ``cv`` default value if None changed from 3-fold to 5-fold.\\n\\n    - `max_n_alphas`: int, default=1000\\n        The maximum number of points on the path used to compute the\\n        residuals in the cross-validation.\\n\\n    - `n_jobs`: int or None, default=None\\n        Number of CPUs to use during the cross validation.\\n        ``None`` means 1 unless in a `joblib.parallel_backend` context.\\n        ``-1`` means using all processors. See `Glossary <n_jobs>`\\n        for more details.\\n\\n    - `eps`: float, default=np.finfo(float).eps\\n        The machine-precision regularization in the computation of the\\n        Cholesky diagonal factors. Increase this for very ill-conditioned\\n        systems. Unlike the ``tol`` parameter in some iterative\\n        optimization-based algorithms, this parameter does not control\\n        the tolerance of the optimization.\\n\\n    - `copy_X`: bool, default=True\\n        If ``True``, X will be copied; else, it may be overwritten.\\n\\n    Attributes\\n    ----------\\n    - `active_`: list of length n_alphas or list of such lists\\n        Indices of active variables at the end of the path.\\n        If this is a list of lists, the outer list length is `n_targets`.\\n\\n    - `coef_`: array-like of shape (n_features,)\\n        parameter vector (w in the formulation formula)\\n\\n    - `intercept_`: float\\n        independent term in decision function\\n\\n    - `coef_path_`: array-like of shape (n_features, n_alphas)\\n        the varying values of the coefficients along the path\\n\\n    - `alpha_`: float\\n        the estimated regularization parameter alpha\\n\\n    - `alphas_`: array-like of shape (n_alphas,)\\n        the different values of alpha along the path\\n\\n    - `cv_alphas_`: array-like of shape (n_cv_alphas,)\\n        all the values of alpha along the path for the different folds\\n\\n    - `mse_path_`: array-like of shape (n_folds, n_cv_alphas)\\n        the mean square error on left-out for each fold along the path\\n        (alpha values given by ``cv_alphas``)\\n\\n    - `n_iter_`: array-like or int\\n        the number of iterations run by Lars with the optimal alpha.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    See Also\\n    --------\\n    - `lars_path`: Compute Least Angle Regression or Lasso\\n        path using LARS algorithm.\\n    - `lasso_path`: Compute Lasso path with coordinate descent.\\n    - `Lasso`: Linear Model trained with L1 prior as\\n        regularizer (aka the Lasso).\\n    - `LassoCV`: Lasso linear model with iterative fitting\\n        along a regularization path.\\n    - `LassoLars`: Lasso model fit with Least Angle Regression a.k.a. Lars.\\n    - `LassoLarsIC`: Lasso model fit with Lars using BIC\\n        or AIC for model selection.\\n    - `sklearn.decomposition.sparse_encode`: Sparse coding.\\n\\n    Notes\\n    -----\\n    In `fit`, once the best parameter `alpha` is found through\\n    cross-validation, the model is fit again using the entire training set.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.linear_model import LarsCV\\n    >>> from sklearn.datasets import make_regression\\n    >>> X, y = make_regression(n_samples=200, noise=4.0, random_state=0)\\n    >>> reg = LarsCV(cv=5, normalize=False).fit(X, y)\\n    >>> reg.score(X, y)\\n    0.9996...\\n    >>> reg.alpha_\\n    0.2961...\\n    >>> reg.predict(X[:1,])\\n    array([154.3996...])\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/lasso\"} \":sklearn.regression/lasso\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [11 2]:\\n\\n|          :name |   :default |\\n|----------------|------------|\\n|     :normalize | deprecated |\\n|      :positive |      false |\\n|           :tol |  0.0001000 |\\n|      :max-iter |       1000 |\\n|  :random-state |            |\\n|        :copy-x |       true |\\n|    :precompute |      false |\\n| :fit-intercept |       true |\\n|         :alpha |      1.000 |\\n|    :warm-start |      false |\\n|     :selection |     cyclic |\\n\"]]] [:span [:p/markdown \"Linear Model trained with L1 prior as regularizer (aka the Lasso).\\n\\n    The optimization objective for Lasso is\\n\\n```python\\n(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\\n\\nnically the Lasso model is optimizing the same objective function as\\nElastic Net with ``l1_ratio=1.0`` (no L2 penalty).\\n\\n more in the :ref:`User Guide <lasso>`.\\n\\nmeters\\n------\\na : float, default=1.0\\nConstant that multiplies the L1 term, controlling regularization\\nstrength. `alpha` must be a non-negative float i.e. in `[0, inf)`.\\n\\nWhen `alpha = 0`, the objective is equivalent to ordinary least\\nsquares, solved by the :class:`LinearRegression` object. For numerical\\nreasons, using `alpha = 0` with the `Lasso` object is not advised.\\nInstead, you should use the :class:`LinearRegression` object.\\n\\nintercept : bool, default=True\\nWhether to calculate the intercept for this model. If set\\nto False, no intercept will be used in calculations\\n(i.e. data is expected to be centered).\\n\\nalize : bool, default=False\\nThis parameter is ignored when ``fit_intercept`` is set to False.\\nIf True, the regressors X will be normalized before regression by\\nsubtracting the mean and dividing by the l2-norm.\\nIf you wish to standardize, please use\\n:class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\\non an estimator with ``normalize=False``.\\n\\n.. deprecated:: 1.0\\n    ``normalize`` was deprecated in version 1.0 and will be removed in\\n    1.2.\\n\\nompute : bool or array-like of shape (n_features, n_features),                 default=False\\nWhether to use a precomputed Gram matrix to speed up\\ncalculations. The Gram matrix can also be passed as argument.\\nFor sparse input this option is always ``False`` to preserve sparsity.\\n\\n_X : bool, default=True\\nIf ``True``, X will be copied; else, it may be overwritten.\\n\\niter : int, default=1000\\nThe maximum number of iterations.\\n\\n: float, default=1e-4\\nThe tolerance for the optimization: if the updates are\\nsmaller than ``tol``, the optimization code checks the\\ndual gap for optimality and continues until it is smaller\\nthan ``tol``, see Notes below.\\n\\n_start : bool, default=False\\nWhen set to True, reuse the solution of the previous call to fit as\\ninitialization, otherwise, just erase the previous solution.\\nSee :term:`the Glossary <warm_start>`.\\n\\ntive : bool, default=False\\nWhen set to ``True``, forces the coefficients to be positive.\\n\\nom_state : int, RandomState instance, default=None\\nThe seed of the pseudo random number generator that selects a random\\nfeature to update. Used when ``selection`` == 'random'.\\nPass an int for reproducible output across multiple function calls.\\nSee :term:`Glossary <random_state>`.\\n\\nction : {'cyclic', 'random'}, default='cyclic'\\nIf set to 'random', a random coefficient is updated every iteration\\nrather than looping over features sequentially by default. This\\n(setting to 'random') often leads to significantly faster convergence\\nespecially when tol is higher than 1e-4.\\n\\nibutes\\n------\\n_ : ndarray of shape (n_features,) or (n_targets, n_features)\\nParameter vector (w in the cost function formula).\\n\\n_gap_ : float or ndarray of shape (n_targets,)\\nGiven param alpha, the dual gaps at the end of the optimization,\\nsame shape as each observation of y.\\n\\nse_coef_ : sparse matrix of shape (n_features, 1) or             (n_targets, n_features)\\nReadonly property derived from ``coef_``.\\n\\nrcept_ : float or ndarray of shape (n_targets,)\\nIndependent term in decision function.\\n\\ner_ : int or list of int\\nNumber of iterations run by the coordinate descent solver to reach\\nthe specified tolerance.\\n\\natures_in_ : int\\nNumber of features seen during :term:`fit`.\\n\\n.. versionadded:: 0.24\\n\\nure_names_in_ : ndarray of shape (`n_features_in_`,)\\nNames of features seen during :term:`fit`. Defined only when `X`\\nhas feature names that are all strings.\\n\\n.. versionadded:: 1.0\\n\\nAlso\\n----\\n_path : Regularization path using LARS.\\no_path : Regularization path using Lasso.\\noLars : Lasso Path along the regularization parameter usingLARS algorithm.\\noCV : Lasso alpha parameter by cross-validation.\\noLarsCV : Lasso least angle parameter algorithm by cross-validation.\\narn.decomposition.sparse_encode : Sparse coding array estimator.\\n\\ns\\n-\\nalgorithm used to fit the model is coordinate descent.\\n\\nvoid unnecessary memory duplication the X argument of the fit method\\nld be directly passed as a Fortran-contiguous numpy array.\\n\\nlarization improves the conditioning of the problem and\\nces the variance of the estimates. Larger values specify stronger\\nlarization. Alpha corresponds to `1 / (2C)` in other linear\\nls such as :class:`~sklearn.linear_model.LogisticRegression` or\\nss:`~sklearn.svm.LinearSVC`. If an array is passed, penalties are\\nmed to be specific to the targets. Hence they must correspond in\\ner.\\n\\nprecise stopping criteria based on `tol` are the following: First, check that\\n maximum coordinate update, i.e. :math:`\\\\max_j |w_j^{new} - w_j^{old}|`\\nmaller than `tol` times the maximum absolute coefficient, :math:`\\\\max_j |w_j|`.\\no, then additionally check whether the dual gap is smaller than `tol` times\\nh:`||y||_2^2 / n_{\\text{samples}}`.\\n\\nples\\n----\\nfrom sklearn import linear_model\\nclf = linear_model.Lasso(alpha=0.1)\\nclf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\\no(alpha=0.1)\\nprint(clf.coef_)\\n5 0.  ]\\nprint(clf.intercept_)\\n...\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/lasso-cv\"} \":sklearn.regression/lasso-cv\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [15 2]:\\n\\n|          :name |   :default |\\n|----------------|------------|\\n|     :normalize | deprecated |\\n|      :positive |      false |\\n|           :tol |  0.0001000 |\\n|      :n-alphas |        100 |\\n|           :eps |   0.001000 |\\n|        :alphas |            |\\n|      :max-iter |       1000 |\\n|        :n-jobs |            |\\n|  :random-state |            |\\n|        :copy-x |       true |\\n|    :precompute |       auto |\\n| :fit-intercept |       true |\\n|            :cv |            |\\n|     :selection |     cyclic |\\n|       :verbose |      false |\\n\"]]] [:span [:p/markdown \"Lasso linear model with iterative fitting along a regularization path.\\n\\n    See glossary entry for `cross-validation estimator`.\\n\\n    The best model is selected by cross-validation.\\n\\n    The optimization objective for Lasso is\\n\\n```python\\n(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\\n\\n more in the :ref:`User Guide <lasso>`.\\n\\nmeters\\n------\\n: float, default=1e-3\\nLength of the path. ``eps=1e-3`` means that\\n``alpha_min / alpha_max = 1e-3``.\\n\\nphas : int, default=100\\nNumber of alphas along the regularization path.\\n\\nas : ndarray, default=None\\nList of alphas where to compute the models.\\nIf ``None`` alphas are set automatically.\\n\\nintercept : bool, default=True\\nWhether to calculate the intercept for this model. If set\\nto false, no intercept will be used in calculations\\n(i.e. data is expected to be centered).\\n\\nalize : bool, default=False\\nThis parameter is ignored when ``fit_intercept`` is set to False.\\nIf True, the regressors X will be normalized before regression by\\nsubtracting the mean and dividing by the l2-norm.\\nIf you wish to standardize, please use\\n:class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\\non an estimator with ``normalize=False``.\\n\\n.. deprecated:: 1.0\\n    ``normalize`` was deprecated in version 1.0 and will be removed in\\n    1.2.\\n\\nompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\\nWhether to use a precomputed Gram matrix to speed up\\ncalculations. If set to ``'auto'`` let us decide. The Gram\\nmatrix can also be passed as argument.\\n\\niter : int, default=1000\\nThe maximum number of iterations.\\n\\n: float, default=1e-4\\nThe tolerance for the optimization: if the updates are\\nsmaller than ``tol``, the optimization code checks the\\ndual gap for optimality and continues until it is smaller\\nthan ``tol``.\\n\\n_X : bool, default=True\\nIf ``True``, X will be copied; else, it may be overwritten.\\n\\n int, cross-validation generator or iterable, default=None\\nDetermines the cross-validation splitting strategy.\\nPossible inputs for cv are:\\n\\n- None, to use the default 5-fold cross-validation,\\n- int, to specify the number of folds.\\n- :term:`CV splitter`,\\n- An iterable yielding (train, test) splits as arrays of indices.\\n\\nFor int/None inputs, :class:`KFold` is used.\\n\\nRefer :ref:`User Guide <cross_validation>` for the various\\ncross-validation strategies that can be used here.\\n\\n.. versionchanged:: 0.22\\n    ``cv`` default value if None changed from 3-fold to 5-fold.\\n\\nose : bool or int, default=False\\nAmount of verbosity.\\n\\nbs : int, default=None\\nNumber of CPUs to use during the cross validation.\\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\\nfor more details.\\n\\ntive : bool, default=False\\nIf positive, restrict regression coefficients to be positive.\\n\\nom_state : int, RandomState instance, default=None\\nThe seed of the pseudo random number generator that selects a random\\nfeature to update. Used when ``selection`` == 'random'.\\nPass an int for reproducible output across multiple function calls.\\nSee :term:`Glossary <random_state>`.\\n\\nction : {'cyclic', 'random'}, default='cyclic'\\nIf set to 'random', a random coefficient is updated every iteration\\nrather than looping over features sequentially by default. This\\n(setting to 'random') often leads to significantly faster convergence\\nespecially when tol is higher than 1e-4.\\n\\nibutes\\n------\\na_ : float\\nThe amount of penalization chosen by cross validation.\\n\\n_ : ndarray of shape (n_features,) or (n_targets, n_features)\\nParameter vector (w in the cost function formula).\\n\\nrcept_ : float or ndarray of shape (n_targets,)\\nIndependent term in decision function.\\n\\npath_ : ndarray of shape (n_alphas, n_folds)\\nMean square error for the test set on each fold, varying alpha.\\n\\nas_ : ndarray of shape (n_alphas,)\\nThe grid of alphas used for fitting.\\n\\n_gap_ : float or ndarray of shape (n_targets,)\\nThe dual gap at the end of the optimization for the optimal alpha\\n(``alpha_``).\\n\\ner_ : int\\nNumber of iterations run by the coordinate descent solver to reach\\nthe specified tolerance for the optimal alpha.\\n\\natures_in_ : int\\nNumber of features seen during :term:`fit`.\\n\\n.. versionadded:: 0.24\\n\\nure_names_in_ : ndarray of shape (`n_features_in_`,)\\nNames of features seen during :term:`fit`. Defined only when `X`\\nhas feature names that are all strings.\\n\\n.. versionadded:: 1.0\\n\\nAlso\\n----\\n_path : Compute Least Angle Regression or Lasso path using LARS\\nalgorithm.\\no_path : Compute Lasso path with coordinate descent.\\no : The Lasso is a linear model that estimates sparse coefficients.\\noLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\\noCV : Lasso linear model with iterative fitting along a regularization\\npath.\\noLarsCV : Cross-validated Lasso using the LARS algorithm.\\n\\ns\\n-\\nfit`, once the best parameter `alpha` is found through\\ns-validation, the model is fit again using the entire training set.\\n\\nvoid unnecessary memory duplication the `X` argument of the `fit`\\nod should be directly passed as a Fortran-contiguous numpy array.\\n\\n an example, see\\nf:`examples/linear_model/plot_lasso_model_selection.py\\nhx_glr_auto_examples_linear_model_plot_lasso_model_selection.py>`.\\n\\nples\\n----\\nfrom sklearn.linear_model import LassoCV\\nfrom sklearn.datasets import make_regression\\nX, y = make_regression(noise=4, random_state=0)\\nreg = LassoCV(cv=5, random_state=0).fit(X, y)\\nreg.score(X, y)\\n93...\\nreg.predict(X[:1,])\\ny([-78.4951...])\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/lasso-lars\"} \":sklearn.regression/lasso-lars\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [12 2]:\\n\\n|          :name |   :default |\\n|----------------|------------|\\n|     :normalize | deprecated |\\n|      :positive |      false |\\n|      :fit-path |       true |\\n|           :eps |  2.220E-16 |\\n|      :max-iter |        500 |\\n|  :random-state |            |\\n|        :jitter |            |\\n|        :copy-x |       true |\\n|    :precompute |       auto |\\n| :fit-intercept |       true |\\n|         :alpha |      1.000 |\\n|       :verbose |      false |\\n\"]]] [:span [:p/markdown \"Lasso model fit with Least Angle Regression a.k.a. Lars.\\n\\n    It is a Linear Model trained with an L1 prior as regularizer.\\n\\n    The optimization objective for Lasso is\\n\\n```python\\n(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\\n\\nRead more in the :ref:`User Guide <least_angle_regression>`.\\n\\nParameters\\n----------\\nalpha : float, default=1.0\\n    Constant that multiplies the penalty term. Defaults to 1.0.\\n    ``alpha = 0`` is equivalent to an ordinary least square, solved\\n    by :class:`LinearRegression`. For numerical reasons, using\\n    ``alpha = 0`` with the LassoLars object is not advised and you\\n    should prefer the LinearRegression object.\\n\\nfit_intercept : bool, default=True\\n    Whether to calculate the intercept for this model. If set\\n    to false, no intercept will be used in calculations\\n    (i.e. data is expected to be centered).\\n\\nverbose : bool or int, default=False\\n    Sets the verbosity amount.\\n\\nnormalize : bool, default=True\\n    This parameter is ignored when ``fit_intercept`` is set to False.\\n    If True, the regressors X will be normalized before regression by\\n    subtracting the mean and dividing by the l2-norm.\\n    If you wish to standardize, please use\\n    :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\\n    on an estimator with ``normalize=False``.\\n\\n    .. deprecated:: 1.0\\n        ``normalize`` was deprecated in version 1.0. It will default\\n        to False in 1.2 and be removed in 1.4.\\n\\nprecompute : bool, 'auto' or array-like, default='auto'\\n    Whether to use a precomputed Gram matrix to speed up\\n    calculations. If set to ``'auto'`` let us decide. The Gram\\n    matrix can also be passed as argument.\\n\\nmax_iter : int, default=500\\n    Maximum number of iterations to perform.\\n\\neps : float, default=np.finfo(float).eps\\n    The machine-precision regularization in the computation of the\\n    Cholesky diagonal factors. Increase this for very ill-conditioned\\n    systems. Unlike the ``tol`` parameter in some iterative\\n    optimization-based algorithms, this parameter does not control\\n    the tolerance of the optimization.\\n\\ncopy_X : bool, default=True\\n    If True, X will be copied; else, it may be overwritten.\\n\\nfit_path : bool, default=True\\n    If ``True`` the full path is stored in the ``coef_path_`` attribute.\\n    If you compute the solution for a large problem or many targets,\\n    setting ``fit_path`` to ``False`` will lead to a speedup, especially\\n    with a small alpha.\\n\\npositive : bool, default=False\\n    Restrict coefficients to be >= 0. Be aware that you might want to\\n    remove fit_intercept which is set True by default.\\n    Under the positive restriction the model coefficients will not converge\\n    to the ordinary-least-squares solution for small values of alpha.\\n    Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\\n    0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\\n    algorithm are typically in congruence with the solution of the\\n    coordinate descent Lasso estimator.\\n\\njitter : float, default=None\\n    Upper bound on a uniform noise parameter to be added to the\\n    `y` values, to satisfy the model's assumption of\\n    one-at-a-time computations. Might help with stability.\\n\\n    .. versionadded:: 0.23\\n\\nrandom_state : int, RandomState instance or None, default=None\\n    Determines random number generation for jittering. Pass an int\\n    for reproducible output across multiple function calls.\\n    See :term:`Glossary <random_state>`. Ignored if `jitter` is None.\\n\\n    .. versionadded:: 0.23\\n\\nAttributes\\n----------\\nalphas_ : array-like of shape (n_alphas + 1,) or list of such arrays\\n    Maximum of covariances (in absolute value) at each iteration.\\n    ``n_alphas`` is either ``max_iter``, ``n_features`` or the\\n    number of nodes in the path with ``alpha >= alpha_min``, whichever\\n    is smaller. If this is a list of array-like, the length of the outer\\n    list is `n_targets`.\\n\\nactive_ : list of length n_alphas or list of such lists\\n    Indices of active variables at the end of the path.\\n    If this is a list of list, the length of the outer list is `n_targets`.\\n\\ncoef_path_ : array-like of shape (n_features, n_alphas + 1) or list             of such arrays\\n    If a list is passed it's expected to be one of n_targets such arrays.\\n    The varying values of the coefficients along the path. It is not\\n    present if the ``fit_path`` parameter is ``False``. If this is a list\\n    of array-like, the length of the outer list is `n_targets`.\\n\\ncoef_ : array-like of shape (n_features,) or (n_targets, n_features)\\n    Parameter vector (w in the formulation formula).\\n\\nintercept_ : float or array-like of shape (n_targets,)\\n    Independent term in decision function.\\n\\nn_iter_ : array-like or int\\n    The number of iterations taken by lars_path to find the\\n    grid of alphas for each target.\\n\\nn_features_in_ : int\\n    Number of features seen during :term:`fit`.\\n\\n    .. versionadded:: 0.24\\n\\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\\n    Names of features seen during :term:`fit`. Defined only when `X`\\n    has feature names that are all strings.\\n\\n    .. versionadded:: 1.0\\n\\nSee Also\\n--------\\nlars_path : Compute Least Angle Regression or Lasso\\n    path using LARS algorithm.\\nlasso_path : Compute Lasso path with coordinate descent.\\nLasso : Linear Model trained with L1 prior as\\n    regularizer (aka the Lasso).\\nLassoCV : Lasso linear model with iterative fitting\\n    along a regularization path.\\nLassoLarsCV: Cross-validated Lasso, using the LARS algorithm.\\nLassoLarsIC : Lasso model fit with Lars using BIC\\n    or AIC for model selection.\\nsklearn.decomposition.sparse_encode : Sparse coding.\\n\\nExamples\\n--------\\n>>> from sklearn import linear_model\\n>>> reg = linear_model.LassoLars(alpha=0.01, normalize=False)\\n>>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1, 0, -1])\\nLassoLars(alpha=0.01, normalize=False)\\n>>> print(reg.coef_)\\n[ 0.         -0.955...]\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/lasso-lars-cv\"} \":sklearn.regression/lasso-lars-cv\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [11 2]:\\n\\n|          :name |   :default |\\n|----------------|------------|\\n|     :normalize | deprecated |\\n|      :positive |      false |\\n|           :eps |  2.220E-16 |\\n|  :max-n-alphas |       1000 |\\n|      :max-iter |        500 |\\n|        :n-jobs |            |\\n|        :copy-x |       true |\\n|    :precompute |       auto |\\n| :fit-intercept |       true |\\n|            :cv |            |\\n|       :verbose |      false |\\n\"]]] [:span [:p/markdown \"Cross-validated Lasso, using the LARS algorithm.\\n\\n    See glossary entry for `cross-validation estimator`.\\n\\n    The optimization objective for Lasso is\\n\\n```python\\n(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\\n\\nRead more in the :ref:`User Guide <least_angle_regression>`.\\n\\nParameters\\n----------\\nfit_intercept : bool, default=True\\n    Whether to calculate the intercept for this model. If set\\n    to false, no intercept will be used in calculations\\n    (i.e. data is expected to be centered).\\n\\nverbose : bool or int, default=False\\n    Sets the verbosity amount.\\n\\nmax_iter : int, default=500\\n    Maximum number of iterations to perform.\\n\\nnormalize : bool, default=True\\n    This parameter is ignored when ``fit_intercept`` is set to False.\\n    If True, the regressors X will be normalized before regression by\\n    subtracting the mean and dividing by the l2-norm.\\n    If you wish to standardize, please use\\n    :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\\n    on an estimator with ``normalize=False``.\\n\\n    .. deprecated:: 1.0\\n        ``normalize`` was deprecated in version 1.0. It will default\\n        to False in 1.2 and be removed in 1.4.\\n\\nprecompute : bool or 'auto' , default='auto'\\n    Whether to use a precomputed Gram matrix to speed up\\n    calculations. If set to ``'auto'`` let us decide. The Gram matrix\\n    cannot be passed as argument since we will use only subsets of X.\\n\\ncv : int, cross-validation generator or an iterable, default=None\\n    Determines the cross-validation splitting strategy.\\n    Possible inputs for cv are:\\n\\n    - None, to use the default 5-fold cross-validation,\\n    - integer, to specify the number of folds.\\n    - :term:`CV splitter`,\\n    - An iterable yielding (train, test) splits as arrays of indices.\\n\\n    For integer/None inputs, :class:`KFold` is used.\\n\\n    Refer :ref:`User Guide <cross_validation>` for the various\\n    cross-validation strategies that can be used here.\\n\\n    .. versionchanged:: 0.22\\n        ``cv`` default value if None changed from 3-fold to 5-fold.\\n\\nmax_n_alphas : int, default=1000\\n    The maximum number of points on the path used to compute the\\n    residuals in the cross-validation.\\n\\nn_jobs : int or None, default=None\\n    Number of CPUs to use during the cross validation.\\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\\n    for more details.\\n\\neps : float, default=np.finfo(float).eps\\n    The machine-precision regularization in the computation of the\\n    Cholesky diagonal factors. Increase this for very ill-conditioned\\n    systems. Unlike the ``tol`` parameter in some iterative\\n    optimization-based algorithms, this parameter does not control\\n    the tolerance of the optimization.\\n\\ncopy_X : bool, default=True\\n    If True, X will be copied; else, it may be overwritten.\\n\\npositive : bool, default=False\\n    Restrict coefficients to be >= 0. Be aware that you might want to\\n    remove fit_intercept which is set True by default.\\n    Under the positive restriction the model coefficients do not converge\\n    to the ordinary-least-squares solution for small values of alpha.\\n    Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\\n    0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\\n    algorithm are typically in congruence with the solution of the\\n    coordinate descent Lasso estimator.\\n    As a consequence using LassoLarsCV only makes sense for problems where\\n    a sparse solution is expected and/or reached.\\n\\nAttributes\\n----------\\ncoef_ : array-like of shape (n_features,)\\n    parameter vector (w in the formulation formula)\\n\\nintercept_ : float\\n    independent term in decision function.\\n\\ncoef_path_ : array-like of shape (n_features, n_alphas)\\n    the varying values of the coefficients along the path\\n\\nalpha_ : float\\n    the estimated regularization parameter alpha\\n\\nalphas_ : array-like of shape (n_alphas,)\\n    the different values of alpha along the path\\n\\ncv_alphas_ : array-like of shape (n_cv_alphas,)\\n    all the values of alpha along the path for the different folds\\n\\nmse_path_ : array-like of shape (n_folds, n_cv_alphas)\\n    the mean square error on left-out for each fold along the path\\n    (alpha values given by ``cv_alphas``)\\n\\nn_iter_ : array-like or int\\n    the number of iterations run by Lars with the optimal alpha.\\n\\nactive_ : list of int\\n    Indices of active variables at the end of the path.\\n\\nn_features_in_ : int\\n    Number of features seen during :term:`fit`.\\n\\n    .. versionadded:: 0.24\\n\\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\\n    Names of features seen during :term:`fit`. Defined only when `X`\\n    has feature names that are all strings.\\n\\n    .. versionadded:: 1.0\\n\\nSee Also\\n--------\\nlars_path : Compute Least Angle Regression or Lasso\\n    path using LARS algorithm.\\nlasso_path : Compute Lasso path with coordinate descent.\\nLasso : Linear Model trained with L1 prior as\\n    regularizer (aka the Lasso).\\nLassoCV : Lasso linear model with iterative fitting\\n    along a regularization path.\\nLassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\\nLassoLarsIC : Lasso model fit with Lars using BIC\\n    or AIC for model selection.\\nsklearn.decomposition.sparse_encode : Sparse coding.\\n\\nNotes\\n-----\\nThe object solves the same problem as the\\n:class:`~sklearn.linear_model.LassoCV` object. However, unlike the\\n:class:`~sklearn.linear_model.LassoCV`, it find the relevant alphas values\\nby itself. In general, because of this property, it will be more stable.\\nHowever, it is more fragile to heavily multicollinear datasets.\\n\\nIt is more efficient than the :class:`~sklearn.linear_model.LassoCV` if\\nonly a small number of features are selected compared to the total number,\\nfor instance if there are very few samples compared to the number of\\nfeatures.\\n\\nIn `fit`, once the best parameter `alpha` is found through\\ncross-validation, the model is fit again using the entire training set.\\n\\nExamples\\n--------\\n>>> from sklearn.linear_model import LassoLarsCV\\n>>> from sklearn.datasets import make_regression\\n>>> X, y = make_regression(noise=4.0, random_state=0)\\n>>> reg = LassoLarsCV(cv=5, normalize=False).fit(X, y)\\n>>> reg.score(X, y)\\n0.9993...\\n>>> reg.alpha_\\n0.3972...\\n>>> reg.predict(X[:1,])\\narray([-78.4831...])\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/lasso-lars-ic\"} \":sklearn.regression/lasso-lars-ic\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [10 2]:\\n\\n|           :name |   :default |\\n|-----------------|------------|\\n|      :normalize | deprecated |\\n|       :positive |      false |\\n|            :eps |  2.220E-16 |\\n| :noise-variance |            |\\n|       :max-iter |        500 |\\n|         :copy-x |       true |\\n|     :precompute |       auto |\\n|  :fit-intercept |       true |\\n|      :criterion |        aic |\\n|        :verbose |      false |\\n\"]]] [:span [:p/markdown \"Lasso model fit with Lars using BIC or AIC for model selection.\\n\\n    The optimization objective for Lasso is\\n\\n```python\\n(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\\n\\nAIC is the Akaike information criterion [2]_ and BIC is the Bayes\\nInformation criterion [3]_. Such criteria are useful to select the value\\nof the regularization parameter by making a trade-off between the\\ngoodness of fit and the complexity of the model. A good model should\\nexplain well the data while being simple.\\n\\nRead more in the :ref:`User Guide <lasso_lars_ic>`.\\n\\nParameters\\n----------\\ncriterion : {'aic', 'bic'}, default='aic'\\n    The type of criterion to use.\\n\\nfit_intercept : bool, default=True\\n    Whether to calculate the intercept for this model. If set\\n    to false, no intercept will be used in calculations\\n    (i.e. data is expected to be centered).\\n\\nverbose : bool or int, default=False\\n    Sets the verbosity amount.\\n\\nnormalize : bool, default=True\\n    This parameter is ignored when ``fit_intercept`` is set to False.\\n    If True, the regressors X will be normalized before regression by\\n    subtracting the mean and dividing by the l2-norm.\\n    If you wish to standardize, please use\\n    :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\\n    on an estimator with ``normalize=False``.\\n\\n    .. deprecated:: 1.0\\n        ``normalize`` was deprecated in version 1.0. It will default\\n        to False in 1.2 and be removed in 1.4.\\n\\nprecompute : bool, 'auto' or array-like, default='auto'\\n    Whether to use a precomputed Gram matrix to speed up\\n    calculations. If set to ``'auto'`` let us decide. The Gram\\n    matrix can also be passed as argument.\\n\\nmax_iter : int, default=500\\n    Maximum number of iterations to perform. Can be used for\\n    early stopping.\\n\\neps : float, default=np.finfo(float).eps\\n    The machine-precision regularization in the computation of the\\n    Cholesky diagonal factors. Increase this for very ill-conditioned\\n    systems. Unlike the ``tol`` parameter in some iterative\\n    optimization-based algorithms, this parameter does not control\\n    the tolerance of the optimization.\\n\\ncopy_X : bool, default=True\\n    If True, X will be copied; else, it may be overwritten.\\n\\npositive : bool, default=False\\n    Restrict coefficients to be >= 0. Be aware that you might want to\\n    remove fit_intercept which is set True by default.\\n    Under the positive restriction the model coefficients do not converge\\n    to the ordinary-least-squares solution for small values of alpha.\\n    Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\\n    0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\\n    algorithm are typically in congruence with the solution of the\\n    coordinate descent Lasso estimator.\\n    As a consequence using LassoLarsIC only makes sense for problems where\\n    a sparse solution is expected and/or reached.\\n\\nnoise_variance : float, default=None\\n    The estimated noise variance of the data. If `None`, an unbiased\\n    estimate is computed by an OLS model. However, it is only possible\\n    in the case where `n_samples > n_features + fit_intercept`.\\n\\n    .. versionadded:: 1.1\\n\\nAttributes\\n----------\\ncoef_ : array-like of shape (n_features,)\\n    parameter vector (w in the formulation formula)\\n\\nintercept_ : float\\n    independent term in decision function.\\n\\nalpha_ : float\\n    the alpha parameter chosen by the information criterion\\n\\nalphas_ : array-like of shape (n_alphas + 1,) or list of such arrays\\n    Maximum of covariances (in absolute value) at each iteration.\\n    ``n_alphas`` is either ``max_iter``, ``n_features`` or the\\n    number of nodes in the path with ``alpha >= alpha_min``, whichever\\n    is smaller. If a list, it will be of length `n_targets`.\\n\\nn_iter_ : int\\n    number of iterations run by lars_path to find the grid of\\n    alphas.\\n\\ncriterion_ : array-like of shape (n_alphas,)\\n    The value of the information criteria ('aic', 'bic') across all\\n    alphas. The alpha which has the smallest information criterion is\\n    chosen, as specified in [1]_.\\n\\nnoise_variance_ : float\\n    The estimated noise variance from the data used to compute the\\n    criterion.\\n\\n    .. versionadded:: 1.1\\n\\nn_features_in_ : int\\n    Number of features seen during :term:`fit`.\\n\\n    .. versionadded:: 0.24\\n\\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\\n    Names of features seen during :term:`fit`. Defined only when `X`\\n    has feature names that are all strings.\\n\\n    .. versionadded:: 1.0\\n\\nSee Also\\n--------\\nlars_path : Compute Least Angle Regression or Lasso\\n    path using LARS algorithm.\\nlasso_path : Compute Lasso path with coordinate descent.\\nLasso : Linear Model trained with L1 prior as\\n    regularizer (aka the Lasso).\\nLassoCV : Lasso linear model with iterative fitting\\n    along a regularization path.\\nLassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\\nLassoLarsCV: Cross-validated Lasso, using the LARS algorithm.\\nsklearn.decomposition.sparse_encode : Sparse coding.\\n\\nNotes\\n-----\\nThe number of degrees of freedom is computed as in [1]_.\\n\\nTo have more details regarding the mathematical formulation of the\\nAIC and BIC criteria, please refer to :ref:`User Guide <lasso_lars_ic>`.\\n\\nReferences\\n----------\\n.. [1] :arxiv:`Zou, Hui, Trevor Hastie, and Robert Tibshirani.\\n        \\\"On the degrees of freedom of the lasso.\\\"\\n        The Annals of Statistics 35.5 (2007): 2173-2192.\\n        <0712.0881>`\\n\\n.. [2] `Wikipedia entry on the Akaike information criterion\\n        <https://en.wikipedia.org/wiki/Akaike_information_criterion>`_\\n\\n.. [3] `Wikipedia entry on the Bayesian information criterion\\n        <https://en.wikipedia.org/wiki/Bayesian_information_criterion>`_\\n\\nExamples\\n--------\\n>>> from sklearn import linear_model\\n>>> reg = linear_model.LassoLarsIC(criterion='bic', normalize=False)\\n>>> X = [[-2, 2], [-1, 1], [0, 0], [1, 1], [2, 2]]\\n>>> y = [-2.2222, -1.1111, 0, -1.1111, -2.2222]\\n>>> reg.fit(X, y)\\nLassoLarsIC(criterion='bic', normalize=False)\\n>>> print(reg.coef_)\\n[ 0.  -1.11...]\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/linear-regression\"} \":sklearn.regression/linear-regression\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [5 2]:\\n\\n|          :name |   :default |\\n|----------------|------------|\\n|        :copy-x |       true |\\n| :fit-intercept |       true |\\n|        :n-jobs |            |\\n|     :normalize | deprecated |\\n|      :positive |      false |\\n\"]]] [:span [:p/markdown \"\\n    Ordinary least squares Linear Regression.\\n\\n    LinearRegression fits a linear model with coefficients w = (w1, ..., wp)\\n    to minimize the residual sum of squares between the observed targets in\\n    the dataset, and the targets predicted by the linear approximation.\\n\\n    Parameters\\n    ----------\\n    - `fit_intercept`: bool, default=True\\n        Whether to calculate the intercept for this model. If set\\n        to False, no intercept will be used in calculations\\n        (i.e. data is expected to be centered).\\n\\n    - `normalize`: bool, default=False\\n        This parameter is ignored when ``fit_intercept`` is set to False.\\n        If True, the regressors X will be normalized before regression by\\n        subtracting the mean and dividing by the l2-norm.\\n        If you wish to standardize, please use\\n        `~sklearn.preprocessing.StandardScaler` before calling ``fit``\\n        on an estimator with ``normalize=False``.\\n\\n        *Deprecated since 1.0*\\n           `normalize` was deprecated in version 1.0 and will be\\n           removed in 1.2.\\n\\n    - `copy_X`: bool, default=True\\n        If True, X will be copied; else, it may be overwritten.\\n\\n    - `n_jobs`: int, default=None\\n        The number of jobs to use for the computation. This will only provide\\n        speedup in case of sufficiently large problems, that is if firstly\\n        `n_targets > 1` and secondly `X` is sparse or if `positive` is set\\n        to `True`. ``None`` means 1 unless in a\\n        `joblib.parallel_backend` context. ``-1`` means using all\\n        processors. See `Glossary <n_jobs>` for more details.\\n\\n    - `positive`: bool, default=False\\n        When set to ``True``, forces the coefficients to be positive. This\\n        option is only supported for dense arrays.\\n\\n        *Added in 0.24*\\n\\n    Attributes\\n    ----------\\n    - `coef_`: array of shape (n_features, ) or (n_targets, n_features)\\n        Estimated coefficients for the linear regression problem.\\n        If multiple targets are passed during the fit (y 2D), this\\n        is a 2D array of shape (n_targets, n_features), while if only\\n        one target is passed, this is a 1D array of length n_features.\\n\\n    - `rank_`: int\\n        Rank of matrix `X`. Only available when `X` is dense.\\n\\n    - `singular_`: array of shape (min(X, y),)\\n        Singular values of `X`. Only available when `X` is dense.\\n\\n    - `intercept_`: float or array of shape (n_targets,)\\n        Independent term in the linear model. Set to 0.0 if\\n        `fit_intercept = False`.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    See Also\\n    --------\\n    - `Ridge`: Ridge regression addresses some of the\\n        problems of Ordinary Least Squares by imposing a penalty on the\\n        size of the coefficients with l2 regularization.\\n    - `Lasso`: The Lasso is a linear model that estimates\\n        sparse coefficients with l1 regularization.\\n    - `ElasticNet`: Elastic-Net is a linear regression\\n        model trained with both l1 and l2 -norm regularization of the\\n        coefficients.\\n\\n    Notes\\n    -----\\n    From the implementation point of view, this is just plain Ordinary\\n    Least Squares (scipy.linalg.lstsq) or Non Negative Least Squares\\n    (scipy.optimize.nnls) wrapped as a predictor object.\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> from sklearn.linear_model import LinearRegression\\n    >>> X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\\n    >>> # y = 1 * x_0 + 2 * x_1 + 3\\n    >>> y = np.dot(X, np.array([1, 2])) + 3\\n    >>> reg = LinearRegression().fit(X, y)\\n    >>> reg.score(X, y)\\n    1.0\\n    >>> reg.coef_\\n    array([1., 2.])\\n    >>> reg.intercept_\\n    3.0...\\n    >>> reg.predict(np.array([[3, 5]]))\\n    array([16.])\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/linear-svr\"} \":sklearn.regression/linear-svr\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [10 2]:\\n\\n|              :name |            :default |\\n|--------------------|---------------------|\\n|               :tol |           0.0001000 |\\n| :intercept-scaling |               1.000 |\\n|                 :c |               1.000 |\\n|          :max-iter |                1000 |\\n|      :random-state |                     |\\n|              :dual |                true |\\n|     :fit-intercept |                true |\\n|              :loss | epsilon_insensitive |\\n|           :verbose |                   0 |\\n|           :epsilon |               0.000 |\\n\"]]] [:span [:p/markdown \"Linear Support Vector Regression.\\n\\n    Similar to SVR with parameter kernel='linear', but implemented in terms of\\n    liblinear rather than libsvm, so it has more flexibility in the choice of\\n    penalties and loss functions and should scale better to large numbers of\\n    samples.\\n\\n    This class supports both dense and sparse input.\\n\\n    Read more in the User Guide: `svm_regression`.\\n\\n    *Added in 0.16*\\n\\n    Parameters\\n    ----------\\n    - `epsilon`: float, default=0.0\\n        Epsilon parameter in the epsilon-insensitive loss function. Note\\n        that the value of this parameter depends on the scale of the target\\n        variable y. If unsure, set ``epsilon=0``.\\n\\n    - `tol`: float, default=1e-4\\n        Tolerance for stopping criteria.\\n\\n    - `C`: float, default=1.0\\n        Regularization parameter. The strength of the regularization is\\n        inversely proportional to C. Must be strictly positive.\\n\\n    - `loss`: {'epsilon_insensitive', 'squared_epsilon_insensitive'},             default='epsilon_insensitive'\\n        Specifies the loss function. The epsilon-insensitive loss\\n        (standard SVR) is the L1 loss, while the squared epsilon-insensitive\\n        loss ('squared_epsilon_insensitive') is the L2 loss.\\n\\n    - `fit_intercept`: bool, default=True\\n        Whether to calculate the intercept for this model. If set\\n        to false, no intercept will be used in calculations\\n        (i.e. data is expected to be already centered).\\n\\n    - `intercept_scaling`: float, default=1.0\\n        When self.fit_intercept is True, instance vector x becomes\\n        [x, self.intercept_scaling],\\n        i.e. a \\\"synthetic\\\" feature with constant value equals to\\n        intercept_scaling is appended to the instance vector.\\n        The intercept becomes intercept_scaling * synthetic feature weight\\n        Note! the synthetic feature weight is subject to l1/l2 regularization\\n        as all other features.\\n        To lessen the effect of regularization on synthetic feature weight\\n        (and therefore on the intercept) intercept_scaling has to be increased.\\n\\n    - `dual`: bool, default=True\\n        Select the algorithm to either solve the dual or primal\\n        optimization problem. Prefer dual=False when n_samples > n_features.\\n\\n    - `verbose`: int, default=0\\n        Enable verbose output. Note that this setting takes advantage of a\\n        per-process runtime setting in liblinear that, if enabled, may not work\\n        properly in a multithreaded context.\\n\\n    - `random_state`: int, RandomState instance or None, default=None\\n        Controls the pseudo random number generation for shuffling the data.\\n        Pass an int for reproducible output across multiple function calls.\\n        See `Glossary <random_state>`.\\n\\n    - `max_iter`: int, default=1000\\n        The maximum number of iterations to be run.\\n\\n    Attributes\\n    ----------\\n    - `coef_`: ndarray of shape (n_features) if n_classes == 2             else (n_classes, n_features)\\n        Weights assigned to the features (coefficients in the primal\\n        problem).\\n\\n        `coef_` is a readonly property derived from `raw_coef_` that\\n        follows the internal memory layout of liblinear.\\n\\n    - `intercept_`: ndarray of shape (1) if n_classes == 2 else (n_classes)\\n        Constants in decision function.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    - `n_iter_`: int\\n        Maximum number of iterations run across all classes.\\n\\n    See Also\\n    --------\\n    - `LinearSVC`: Implementation of Support Vector Machine classifier using the\\n        same library as this class (liblinear).\\n\\n    - `SVR`: Implementation of Support Vector Machine regression using libsvm:\\n        the kernel can be non-linear but its SMO algorithm does not\\n        scale to large number of samples as LinearSVC does.\\n\\n    - `sklearn.linear_model.SGDRegressor`: SGDRegressor can optimize the same cost\\n        function as LinearSVR\\n        by adjusting the penalty and loss parameters. In addition it requires\\n        less memory, allows incremental (online) learning, and implements\\n        various loss functions and regularization regimes.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.svm import LinearSVR\\n    >>> from sklearn.pipeline import make_pipeline\\n    >>> from sklearn.preprocessing import StandardScaler\\n    >>> from sklearn.datasets import make_regression\\n    >>> X, y = make_regression(n_features=4, random_state=0)\\n    >>> regr = make_pipeline(StandardScaler(),\\n    ...                      LinearSVR(random_state=0, tol=1e-5))\\n    >>> regr.fit(X, y)\\n    Pipeline(steps=[('standardscaler', StandardScaler()),\\n                    ('linearsvr', LinearSVR(random_state=0, tol=1e-05))])\\n\\n    >>> print(regr.named_steps['linearsvr'].coef_)\\n    [18.582... 27.023... 44.357... 64.522...]\\n    >>> print(regr.named_steps['linearsvr'].intercept_)\\n    [-4...]\\n    >>> print(regr.predict([[0, 0, 0, 0]]))\\n    [-2.384...]\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/mlp-regressor\"} \":sklearn.regression/mlp-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [23 2]:\\n\\n|                :name |  :default |\\n|----------------------|-----------|\\n|    :n-iter-no-change |        10 |\\n|       :learning-rate |  constant |\\n|          :activation |      relu |\\n|  :hidden-layer-sizes |     [100] |\\n|                 :tol | 0.0001000 |\\n|              :beta-2 |    0.9990 |\\n|      :early-stopping |     false |\\n|  :nesterovs-momentum |      true |\\n|          :batch-size |      auto |\\n|              :solver |      adam |\\n|                  ... |       ... |\\n|             :max-fun |     15000 |\\n|              :beta-1 |    0.9000 |\\n|            :max-iter |       200 |\\n|        :random-state |           |\\n|            :momentum |    0.9000 |\\n|  :learning-rate-init |  0.001000 |\\n|               :alpha | 0.0001000 |\\n|          :warm-start |     false |\\n| :validation-fraction |    0.1000 |\\n|             :verbose |     false |\\n|             :epsilon | 1.000E-08 |\\n\"]]] [:span [:p/markdown \"Multi-layer Perceptron regressor.\\n\\n    This model optimizes the squared error using LBFGS or stochastic gradient\\n    descent.\\n\\n    *Added in 0.18*\\n\\n    Parameters\\n    ----------\\n    - `hidden_layer_sizes`: tuple, length = n_layers - 2, default=(100,)\\n        The ith element represents the number of neurons in the ith\\n        hidden layer.\\n\\n    - `activation`: {'identity', 'logistic', 'tanh', 'relu'}, default='relu'\\n        Activation function for the hidden layer.\\n\\n        - 'identity', no-op activation, useful to implement linear bottleneck,\\n          returns f(x) = x\\n\\n        - 'logistic', the logistic sigmoid function,\\n          returns f(x) = 1 / (1 + exp(-x)).\\n\\n        - 'tanh', the hyperbolic tan function,\\n          returns f(x) = tanh(x).\\n\\n        - 'relu', the rectified linear unit function,\\n          returns f(x) = max(0, x)\\n\\n    - `solver`: {'lbfgs', 'sgd', 'adam'}, default='adam'\\n        The solver for weight optimization.\\n\\n        - 'lbfgs' is an optimizer in the family of quasi-Newton methods.\\n\\n        - 'sgd' refers to stochastic gradient descent.\\n\\n        - 'adam' refers to a stochastic gradient-based optimizer proposed by\\n          Kingma, Diederik, and Jimmy Ba\\n\\n        Note: The default solver 'adam' works pretty well on relatively\\n        large datasets (with thousands of training samples or more) in terms of\\n        both training time and validation score.\\n        For small datasets, however, 'lbfgs' can converge faster and perform\\n        better.\\n\\n    - `alpha`: float, default=0.0001\\n        Strength of the L2 regularization term. The L2 regularization term\\n        is divided by the sample size when added to the loss.\\n\\n    - `batch_size`: int, default='auto'\\n        Size of minibatches for stochastic optimizers.\\n        If the solver is 'lbfgs', the classifier will not use minibatch.\\n        When set to \\\"auto\\\", `batch_size=min(200, n_samples)`.\\n\\n    - `learning_rate`: {'constant', 'invscaling', 'adaptive'}, default='constant'\\n        Learning rate schedule for weight updates.\\n\\n        - 'constant' is a constant learning rate given by\\n          'learning_rate_init'.\\n\\n        - 'invscaling' gradually decreases the learning rate ``learning_rate_``\\n          at each time step 't' using an inverse scaling exponent of 'power_t'.\\n          effective_learning_rate = learning_rate_init / pow(t, power_t)\\n\\n        - 'adaptive' keeps the learning rate constant to\\n          'learning_rate_init' as long as training loss keeps decreasing.\\n          Each time two consecutive epochs fail to decrease training loss by at\\n          least tol, or fail to increase validation score by at least tol if\\n          'early_stopping' is on, the current learning rate is divided by 5.\\n\\n        Only used when solver='sgd'.\\n\\n    - `learning_rate_init`: float, default=0.001\\n        The initial learning rate used. It controls the step-size\\n        in updating the weights. Only used when solver='sgd' or 'adam'.\\n\\n    - `power_t`: float, default=0.5\\n        The exponent for inverse scaling learning rate.\\n        It is used in updating effective learning rate when the learning_rate\\n        is set to 'invscaling'. Only used when solver='sgd'.\\n\\n    - `max_iter`: int, default=200\\n        Maximum number of iterations. The solver iterates until convergence\\n        (determined by 'tol') or this number of iterations. For stochastic\\n        solvers ('sgd', 'adam'), note that this determines the number of epochs\\n        (how many times each data point will be used), not the number of\\n        gradient steps.\\n\\n    - `shuffle`: bool, default=True\\n        Whether to shuffle samples in each iteration. Only used when\\n        solver='sgd' or 'adam'.\\n\\n    - `random_state`: int, RandomState instance, default=None\\n        Determines random number generation for weights and bias\\n        initialization, train-test split if early stopping is used, and batch\\n        sampling when solver='sgd' or 'adam'.\\n        Pass an int for reproducible results across multiple function calls.\\n        See `Glossary <random_state>`.\\n\\n    - `tol`: float, default=1e-4\\n        Tolerance for the optimization. When the loss or score is not improving\\n        by at least ``tol`` for ``n_iter_no_change`` consecutive iterations,\\n        unless ``learning_rate`` is set to 'adaptive', convergence is\\n        considered to be reached and training stops.\\n\\n    - `verbose`: bool, default=False\\n        Whether to print progress messages to stdout.\\n\\n    - `warm_start`: bool, default=False\\n        When set to True, reuse the solution of the previous\\n        call to fit as initialization, otherwise, just erase the\\n        previous solution. See `the Glossary <warm_start>`.\\n\\n    - `momentum`: float, default=0.9\\n        Momentum for gradient descent update.  Should be between 0 and 1. Only\\n        used when solver='sgd'.\\n\\n    - `nesterovs_momentum`: bool, default=True\\n        Whether to use Nesterov's momentum. Only used when solver='sgd' and\\n        momentum > 0.\\n\\n    - `early_stopping`: bool, default=False\\n        Whether to use early stopping to terminate training when validation\\n        score is not improving. If set to true, it will automatically set\\n        aside 10% of training data as validation and terminate training when\\n        validation score is not improving by at least ``tol`` for\\n        ``n_iter_no_change`` consecutive epochs.\\n        Only effective when solver='sgd' or 'adam'.\\n\\n    - `validation_fraction`: float, default=0.1\\n        The proportion of training data to set aside as validation set for\\n        early stopping. Must be between 0 and 1.\\n        Only used if early_stopping is True.\\n\\n    - `beta_1`: float, default=0.9\\n        Exponential decay rate for estimates of first moment vector in adam,\\n        should be in [0, 1). Only used when solver='adam'.\\n\\n    - `beta_2`: float, default=0.999\\n        Exponential decay rate for estimates of second moment vector in adam,\\n        should be in [0, 1). Only used when solver='adam'.\\n\\n    - `epsilon`: float, default=1e-8\\n        Value for numerical stability in adam. Only used when solver='adam'.\\n\\n    - `n_iter_no_change`: int, default=10\\n        Maximum number of epochs to not meet ``tol`` improvement.\\n        Only effective when solver='sgd' or 'adam'.\\n\\n        *Added in 0.20*\\n\\n    - `max_fun`: int, default=15000\\n        Only used when solver='lbfgs'. Maximum number of function calls.\\n        The solver iterates until convergence (determined by 'tol'), number\\n        of iterations reaches max_iter, or this number of function calls.\\n        Note that number of function calls will be greater than or equal to\\n        the number of iterations for the MLPRegressor.\\n\\n        *Added in 0.22*\\n\\n    Attributes\\n    ----------\\n    - `loss_`: float\\n        The current loss computed with the loss function.\\n\\n    - `best_loss_`: float\\n        The minimum loss reached by the solver throughout fitting.\\n\\n    - `loss_curve_`: list of shape (`n_iter_`,)\\n        Loss value evaluated at the end of each training step.\\n        The ith element in the list represents the loss at the ith iteration.\\n\\n    - `t_`: int\\n        The number of training samples seen by the solver during fitting.\\n        Mathematically equals `n_iters * X.shape[0]`, it means\\n        `time_step` and it is used by optimizer's learning rate scheduler.\\n\\n    - `coefs_`: list of shape (n_layers - 1,)\\n        The ith element in the list represents the weight matrix corresponding\\n        to layer i.\\n\\n    - `intercepts_`: list of shape (n_layers - 1,)\\n        The ith element in the list represents the bias vector corresponding to\\n        layer i + 1.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    - `n_iter_`: int\\n        The number of iterations the solver has run.\\n\\n    - `n_layers_`: int\\n        Number of layers.\\n\\n    - `n_outputs_`: int\\n        Number of outputs.\\n\\n    - `out_activation_`: str\\n        Name of the output activation function.\\n\\n    See Also\\n    --------\\n    - `BernoulliRBM`: Bernoulli Restricted Boltzmann Machine (RBM).\\n    - `MLPClassifier`: Multi-layer Perceptron classifier.\\n    - `sklearn.linear_model.SGDRegressor`: Linear model fitted by minimizing\\n        a regularized empirical loss with SGD.\\n\\n    Notes\\n    -----\\n    MLPRegressor trains iteratively since at each time step\\n    the partial derivatives of the loss function with respect to the model\\n    parameters are computed to update the parameters.\\n\\n    It can also have a regularization term added to the loss function\\n    that shrinks model parameters to prevent overfitting.\\n\\n    This implementation works with data represented as dense and sparse numpy\\n    arrays of floating point values.\\n\\n    References\\n    ----------\\n    Hinton, Geoffrey E. \\\"Connectionist learning procedures.\\\"\\n    Artificial intelligence 40.1 (1989): 185-234.\\n\\n    Glorot, Xavier, and Yoshua Bengio.\\n    \\\"Understanding the difficulty of training deep feedforward neural networks.\\\"\\n    International Conference on Artificial Intelligence and Statistics. 2010.\\n\\n    :arxiv:`He, Kaiming, et al (2015). \\\"Delving deep into rectifiers:\\n    Surpassing human-level performance on imagenet classification.\\\" <1502.01852>`\\n\\n    :arxiv:`Kingma, Diederik, and Jimmy Ba (2014)\\n    \\\"Adam: A method for stochastic optimization.\\\" <1412.6980>`\\n\\n    Examples\\n    --------\\n    >>> from sklearn.neural_network import MLPRegressor\\n    >>> from sklearn.datasets import make_regression\\n    >>> from sklearn.model_selection import train_test_split\\n    >>> X, y = make_regression(n_samples=200, random_state=1)\\n    >>> X_train, X_test, y_train, y_test = train_test_split(X, y,\\n    ...                                                     random_state=1)\\n    >>> regr = MLPRegressor(random_state=1, max_iter=500).fit(X_train, y_train)\\n    >>> regr.predict(X_test[:2])\\n    array([-0.9..., -7.1...])\\n    >>> regr.score(X_test, y_test)\\n    0.4...\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/multi-task-elastic-net\"} \":sklearn.regression/multi-task-elastic-net\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [10 2]:\\n\\n|          :name |   :default |\\n|----------------|------------|\\n|     :normalize | deprecated |\\n|           :tol |  0.0001000 |\\n|      :max-iter |       1000 |\\n|  :random-state |            |\\n|        :copy-x |       true |\\n| :fit-intercept |       true |\\n|         :alpha |      1.000 |\\n|    :warm-start |      false |\\n|     :selection |     cyclic |\\n|     :l-1-ratio |     0.5000 |\\n\"]]] [:span [:p/markdown \"Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer.\\n\\n    The optimization objective for MultiTaskElasticNet is\\n\\n```python\\n(1 / (2 * n_samples)) * ||Y - XW||_Fro^2\\n+ alpha * l1_ratio * ||W||_21\\n+ 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\\n\\ne::\\n\\n||W||_21 = sum_i sqrt(sum_j W_ij ^ 2)\\n\\n the sum of norms of each row.\\n\\n more in the :ref:`User Guide <multi_task_elastic_net>`.\\n\\nmeters\\n------\\na : float, default=1.0\\nConstant that multiplies the L1/L2 term. Defaults to 1.0.\\n\\natio : float, default=0.5\\nThe ElasticNet mixing parameter, with 0 < l1_ratio <= 1.\\nFor l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it\\nis an L2 penalty.\\nFor ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2.\\n\\nintercept : bool, default=True\\nWhether to calculate the intercept for this model. If set\\nto false, no intercept will be used in calculations\\n(i.e. data is expected to be centered).\\n\\nalize : bool, default=False\\nThis parameter is ignored when ``fit_intercept`` is set to False.\\nIf True, the regressors X will be normalized before regression by\\nsubtracting the mean and dividing by the l2-norm.\\nIf you wish to standardize, please use\\n:class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\\non an estimator with ``normalize=False``.\\n\\n.. deprecated:: 1.0\\n    ``normalize`` was deprecated in version 1.0 and will be removed in\\n    1.2.\\n\\n_X : bool, default=True\\nIf ``True``, X will be copied; else, it may be overwritten.\\n\\niter : int, default=1000\\nThe maximum number of iterations.\\n\\n: float, default=1e-4\\nThe tolerance for the optimization: if the updates are\\nsmaller than ``tol``, the optimization code checks the\\ndual gap for optimality and continues until it is smaller\\nthan ``tol``.\\n\\n_start : bool, default=False\\nWhen set to ``True``, reuse the solution of the previous call to fit as\\ninitialization, otherwise, just erase the previous solution.\\nSee :term:`the Glossary <warm_start>`.\\n\\nom_state : int, RandomState instance, default=None\\nThe seed of the pseudo random number generator that selects a random\\nfeature to update. Used when ``selection`` == 'random'.\\nPass an int for reproducible output across multiple function calls.\\nSee :term:`Glossary <random_state>`.\\n\\nction : {'cyclic', 'random'}, default='cyclic'\\nIf set to 'random', a random coefficient is updated every iteration\\nrather than looping over features sequentially by default. This\\n(setting to 'random') often leads to significantly faster convergence\\nespecially when tol is higher than 1e-4.\\n\\nibutes\\n------\\nrcept_ : ndarray of shape (n_targets,)\\nIndependent term in decision function.\\n\\n_ : ndarray of shape (n_targets, n_features)\\nParameter vector (W in the cost function formula). If a 1D y is\\npassed in at fit (non multi-task usage), ``coef_`` is then a 1D array.\\nNote that ``coef_`` stores the transpose of ``W``, ``W.T``.\\n\\ner_ : int\\nNumber of iterations run by the coordinate descent solver to reach\\nthe specified tolerance.\\n\\n_gap_ : float\\nThe dual gaps at the end of the optimization.\\n\\n : float\\nThe tolerance scaled scaled by the variance of the target `y`.\\n\\nse_coef_ : sparse matrix of shape (n_features,) or             (n_targets, n_features)\\nSparse representation of the `coef_`.\\n\\natures_in_ : int\\nNumber of features seen during :term:`fit`.\\n\\n.. versionadded:: 0.24\\n\\nure_names_in_ : ndarray of shape (`n_features_in_`,)\\nNames of features seen during :term:`fit`. Defined only when `X`\\nhas feature names that are all strings.\\n\\n.. versionadded:: 1.0\\n\\nAlso\\n----\\niTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in\\ncross-validation.\\nticNet : Linear regression with combined L1 and L2 priors as regularizer.\\niTaskLasso : Multi-task L1/L2 Lasso with built-in cross-validation.\\n\\ns\\n-\\nalgorithm used to fit the model is coordinate descent.\\n\\nvoid unnecessary memory duplication the X and y arguments of the fit\\nod should be directly passed as Fortran-contiguous numpy arrays.\\n\\nples\\n----\\nfrom sklearn import linear_model\\nclf = linear_model.MultiTaskElasticNet(alpha=0.1)\\nclf.fit([[0,0], [1, 1], [2, 2]], [[0, 0], [1, 1], [2, 2]])\\niTaskElasticNet(alpha=0.1)\\nprint(clf.coef_)\\n45663524 0.45612256]\\n45663524 0.45612256]]\\nprint(clf.intercept_)\\n872422 0.0872422]\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/multi-task-elastic-net-cv\"} \":sklearn.regression/multi-task-elastic-net-cv\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [14 2]:\\n\\n|          :name |   :default |\\n|----------------|------------|\\n|     :normalize | deprecated |\\n|           :tol |  0.0001000 |\\n|      :n-alphas |        100 |\\n|           :eps |   0.001000 |\\n|        :alphas |            |\\n|      :max-iter |       1000 |\\n|        :n-jobs |            |\\n|  :random-state |            |\\n|        :copy-x |       true |\\n| :fit-intercept |       true |\\n|            :cv |            |\\n|     :selection |     cyclic |\\n|     :l-1-ratio |     0.5000 |\\n|       :verbose |          0 |\\n\"]]] [:span [:p/markdown \"Multi-task L1/L2 ElasticNet with built-in cross-validation.\\n\\n    See glossary entry for `cross-validation estimator`.\\n\\n    The optimization objective for MultiTaskElasticNet is\\n\\n```python\\n(1 / (2 * n_samples)) * ||Y - XW||^Fro_2\\n+ alpha * l1_ratio * ||W||_21\\n+ 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\\n\\ne::\\n\\n||W||_21 = \\\\sum_i \\\\sqrt{\\\\sum_j w_{ij}^2}\\n\\n the sum of norm of each row.\\n\\n more in the :ref:`User Guide <multi_task_elastic_net>`.\\n\\nersionadded:: 0.15\\n\\nmeters\\n------\\natio : float or list of float, default=0.5\\nThe ElasticNet mixing parameter, with 0 < l1_ratio <= 1.\\nFor l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it\\nis an L2 penalty.\\nFor ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2.\\nThis parameter can be a list, in which case the different\\nvalues are tested by cross-validation and the one giving the best\\nprediction score is used. Note that a good choice of list of\\nvalues for l1_ratio is often to put more values close to 1\\n(i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7,\\n.9, .95, .99, 1]``.\\n\\n: float, default=1e-3\\nLength of the path. ``eps=1e-3`` means that\\n``alpha_min / alpha_max = 1e-3``.\\n\\nphas : int, default=100\\nNumber of alphas along the regularization path.\\n\\nas : array-like, default=None\\nList of alphas where to compute the models.\\nIf not provided, set automatically.\\n\\nintercept : bool, default=True\\nWhether to calculate the intercept for this model. If set\\nto false, no intercept will be used in calculations\\n(i.e. data is expected to be centered).\\n\\nalize : bool, default=False\\nThis parameter is ignored when ``fit_intercept`` is set to False.\\nIf True, the regressors X will be normalized before regression by\\nsubtracting the mean and dividing by the l2-norm.\\nIf you wish to standardize, please use\\n:class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\\non an estimator with ``normalize=False``.\\n\\n.. deprecated:: 1.0\\n    ``normalize`` was deprecated in version 1.0 and will be removed in\\n    1.2.\\n\\niter : int, default=1000\\nThe maximum number of iterations.\\n\\n: float, default=1e-4\\nThe tolerance for the optimization: if the updates are\\nsmaller than ``tol``, the optimization code checks the\\ndual gap for optimality and continues until it is smaller\\nthan ``tol``.\\n\\n int, cross-validation generator or iterable, default=None\\nDetermines the cross-validation splitting strategy.\\nPossible inputs for cv are:\\n\\n- None, to use the default 5-fold cross-validation,\\n- int, to specify the number of folds.\\n- :term:`CV splitter`,\\n- An iterable yielding (train, test) splits as arrays of indices.\\n\\nFor int/None inputs, :class:`KFold` is used.\\n\\nRefer :ref:`User Guide <cross_validation>` for the various\\ncross-validation strategies that can be used here.\\n\\n.. versionchanged:: 0.22\\n    ``cv`` default value if None changed from 3-fold to 5-fold.\\n\\n_X : bool, default=True\\nIf ``True``, X will be copied; else, it may be overwritten.\\n\\nose : bool or int, default=0\\nAmount of verbosity.\\n\\nbs : int, default=None\\nNumber of CPUs to use during the cross validation. Note that this is\\nused only if multiple values for l1_ratio are given.\\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\\nfor more details.\\n\\nom_state : int, RandomState instance, default=None\\nThe seed of the pseudo random number generator that selects a random\\nfeature to update. Used when ``selection`` == 'random'.\\nPass an int for reproducible output across multiple function calls.\\nSee :term:`Glossary <random_state>`.\\n\\nction : {'cyclic', 'random'}, default='cyclic'\\nIf set to 'random', a random coefficient is updated every iteration\\nrather than looping over features sequentially by default. This\\n(setting to 'random') often leads to significantly faster convergence\\nespecially when tol is higher than 1e-4.\\n\\nibutes\\n------\\nrcept_ : ndarray of shape (n_targets,)\\nIndependent term in decision function.\\n\\n_ : ndarray of shape (n_targets, n_features)\\nParameter vector (W in the cost function formula).\\nNote that ``coef_`` stores the transpose of ``W``, ``W.T``.\\n\\na_ : float\\nThe amount of penalization chosen by cross validation.\\n\\npath_ : ndarray of shape (n_alphas, n_folds) or                 (n_l1_ratio, n_alphas, n_folds)\\nMean square error for the test set on each fold, varying alpha.\\n\\nas_ : ndarray of shape (n_alphas,) or (n_l1_ratio, n_alphas)\\nThe grid of alphas used for fitting, for each l1_ratio.\\n\\natio_ : float\\nBest l1_ratio obtained by cross-validation.\\n\\ner_ : int\\nNumber of iterations run by the coordinate descent solver to reach\\nthe specified tolerance for the optimal alpha.\\n\\n_gap_ : float\\nThe dual gap at the end of the optimization for the optimal alpha.\\n\\natures_in_ : int\\nNumber of features seen during :term:`fit`.\\n\\n.. versionadded:: 0.24\\n\\nure_names_in_ : ndarray of shape (`n_features_in_`,)\\nNames of features seen during :term:`fit`. Defined only when `X`\\nhas feature names that are all strings.\\n\\n.. versionadded:: 1.0\\n\\nAlso\\n----\\niTaskElasticNet : Multi-task L1/L2 ElasticNet with built-in cross-validation.\\nticNetCV : Elastic net model with best model selection by\\ncross-validation.\\niTaskLassoCV : Multi-task Lasso model trained with L1/L2\\nmixed-norm as regularizer.\\n\\ns\\n-\\nalgorithm used to fit the model is coordinate descent.\\n\\nfit`, once the best parameters `l1_ratio` and `alpha` are found through\\ns-validation, the model is fit again using the entire training set.\\n\\nvoid unnecessary memory duplication the `X` and `y` arguments of the\\n` method should be directly passed as Fortran-contiguous numpy arrays.\\n\\nples\\n----\\nfrom sklearn import linear_model\\nclf = linear_model.MultiTaskElasticNetCV(cv=3)\\nclf.fit([[0,0], [1, 1], [2, 2]],\\n        [[0, 0], [1, 1], [2, 2]])\\niTaskElasticNetCV(cv=3)\\nprint(clf.coef_)\\n52875032 0.46958558]\\n52875032 0.46958558]]\\nprint(clf.intercept_)\\n0166409 0.00166409]\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/multi-task-lasso\"} \":sklearn.regression/multi-task-lasso\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [9 2]:\\n\\n|          :name |   :default |\\n|----------------|------------|\\n|     :normalize | deprecated |\\n|           :tol |  0.0001000 |\\n|      :max-iter |       1000 |\\n|  :random-state |            |\\n|        :copy-x |       true |\\n| :fit-intercept |       true |\\n|         :alpha |      1.000 |\\n|    :warm-start |      false |\\n|     :selection |     cyclic |\\n\"]]] [:span [:p/markdown \"Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.\\n\\n    The optimization objective for Lasso is\\n\\n```python\\n(1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\\n\\ne::\\n\\n||W||_21 = \\\\sum_i \\\\sqrt{\\\\sum_j w_{ij}^2}\\n\\n the sum of norm of each row.\\n\\n more in the :ref:`User Guide <multi_task_lasso>`.\\n\\nmeters\\n------\\na : float, default=1.0\\nConstant that multiplies the L1/L2 term. Defaults to 1.0.\\n\\nintercept : bool, default=True\\nWhether to calculate the intercept for this model. If set\\nto false, no intercept will be used in calculations\\n(i.e. data is expected to be centered).\\n\\nalize : bool, default=False\\nThis parameter is ignored when ``fit_intercept`` is set to False.\\nIf True, the regressors X will be normalized before regression by\\nsubtracting the mean and dividing by the l2-norm.\\nIf you wish to standardize, please use\\n:class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\\non an estimator with ``normalize=False``.\\n\\n.. deprecated:: 1.0\\n    ``normalize`` was deprecated in version 1.0 and will be removed in\\n    1.2.\\n\\n_X : bool, default=True\\nIf ``True``, X will be copied; else, it may be overwritten.\\n\\niter : int, default=1000\\nThe maximum number of iterations.\\n\\n: float, default=1e-4\\nThe tolerance for the optimization: if the updates are\\nsmaller than ``tol``, the optimization code checks the\\ndual gap for optimality and continues until it is smaller\\nthan ``tol``.\\n\\n_start : bool, default=False\\nWhen set to ``True``, reuse the solution of the previous call to fit as\\ninitialization, otherwise, just erase the previous solution.\\nSee :term:`the Glossary <warm_start>`.\\n\\nom_state : int, RandomState instance, default=None\\nThe seed of the pseudo random number generator that selects a random\\nfeature to update. Used when ``selection`` == 'random'.\\nPass an int for reproducible output across multiple function calls.\\nSee :term:`Glossary <random_state>`.\\n\\nction : {'cyclic', 'random'}, default='cyclic'\\nIf set to 'random', a random coefficient is updated every iteration\\nrather than looping over features sequentially by default. This\\n(setting to 'random') often leads to significantly faster convergence\\nespecially when tol is higher than 1e-4.\\n\\nibutes\\n------\\n_ : ndarray of shape (n_targets, n_features)\\nParameter vector (W in the cost function formula).\\nNote that ``coef_`` stores the transpose of ``W``, ``W.T``.\\n\\nrcept_ : ndarray of shape (n_targets,)\\nIndependent term in decision function.\\n\\ner_ : int\\nNumber of iterations run by the coordinate descent solver to reach\\nthe specified tolerance.\\n\\n_gap_ : ndarray of shape (n_alphas,)\\nThe dual gaps at the end of the optimization for each alpha.\\n\\n : float\\nThe tolerance scaled scaled by the variance of the target `y`.\\n\\nse_coef_ : sparse matrix of shape (n_features,) or             (n_targets, n_features)\\nSparse representation of the `coef_`.\\n\\natures_in_ : int\\nNumber of features seen during :term:`fit`.\\n\\n.. versionadded:: 0.24\\n\\nure_names_in_ : ndarray of shape (`n_features_in_`,)\\nNames of features seen during :term:`fit`. Defined only when `X`\\nhas feature names that are all strings.\\n\\n.. versionadded:: 1.0\\n\\nAlso\\n----\\no: Linear Model trained with L1 prior as regularizer (aka the Lasso).\\niTaskLasso: Multi-task L1/L2 Lasso with built-in cross-validation.\\niTaskElasticNet: Multi-task L1/L2 ElasticNet with built-in cross-validation.\\n\\ns\\n-\\nalgorithm used to fit the model is coordinate descent.\\n\\nvoid unnecessary memory duplication the X and y arguments of the fit\\nod should be directly passed as Fortran-contiguous numpy arrays.\\n\\nples\\n----\\nfrom sklearn import linear_model\\nclf = linear_model.MultiTaskLasso(alpha=0.1)\\nclf.fit([[0, 1], [1, 2], [2, 4]], [[0, 0], [1, 1], [2, 3]])\\niTaskLasso(alpha=0.1)\\nprint(clf.coef_)\\n         0.60809415]\\n        0.94592424]]\\nprint(clf.intercept_)\\n41888636 -0.87382323]\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/multi-task-lasso-cv\"} \":sklearn.regression/multi-task-lasso-cv\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [13 2]:\\n\\n|          :name |   :default |\\n|----------------|------------|\\n|     :normalize | deprecated |\\n|           :tol |  0.0001000 |\\n|      :n-alphas |        100 |\\n|           :eps |   0.001000 |\\n|        :alphas |            |\\n|      :max-iter |       1000 |\\n|        :n-jobs |            |\\n|  :random-state |            |\\n|        :copy-x |       true |\\n| :fit-intercept |       true |\\n|            :cv |            |\\n|     :selection |     cyclic |\\n|       :verbose |      false |\\n\"]]] [:span [:p/markdown \"Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.\\n\\n    See glossary entry for `cross-validation estimator`.\\n\\n    The optimization objective for MultiTaskLasso is\\n\\n```python\\n(1 / (2 * n_samples)) * ||Y - XW||^Fro_2 + alpha * ||W||_21\\n\\ne::\\n\\n||W||_21 = \\\\sum_i \\\\sqrt{\\\\sum_j w_{ij}^2}\\n\\n the sum of norm of each row.\\n\\n more in the :ref:`User Guide <multi_task_lasso>`.\\n\\nersionadded:: 0.15\\n\\nmeters\\n------\\n: float, default=1e-3\\nLength of the path. ``eps=1e-3`` means that\\n``alpha_min / alpha_max = 1e-3``.\\n\\nphas : int, default=100\\nNumber of alphas along the regularization path.\\n\\nas : array-like, default=None\\nList of alphas where to compute the models.\\nIf not provided, set automatically.\\n\\nintercept : bool, default=True\\nWhether to calculate the intercept for this model. If set\\nto false, no intercept will be used in calculations\\n(i.e. data is expected to be centered).\\n\\nalize : bool, default=False\\nThis parameter is ignored when ``fit_intercept`` is set to False.\\nIf True, the regressors X will be normalized before regression by\\nsubtracting the mean and dividing by the l2-norm.\\nIf you wish to standardize, please use\\n:class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\\non an estimator with ``normalize=False``.\\n\\n.. deprecated:: 1.0\\n    ``normalize`` was deprecated in version 1.0 and will be removed in\\n    1.2.\\n\\niter : int, default=1000\\nThe maximum number of iterations.\\n\\n: float, default=1e-4\\nThe tolerance for the optimization: if the updates are\\nsmaller than ``tol``, the optimization code checks the\\ndual gap for optimality and continues until it is smaller\\nthan ``tol``.\\n\\n_X : bool, default=True\\nIf ``True``, X will be copied; else, it may be overwritten.\\n\\n int, cross-validation generator or iterable, default=None\\nDetermines the cross-validation splitting strategy.\\nPossible inputs for cv are:\\n\\n- None, to use the default 5-fold cross-validation,\\n- int, to specify the number of folds.\\n- :term:`CV splitter`,\\n- An iterable yielding (train, test) splits as arrays of indices.\\n\\nFor int/None inputs, :class:`KFold` is used.\\n\\nRefer :ref:`User Guide <cross_validation>` for the various\\ncross-validation strategies that can be used here.\\n\\n.. versionchanged:: 0.22\\n    ``cv`` default value if None changed from 3-fold to 5-fold.\\n\\nose : bool or int, default=False\\nAmount of verbosity.\\n\\nbs : int, default=None\\nNumber of CPUs to use during the cross validation. Note that this is\\nused only if multiple values for l1_ratio are given.\\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\\nfor more details.\\n\\nom_state : int, RandomState instance, default=None\\nThe seed of the pseudo random number generator that selects a random\\nfeature to update. Used when ``selection`` == 'random'.\\nPass an int for reproducible output across multiple function calls.\\nSee :term:`Glossary <random_state>`.\\n\\nction : {'cyclic', 'random'}, default='cyclic'\\nIf set to 'random', a random coefficient is updated every iteration\\nrather than looping over features sequentially by default. This\\n(setting to 'random') often leads to significantly faster convergence\\nespecially when tol is higher than 1e-4.\\n\\nibutes\\n------\\nrcept_ : ndarray of shape (n_targets,)\\nIndependent term in decision function.\\n\\n_ : ndarray of shape (n_targets, n_features)\\nParameter vector (W in the cost function formula).\\nNote that ``coef_`` stores the transpose of ``W``, ``W.T``.\\n\\na_ : float\\nThe amount of penalization chosen by cross validation.\\n\\npath_ : ndarray of shape (n_alphas, n_folds)\\nMean square error for the test set on each fold, varying alpha.\\n\\nas_ : ndarray of shape (n_alphas,)\\nThe grid of alphas used for fitting.\\n\\ner_ : int\\nNumber of iterations run by the coordinate descent solver to reach\\nthe specified tolerance for the optimal alpha.\\n\\n_gap_ : float\\nThe dual gap at the end of the optimization for the optimal alpha.\\n\\natures_in_ : int\\nNumber of features seen during :term:`fit`.\\n\\n.. versionadded:: 0.24\\n\\nure_names_in_ : ndarray of shape (`n_features_in_`,)\\nNames of features seen during :term:`fit`. Defined only when `X`\\nhas feature names that are all strings.\\n\\n.. versionadded:: 1.0\\n\\nAlso\\n----\\niTaskElasticNet : Multi-task ElasticNet model trained with L1/L2\\nmixed-norm as regularizer.\\nticNetCV : Elastic net model with best model selection by\\ncross-validation.\\niTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in\\ncross-validation.\\n\\ns\\n-\\nalgorithm used to fit the model is coordinate descent.\\n\\nfit`, once the best parameter `alpha` is found through\\ns-validation, the model is fit again using the entire training set.\\n\\nvoid unnecessary memory duplication the `X` and `y` arguments of the\\n` method should be directly passed as Fortran-contiguous numpy arrays.\\n\\nples\\n----\\nfrom sklearn.linear_model import MultiTaskLassoCV\\nfrom sklearn.datasets import make_regression\\nfrom sklearn.metrics import r2_score\\nX, y = make_regression(n_targets=2, noise=4, random_state=0)\\nreg = MultiTaskLassoCV(cv=5, random_state=0).fit(X, y)\\nr2_score(y, reg.predict(X))\\n94...\\nreg.alpha_\\n13...\\nreg.predict(X[:1,])\\ny([[153.7971...,  94.9015...]])\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/nu-svr\"} \":sklearn.regression/nu-svr\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [11 2]:\\n\\n|       :name | :default |\\n|-------------|----------|\\n|     :kernel |      rbf |\\n|      :gamma |    scale |\\n|     :degree |        3 |\\n|        :tol | 0.001000 |\\n|         :nu |   0.5000 |\\n|  :shrinking |     true |\\n|          :c |    1.000 |\\n|   :max-iter |       -1 |\\n|     :coef-0 |    0.000 |\\n| :cache-size |      200 |\\n|    :verbose |    false |\\n\"]]] [:span [:p/markdown \"Nu Support Vector Regression.\\n\\n    Similar to NuSVC, for regression, uses a parameter nu to control\\n    the number of support vectors. However, unlike NuSVC, where nu\\n    replaces C, here nu replaces the parameter epsilon of epsilon-SVR.\\n\\n    The implementation is based on libsvm.\\n\\n    Read more in the User Guide: `svm_regression`.\\n\\n    Parameters\\n    ----------\\n    - `nu`: float, default=0.5\\n        An upper bound on the fraction of training errors and a lower bound of\\n        the fraction of support vectors. Should be in the interval (0, 1].  By\\n        default 0.5 will be taken.\\n\\n    - `C`: float, default=1.0\\n        Penalty parameter C of the error term.\\n\\n    - `kernel`: {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'} or callable,          default='rbf'\\n         Specifies the kernel type to be used in the algorithm.\\n         If none is given, 'rbf' will be used. If a callable is given it is\\n         used to precompute the kernel matrix.\\n\\n    - `degree`: int, default=3\\n        Degree of the polynomial kernel function ('poly').\\n        Ignored by all other kernels.\\n\\n    - `gamma`: {'scale', 'auto'} or float, default='scale'\\n        Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\\n\\n        - if ``gamma='scale'`` (default) is passed then it uses\\n          1 / (n_features * X.var()) as value of gamma,\\n        - if 'auto', uses 1 / n_features.\\n\\n        *Changed in 0.22*\\n           The default value of ``gamma`` changed from 'auto' to 'scale'.\\n\\n    - `coef0`: float, default=0.0\\n        Independent term in kernel function.\\n        It is only significant in 'poly' and 'sigmoid'.\\n\\n    - `shrinking`: bool, default=True\\n        Whether to use the shrinking heuristic.\\n        See the User Guide: `shrinking_svm`.\\n\\n    - `tol`: float, default=1e-3\\n        Tolerance for stopping criterion.\\n\\n    - `cache_size`: float, default=200\\n        Specify the size of the kernel cache (in MB).\\n\\n    - `verbose`: bool, default=False\\n        Enable verbose output. Note that this setting takes advantage of a\\n        per-process runtime setting in libsvm that, if enabled, may not work\\n        properly in a multithreaded context.\\n\\n    - `max_iter`: int, default=-1\\n        Hard limit on iterations within solver, or -1 for no limit.\\n\\n    Attributes\\n    ----------\\n    - `class_weight_`: ndarray of shape (n_classes,)\\n        Multipliers of parameter C for each class.\\n        Computed based on the ``class_weight`` parameter.\\n\\n    - `coef_`: ndarray of shape (1, n_features)\\n        Weights assigned to the features (coefficients in the primal\\n        problem). This is only available in the case of a linear kernel.\\n\\n        `coef_` is readonly property derived from `dual_coef_` and\\n        `support_vectors_`.\\n\\n    - `dual_coef_`: ndarray of shape (1, n_SV)\\n        Coefficients of the support vector in the decision function.\\n\\n    - `fit_status_`: int\\n        0 if correctly fitted, 1 otherwise (will raise warning)\\n\\n    - `intercept_`: ndarray of shape (1,)\\n        Constants in decision function.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    - `n_iter_`: int\\n        Number of iterations run by the optimization routine to fit the model.\\n\\n        *Added in 1.1*\\n\\n    - `n_support_`: ndarray of shape (n_classes,), dtype=int32\\n        Number of support vectors for each class.\\n\\n    - `shape_fit_`: tuple of int of shape (n_dimensions_of_X,)\\n        Array dimensions of training vector ``X``.\\n\\n    - `support_`: ndarray of shape (n_SV,)\\n        Indices of support vectors.\\n\\n    - `support_vectors_`: ndarray of shape (n_SV, n_features)\\n        Support vectors.\\n\\n    See Also\\n    --------\\n    - `NuSVC`: Support Vector Machine for classification implemented with libsvm\\n        with a parameter to control the number of support vectors.\\n\\n    - `SVR`: Epsilon Support Vector Machine for regression implemented with\\n        libsvm.\\n\\n    References\\n    ----------\\n - [1] [LIBSVM: A Library for Support Vector Machines\\n        ](http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf)\\n\\n - [2] [Platt, John (1999). \\\"Probabilistic outputs for support vector\\n        machines and comparison to regularizedlikelihood methods.\\\"\\n        ](http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639)\\n\\n    Examples\\n    --------\\n    >>> from sklearn.svm import NuSVR\\n    >>> from sklearn.pipeline import make_pipeline\\n    >>> from sklearn.preprocessing import StandardScaler\\n    >>> import numpy as np\\n    >>> n_samples, n_features = 10, 5\\n    >>> np.random.seed(0)\\n    >>> y = np.random.randn(n_samples)\\n    >>> X = np.random.randn(n_samples, n_features)\\n    >>> regr = make_pipeline(StandardScaler(), NuSVR(C=1.0, nu=0.1))\\n    >>> regr.fit(X, y)\\n    Pipeline(steps=[('standardscaler', StandardScaler()),\\n                    ('nusvr', NuSVR(nu=0.1))])\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/orthogonal-matching-pursuit\"} \":sklearn.regression/orthogonal-matching-pursuit\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [5 2]:\\n\\n|            :name |   :default |\\n|------------------|------------|\\n|   :fit-intercept |       true |\\n| :n-nonzero-coefs |            |\\n|       :normalize | deprecated |\\n|      :precompute |       auto |\\n|             :tol |            |\\n\"]]] [:span [:p/markdown \"Orthogonal Matching Pursuit model (OMP).\\n\\n    Read more in the User Guide: `omp`.\\n\\n    Parameters\\n    ----------\\n    - `n_nonzero_coefs`: int, default=None\\n        Desired number of non-zero entries in the solution. If None (by\\n        default) this value is set to 10% of n_features.\\n\\n    - `tol`: float, default=None\\n        Maximum norm of the residual. If not None, overrides n_nonzero_coefs.\\n\\n    - `fit_intercept`: bool, default=True\\n        Whether to calculate the intercept for this model. If set\\n        to false, no intercept will be used in calculations\\n        (i.e. data is expected to be centered).\\n\\n    - `normalize`: bool, default=True\\n        This parameter is ignored when ``fit_intercept`` is set to False.\\n        If True, the regressors X will be normalized before regression by\\n        subtracting the mean and dividing by the l2-norm.\\n        If you wish to standardize, please use\\n        `~sklearn.preprocessing.StandardScaler` before calling ``fit``\\n        on an estimator with ``normalize=False``.\\n\\n        *Deprecated since 1.0*\\n            ``normalize`` was deprecated in version 1.0. It will default\\n            to False in 1.2 and be removed in 1.4.\\n\\n    - `precompute`: 'auto' or bool, default='auto'\\n        Whether to use a precomputed Gram and Xy matrix to speed up\\n        calculations. Improves performance when `n_targets` or\\n        `n_samples` is very large. Note that if you already have such\\n        matrices, you can pass them directly to the fit method.\\n\\n    Attributes\\n    ----------\\n    - `coef_`: ndarray of shape (n_features,) or (n_targets, n_features)\\n        Parameter vector (w in the formula).\\n\\n    - `intercept_`: float or ndarray of shape (n_targets,)\\n        Independent term in decision function.\\n\\n    - `n_iter_`: int or array-like\\n        Number of active features across every target.\\n\\n    - `n_nonzero_coefs_`: int\\n        The number of non-zero coefficients in the solution. If\\n        `n_nonzero_coefs` is None and `tol` is None this value is either set\\n        to 10% of `n_features` or 1, whichever is greater.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    See Also\\n    --------\\n    - `orthogonal_mp`: Solves n_targets Orthogonal Matching Pursuit problems.\\n    - `orthogonal_mp_gram`:  Solves n_targets Orthogonal Matching Pursuit\\n        problems using only the Gram matrix X.T * X and the product X.T * y.\\n    - `lars_path`: Compute Least Angle Regression or Lasso path using LARS algorithm.\\n    - `Lars`: Least Angle Regression model a.k.a. LAR.\\n    - `LassoLars`: Lasso model fit with Least Angle Regression a.k.a. Lars.\\n    - `sklearn.decomposition.sparse_encode`: Generic sparse coding.\\n        Each column of the result is the solution to a Lasso problem.\\n    - `OrthogonalMatchingPursuitCV`: Cross-validated\\n        Orthogonal Matching Pursuit model (OMP).\\n\\n    Notes\\n    -----\\n    Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang,\\n    Matching pursuits with time-frequency dictionaries, IEEE Transactions on\\n    Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\\n    (http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf)\\n\\n    This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\\n    M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\\n    Matching Pursuit Technical Report - CS Technion, April 2008.\\n    https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf\\n\\n    Examples\\n    --------\\n    >>> from sklearn.linear_model import OrthogonalMatchingPursuit\\n    >>> from sklearn.datasets import make_regression\\n    >>> X, y = make_regression(noise=4, random_state=0)\\n    >>> reg = OrthogonalMatchingPursuit(normalize=False).fit(X, y)\\n    >>> reg.score(X, y)\\n    0.9991...\\n    >>> reg.predict(X[:1,])\\n    array([-78.3854...])\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/orthogonal-matching-pursuit-cv\"} \":sklearn.regression/orthogonal-matching-pursuit-cv\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [7 2]:\\n\\n|          :name |   :default |\\n|----------------|------------|\\n|          :copy |       true |\\n|            :cv |            |\\n| :fit-intercept |       true |\\n|      :max-iter |            |\\n|        :n-jobs |            |\\n|     :normalize | deprecated |\\n|       :verbose |      false |\\n\"]]] [:span [:p/markdown \"Cross-validated Orthogonal Matching Pursuit model (OMP).\\n\\n    See glossary entry for `cross-validation estimator`.\\n\\n    Read more in the User Guide: `omp`.\\n\\n    Parameters\\n    ----------\\n    - `copy`: bool, default=True\\n        Whether the design matrix X must be copied by the algorithm. A false\\n        value is only helpful if X is already Fortran-ordered, otherwise a\\n        copy is made anyway.\\n\\n    - `fit_intercept`: bool, default=True\\n        Whether to calculate the intercept for this model. If set\\n        to false, no intercept will be used in calculations\\n        (i.e. data is expected to be centered).\\n\\n    - `normalize`: bool, default=True\\n        This parameter is ignored when ``fit_intercept`` is set to False.\\n        If True, the regressors X will be normalized before regression by\\n        subtracting the mean and dividing by the l2-norm.\\n        If you wish to standardize, please use\\n        `~sklearn.preprocessing.StandardScaler` before calling ``fit``\\n        on an estimator with ``normalize=False``.\\n\\n        *Deprecated since 1.0*\\n            ``normalize`` was deprecated in version 1.0. It will default\\n            to False in 1.2 and be removed in 1.4.\\n\\n    - `max_iter`: int, default=None\\n        Maximum numbers of iterations to perform, therefore maximum features\\n        to include. 10% of ``n_features`` but at least 5 if available.\\n\\n    - `cv`: int, cross-validation generator or iterable, default=None\\n        Determines the cross-validation splitting strategy.\\n        Possible inputs for cv are:\\n\\n        - None, to use the default 5-fold cross-validation,\\n        - integer, to specify the number of folds.\\n        - `CV splitter`,\\n        - An iterable yielding (train, test) splits as arrays of indices.\\n\\n        For integer/None inputs, `KFold` is used.\\n\\n        Refer User Guide: `cross_validation` for the various\\n        cross-validation strategies that can be used here.\\n\\n        *Changed in 0.22*\\n            ``cv`` default value if None changed from 3-fold to 5-fold.\\n\\n    - `n_jobs`: int, default=None\\n        Number of CPUs to use during the cross validation.\\n        ``None`` means 1 unless in a `joblib.parallel_backend` context.\\n        ``-1`` means using all processors. See `Glossary <n_jobs>`\\n        for more details.\\n\\n    - `verbose`: bool or int, default=False\\n        Sets the verbosity amount.\\n\\n    Attributes\\n    ----------\\n    - `intercept_`: float or ndarray of shape (n_targets,)\\n        Independent term in decision function.\\n\\n    - `coef_`: ndarray of shape (n_features,) or (n_targets, n_features)\\n        Parameter vector (w in the problem formulation).\\n\\n    - `n_nonzero_coefs_`: int\\n        Estimated number of non-zero coefficients giving the best mean squared\\n        error over the cross-validation folds.\\n\\n    - `n_iter_`: int or array-like\\n        Number of active features across every target for the model refit with\\n        the best hyperparameters got by cross-validating across all folds.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    See Also\\n    --------\\n    - `orthogonal_mp`: Solves n_targets Orthogonal Matching Pursuit problems.\\n    - `orthogonal_mp_gram`: Solves n_targets Orthogonal Matching Pursuit\\n        problems using only the Gram matrix X.T * X and the product X.T * y.\\n    - `lars_path`: Compute Least Angle Regression or Lasso path using LARS algorithm.\\n    - `Lars`: Least Angle Regression model a.k.a. LAR.\\n    - `LassoLars`: Lasso model fit with Least Angle Regression a.k.a. Lars.\\n    - `OrthogonalMatchingPursuit`: Orthogonal Matching Pursuit model (OMP).\\n    - `LarsCV`: Cross-validated Least Angle Regression model.\\n    - `LassoLarsCV`: Cross-validated Lasso model fit with Least Angle Regression.\\n    - `sklearn.decomposition.sparse_encode`: Generic sparse coding.\\n        Each column of the result is the solution to a Lasso problem.\\n\\n    Notes\\n    -----\\n    In `fit`, once the optimal number of non-zero coefficients is found through\\n    cross-validation, the model is fit again using the entire training set.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.linear_model import OrthogonalMatchingPursuitCV\\n    >>> from sklearn.datasets import make_regression\\n    >>> X, y = make_regression(n_features=100, n_informative=10,\\n    ...                        noise=4, random_state=0)\\n    >>> reg = OrthogonalMatchingPursuitCV(cv=5, normalize=False).fit(X, y)\\n    >>> reg.score(X, y)\\n    0.9991...\\n    >>> reg.n_nonzero_coefs_\\n    10\\n    >>> reg.predict(X[:1,])\\n    array([-78.3854...])\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/passive-aggressive-regressor\"} \":sklearn.regression/passive-aggressive-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [14 2]:\\n\\n|                :name |            :default |\\n|----------------------|---------------------|\\n|    :n-iter-no-change |                   5 |\\n|             :average |               false |\\n|                 :tol |            0.001000 |\\n|      :early-stopping |               false |\\n|             :shuffle |                true |\\n|                   :c |               1.000 |\\n|            :max-iter |                1000 |\\n|        :random-state |                     |\\n|       :fit-intercept |                true |\\n|          :warm-start |               false |\\n| :validation-fraction |              0.1000 |\\n|                :loss | epsilon_insensitive |\\n|             :verbose |                   0 |\\n|             :epsilon |              0.1000 |\\n\"]]] [:span [:p/markdown \"Passive Aggressive Regressor.\\n\\n    Read more in the User Guide: `passive_aggressive`.\\n\\n    Parameters\\n    ----------\\n\\n    - `C`: float, default=1.0\\n        Maximum step size (regularization). Defaults to 1.0.\\n\\n    - `fit_intercept`: bool, default=True\\n        Whether the intercept should be estimated or not. If False, the\\n        data is assumed to be already centered. Defaults to True.\\n\\n    - `max_iter`: int, default=1000\\n        The maximum number of passes over the training data (aka epochs).\\n        It only impacts the behavior in the ``fit`` method, and not the\\n        `partial_fit` method.\\n\\n        *Added in 0.19*\\n\\n    - `tol`: float or None, default=1e-3\\n        The stopping criterion. If it is not None, the iterations will stop\\n        when (loss > previous_loss - tol).\\n\\n        *Added in 0.19*\\n\\n    - `early_stopping`: bool, default=False\\n        Whether to use early stopping to terminate training when validation.\\n        score is not improving. If set to True, it will automatically set aside\\n        a fraction of training data as validation and terminate\\n        training when validation score is not improving by at least tol for\\n        n_iter_no_change consecutive epochs.\\n\\n        *Added in 0.20*\\n\\n    - `validation_fraction`: float, default=0.1\\n        The proportion of training data to set aside as validation set for\\n        early stopping. Must be between 0 and 1.\\n        Only used if early_stopping is True.\\n\\n        *Added in 0.20*\\n\\n    - `n_iter_no_change`: int, default=5\\n        Number of iterations with no improvement to wait before early stopping.\\n\\n        *Added in 0.20*\\n\\n    - `shuffle`: bool, default=True\\n        Whether or not the training data should be shuffled after each epoch.\\n\\n    - `verbose`: int, default=0\\n        The verbosity level.\\n\\n    - `loss`: str, default=\\\"epsilon_insensitive\\\"\\n        The loss function to be used:\\n        epsilon_insensitive: equivalent to PA-I in the reference paper.\\n        squared_epsilon_insensitive: equivalent to PA-II in the reference\\n        paper.\\n\\n    - `epsilon`: float, default=0.1\\n        If the difference between the current prediction and the correct label\\n        is below this threshold, the model is not updated.\\n\\n    - `random_state`: int, RandomState instance, default=None\\n        Used to shuffle the training data, when ``shuffle`` is set to\\n        ``True``. Pass an int for reproducible output across multiple\\n        function calls.\\n        See `Glossary <random_state>`.\\n\\n    - `warm_start`: bool, default=False\\n        When set to True, reuse the solution of the previous call to fit as\\n        initialization, otherwise, just erase the previous solution.\\n        See `the Glossary <warm_start>`.\\n\\n        Repeatedly calling fit or partial_fit when warm_start is True can\\n        result in a different solution than when calling fit a single time\\n        because of the way the data is shuffled.\\n\\n    - `average`: bool or int, default=False\\n        When set to True, computes the averaged SGD weights and stores the\\n        result in the ``coef_`` attribute. If set to an int greater than 1,\\n        averaging will begin once the total number of samples seen reaches\\n        average. So average=10 will begin averaging after seeing 10 samples.\\n\\n        *Added in 0.19*\\n           parameter *average* to use weights averaging in SGD.\\n\\n    Attributes\\n    ----------\\n    - `coef_`: array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]\\n        Weights assigned to the features.\\n\\n    - `intercept_`: array, shape = [1] if n_classes == 2 else [n_classes]\\n        Constants in decision function.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    - `n_iter_`: int\\n        The actual number of iterations to reach the stopping criterion.\\n\\n    - `t_`: int\\n        Number of weight updates performed during training.\\n        Same as ``(n_iter_ * n_samples)``.\\n\\n    See Also\\n    --------\\n    - `SGDRegressor`: Linear model fitted by minimizing a regularized\\n        empirical loss with SGD.\\n\\n    References\\n    ----------\\n    Online Passive-Aggressive Algorithms\\n    <http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf>\\n    K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006).\\n\\n    Examples\\n    --------\\n    >>> from sklearn.linear_model import PassiveAggressiveRegressor\\n    >>> from sklearn.datasets import make_regression\\n\\n    >>> X, y = make_regression(n_features=4, random_state=0)\\n    >>> regr = PassiveAggressiveRegressor(max_iter=100, random_state=0,\\n    ... tol=1e-3)\\n    >>> regr.fit(X, y)\\n    PassiveAggressiveRegressor(max_iter=100, random_state=0)\\n    >>> print(regr.coef_)\\n    [20.48736655 34.18818427 67.59122734 87.94731329]\\n    >>> print(regr.intercept_)\\n    [-0.02306214]\\n    >>> print(regr.predict([[0, 0, 0, 0]]))\\n    [-0.02306214]\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/pls-canonical\"} \":sklearn.regression/pls-canonical\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [6 2]:\\n\\n|         :name |  :default |\\n|---------------|-----------|\\n|    :algorithm |    nipals |\\n|         :copy |      true |\\n|     :max-iter |       500 |\\n| :n-components |         2 |\\n|        :scale |      true |\\n|          :tol | 1.000E-06 |\\n\"]]] [:span [:p/markdown \"Partial Least Squares transformer and regressor.\\n\\n    Read more in the User Guide: `cross_decomposition`.\\n\\n    *Added in 0.8*\\n\\n    Parameters\\n    ----------\\n    - `n_components`: int, default=2\\n        Number of components to keep. Should be in `[1, min(n_samples,\\n        n_features, n_targets)]`.\\n\\n    - `scale`: bool, default=True\\n        Whether to scale `X` and `Y`.\\n\\n    - `algorithm`: {'nipals', 'svd'}, default='nipals'\\n        The algorithm used to estimate the first singular vectors of the\\n        cross-covariance matrix. 'nipals' uses the power method while 'svd'\\n        will compute the whole SVD.\\n\\n    - `max_iter`: int, default=500\\n        The maximum number of iterations of the power method when\\n        `algorithm='nipals'`. Ignored otherwise.\\n\\n    - `tol`: float, default=1e-06\\n        The tolerance used as convergence criteria in the power method: the\\n        algorithm stops whenever the squared norm of `u_i - u_{i-1}` is less\\n        than `tol`, where `u` corresponds to the left singular vector.\\n\\n    - `copy`: bool, default=True\\n        Whether to copy `X` and `Y` in fit before applying centering, and\\n        potentially scaling. If False, these operations will be done inplace,\\n        modifying both arrays.\\n\\n    Attributes\\n    ----------\\n    - `x_weights_`: ndarray of shape (n_features, n_components)\\n        The left singular vectors of the cross-covariance matrices of each\\n        iteration.\\n\\n    - `y_weights_`: ndarray of shape (n_targets, n_components)\\n        The right singular vectors of the cross-covariance matrices of each\\n        iteration.\\n\\n    - `x_loadings_`: ndarray of shape (n_features, n_components)\\n        The loadings of `X`.\\n\\n    - `y_loadings_`: ndarray of shape (n_targets, n_components)\\n        The loadings of `Y`.\\n\\n    - `x_rotations_`: ndarray of shape (n_features, n_components)\\n        The projection matrix used to transform `X`.\\n\\n    - `y_rotations_`: ndarray of shape (n_features, n_components)\\n        The projection matrix used to transform `Y`.\\n\\n    - `coef_`: ndarray of shape (n_features, n_targets)\\n        The coefficients of the linear model such that `Y` is approximated as\\n        `Y = X @ coef_ + intercept_`.\\n\\n    - `intercept_`: ndarray of shape (n_targets,)\\n        The intercepts of the linear model such that `Y` is approximated as\\n        `Y = X @ coef_ + intercept_`.\\n\\n        *Added in 1.1*\\n\\n    - `n_iter_`: list of shape (n_components,)\\n        Number of iterations of the power method, for each\\n        component. Empty if `algorithm='svd'`.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    See Also\\n    --------\\n    - `CCA`: Canonical Correlation Analysis.\\n    - `PLSSVD`: Partial Least Square SVD.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.cross_decomposition import PLSCanonical\\n    >>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]]\\n    >>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\\n    >>> plsca = PLSCanonical(n_components=2)\\n    >>> plsca.fit(X, Y)\\n    PLSCanonical()\\n    >>> X_c, Y_c = plsca.transform(X, Y)\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/pls-regression\"} \":sklearn.regression/pls-regression\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [5 2]:\\n\\n|         :name |  :default |\\n|---------------|-----------|\\n|         :copy |      true |\\n|     :max-iter |       500 |\\n| :n-components |         2 |\\n|        :scale |      true |\\n|          :tol | 1.000E-06 |\\n\"]]] [:span [:p/markdown \"PLS regression.\\n\\n    PLSRegression is also known as PLS2 or PLS1, depending on the number of\\n    targets.\\n\\n    Read more in the User Guide: `cross_decomposition`.\\n\\n    *Added in 0.8*\\n\\n    Parameters\\n    ----------\\n    - `n_components`: int, default=2\\n        Number of components to keep. Should be in `[1, min(n_samples,\\n        n_features, n_targets)]`.\\n\\n    - `scale`: bool, default=True\\n        Whether to scale `X` and `Y`.\\n\\n    - `max_iter`: int, default=500\\n        The maximum number of iterations of the power method when\\n        `algorithm='nipals'`. Ignored otherwise.\\n\\n    - `tol`: float, default=1e-06\\n        The tolerance used as convergence criteria in the power method: the\\n        algorithm stops whenever the squared norm of `u_i - u_{i-1}` is less\\n        than `tol`, where `u` corresponds to the left singular vector.\\n\\n    - `copy`: bool, default=True\\n        Whether to copy `X` and `Y` in `fit` before applying centering,\\n        and potentially scaling. If `False`, these operations will be done\\n        inplace, modifying both arrays.\\n\\n    Attributes\\n    ----------\\n    - `x_weights_`: ndarray of shape (n_features, n_components)\\n        The left singular vectors of the cross-covariance matrices of each\\n        iteration.\\n\\n    - `y_weights_`: ndarray of shape (n_targets, n_components)\\n        The right singular vectors of the cross-covariance matrices of each\\n        iteration.\\n\\n    - `x_loadings_`: ndarray of shape (n_features, n_components)\\n        The loadings of `X`.\\n\\n    - `y_loadings_`: ndarray of shape (n_targets, n_components)\\n        The loadings of `Y`.\\n\\n    - `x_scores_`: ndarray of shape (n_samples, n_components)\\n        The transformed training samples.\\n\\n    - `y_scores_`: ndarray of shape (n_samples, n_components)\\n        The transformed training targets.\\n\\n    - `x_rotations_`: ndarray of shape (n_features, n_components)\\n        The projection matrix used to transform `X`.\\n\\n    - `y_rotations_`: ndarray of shape (n_features, n_components)\\n        The projection matrix used to transform `Y`.\\n\\n    - `coef_`: ndarray of shape (n_features, n_targets)\\n        The coefficients of the linear model such that `Y` is approximated as\\n        `Y = X @ coef_ + intercept_`.\\n\\n    - `intercept_`: ndarray of shape (n_targets,)\\n        The intercepts of the linear model such that `Y` is approximated as\\n        `Y = X @ coef_ + intercept_`.\\n\\n        *Added in 1.1*\\n\\n    - `n_iter_`: list of shape (n_components,)\\n        Number of iterations of the power method, for each\\n        component.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    See Also\\n    --------\\n    - `PLSCanonical`: Partial Least Squares transformer and regressor.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.cross_decomposition import PLSRegression\\n    >>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]]\\n    >>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\\n    >>> pls2 = PLSRegression(n_components=2)\\n    >>> pls2.fit(X, Y)\\n    PLSRegression()\\n    >>> Y_pred = pls2.predict(X)\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/poisson-regressor\"} \":sklearn.regression/poisson-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [6 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|         :alpha |     1.000 |\\n| :fit-intercept |      true |\\n|      :max-iter |       100 |\\n|           :tol | 0.0001000 |\\n|       :verbose |         0 |\\n|    :warm-start |     false |\\n\"]]] [:span [:p/markdown \"Generalized Linear Model with a Poisson distribution.\\n\\n    This regressor uses the 'log' link function.\\n\\n    Read more in the User Guide: `Generalized_linear_regression`.\\n\\n    *Added in 0.23*\\n\\n    Parameters\\n    ----------\\n    - `alpha`: float, default=1\\n        Constant that multiplies the penalty term and thus determines the\\n        regularization strength. ``alpha = 0`` is equivalent to unpenalized\\n        GLMs. In this case, the design matrix `X` must have full column rank\\n        (no collinearities).\\n        Values must be in the range `[0.0, inf)`.\\n\\n    - `fit_intercept`: bool, default=True\\n        Specifies if a constant (a.k.a. bias or intercept) should be\\n        added to the linear predictor (X @ coef + intercept).\\n\\n    - `max_iter`: int, default=100\\n        The maximal number of iterations for the solver.\\n        Values must be in the range `[1, inf)`.\\n\\n    - `tol`: float, default=1e-4\\n        Stopping criterion. For the lbfgs solver,\\n        the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol``\\n        where ``g_j`` is the j-th component of the gradient (derivative) of\\n        the objective function.\\n        Values must be in the range `(0.0, inf)`.\\n\\n    - `warm_start`: bool, default=False\\n        If set to ``True``, reuse the solution of the previous call to ``fit``\\n        as initialization for ``coef_`` and ``intercept_`` .\\n\\n    - `verbose`: int, default=0\\n        For the lbfgs solver set verbose to any positive number for verbosity.\\n        Values must be in the range `[0, inf)`.\\n\\n    Attributes\\n    ----------\\n    - `coef_`: array of shape (n_features,)\\n        Estimated coefficients for the linear predictor (`X @ coef_ +\\n        intercept_`) in the GLM.\\n\\n    - `intercept_`: float\\n        Intercept (a.k.a. bias) added to linear predictor.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    - `n_iter_`: int\\n        Actual number of iterations used in the solver.\\n\\n    See Also\\n    --------\\n    - `TweedieRegressor`: Generalized Linear Model with a Tweedie distribution.\\n\\n    Examples\\n    --------\\n    >>> from sklearn import linear_model\\n    >>> clf = linear_model.PoissonRegressor()\\n    >>> X = [[1, 2], [2, 3], [3, 4], [4, 3]]\\n    >>> y = [12, 17, 22, 21]\\n    >>> clf.fit(X, y)\\n    PoissonRegressor()\\n    >>> clf.score(X, y)\\n    0.990...\\n    >>> clf.coef_\\n    array([0.121..., 0.158...])\\n    >>> clf.intercept_\\n    2.088...\\n    >>> clf.predict([[1, 1], [3, 4]])\\n    array([10.676..., 21.875...])\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/quantile-regressor\"} \":sklearn.regression/quantile-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [5 2]:\\n\\n|           :name |       :default |\\n|-----------------|----------------|\\n|          :alpha |          1.000 |\\n|  :fit-intercept |           true |\\n|       :quantile |         0.5000 |\\n|         :solver | interior-point |\\n| :solver-options |                |\\n\"]]] [:span [:p/markdown \"Linear regression model that predicts conditional quantiles.\\n\\n    The linear `QuantileRegressor` optimizes the pinball loss for a\\n    desired `quantile` and is robust to outliers.\\n\\n    This model uses an L1 regularization like\\n    `~sklearn.linear_model.Lasso`.\\n\\n    Read more in the User Guide: `quantile_regression`.\\n\\n    *Added in 1.0*\\n\\n    Parameters\\n    ----------\\n    - `quantile`: float, default=0.5\\n        The quantile that the model tries to predict. It must be strictly\\n        between 0 and 1. If 0.5 (default), the model predicts the 50%\\n        quantile, i.e. the median.\\n\\n    - `alpha`: float, default=1.0\\n        Regularization constant that multiplies the L1 penalty term.\\n\\n    - `fit_intercept`: bool, default=True\\n        Whether or not to fit the intercept.\\n\\n    - `solver`: {'highs-ds', 'highs-ipm', 'highs', 'interior-point',             'revised simplex'}, default='interior-point'\\n        Method used by `scipy.optimize.linprog` to solve the linear\\n        programming formulation. Note that the highs methods are recommended\\n        for usage with `scipy>=1.6.0` because they are the fastest ones.\\n        Solvers \\\"highs-ds\\\", \\\"highs-ipm\\\" and \\\"highs\\\" support\\n        sparse input data and, in fact, always convert to sparse csc.\\n\\n    - `solver_options`: dict, default=None\\n        Additional parameters passed to `scipy.optimize.linprog` as\\n        options. If `None` and if `solver='interior-point'`, then\\n        `{\\\"lstsq\\\": True}` is passed to `scipy.optimize.linprog` for the\\n        sake of stability.\\n\\n    Attributes\\n    ----------\\n    - `coef_`: array of shape (n_features,)\\n        Estimated coefficients for the features.\\n\\n    - `intercept_`: float\\n        The intercept of the model, aka bias term.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    - `n_iter_`: int\\n        The actual number of iterations performed by the solver.\\n\\n    See Also\\n    --------\\n    - `Lasso`: The Lasso is a linear model that estimates sparse coefficients\\n        with l1 regularization.\\n    - `HuberRegressor`: Linear regression model that is robust to outliers.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.linear_model import QuantileRegressor\\n    >>> import numpy as np\\n    >>> n_samples, n_features = 10, 2\\n    >>> rng = np.random.RandomState(0)\\n    >>> y = rng.randn(n_samples)\\n    >>> X = rng.randn(n_samples, n_features)\\n    >>> reg = QuantileRegressor(quantile=0.8).fit(X, y)\\n    >>> np.mean(y <= reg.predict(X))\\n    0.8\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/radius-neighbors-regressor\"} \":sklearn.regression/radius-neighbors-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [8 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|     :algorithm |      auto |\\n|     :leaf-size |        30 |\\n|        :metric | minkowski |\\n| :metric-params |           |\\n|        :n-jobs |           |\\n|             :p |         2 |\\n|        :radius |     1.000 |\\n|       :weights |   uniform |\\n\"]]] [:span [:p/markdown \"Regression based on neighbors within a fixed radius.\\n\\n    The target is predicted by local interpolation of the targets\\n    associated of the nearest neighbors in the training set.\\n\\n    Read more in the User Guide: `regression`.\\n\\n    *Added in 0.9*\\n\\n    Parameters\\n    ----------\\n    - `radius`: float, default=1.0\\n        Range of parameter space to use by default for `radius_neighbors`\\n        queries.\\n\\n    - `weights`: {'uniform', 'distance'} or callable, default='uniform'\\n        Weight function used in prediction.  Possible values:\\n\\n        - 'uniform' : uniform weights.  All points in each neighborhood\\n          are weighted equally.\\n        - 'distance' : weight points by the inverse of their distance.\\n          in this case, closer neighbors of a query point will have a\\n          greater influence than neighbors which are further away.\\n        - [callable] : a user-defined function which accepts an\\n          array of distances, and returns an array of the same shape\\n          containing the weights.\\n\\n        Uniform weights are used by default.\\n\\n    - `algorithm`: {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\\n        Algorithm used to compute the nearest neighbors:\\n\\n        - 'ball_tree' will use `BallTree`\\n        - 'kd_tree' will use `KDTree`\\n        - 'brute' will use a brute-force search.\\n        - 'auto' will attempt to decide the most appropriate algorithm\\n          based on the values passed to `fit` method.\\n\\n        Note: fitting on sparse input will override the setting of\\n        this parameter, using brute force.\\n\\n    - `leaf_size`: int, default=30\\n        Leaf size passed to BallTree or KDTree.  This can affect the\\n        speed of the construction and query, as well as the memory\\n        required to store the tree.  The optimal value depends on the\\n        nature of the problem.\\n\\n    - `p`: int, default=2\\n        Power parameter for the Minkowski metric. When p = 1, this is\\n        equivalent to using manhattan_distance (l1), and euclidean_distance\\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\\n\\n    - `metric`: str or callable, default='minkowski'\\n        The distance metric to use for the tree.  The default metric is\\n        minkowski, and with p=2 is equivalent to the standard Euclidean\\n        metric. See the documentation of `DistanceMetric` for a\\n        list of available metrics.\\n        If metric is \\\"precomputed\\\", X is assumed to be a distance matrix and\\n        must be square during fit. X may be a `sparse graph`,\\n        in which case only \\\"nonzero\\\" elements may be considered neighbors.\\n\\n    - `metric_params`: dict, default=None\\n        Additional keyword arguments for the metric function.\\n\\n    - `n_jobs`: int, default=None\\n        The number of parallel jobs to run for neighbors search.\\n        ``None`` means 1 unless in a `joblib.parallel_backend` context.\\n        ``-1`` means using all processors. See `Glossary <n_jobs>`\\n        for more details.\\n\\n    Attributes\\n    ----------\\n    - `effective_metric_`: str or callable\\n        The distance metric to use. It will be same as the `metric` parameter\\n        or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\\n        'minkowski' and `p` parameter set to 2.\\n\\n    - `effective_metric_params_`: dict\\n        Additional keyword arguments for the metric function. For most metrics\\n        will be same with `metric_params` parameter, but may also contain the\\n        `p` parameter value if the `effective_metric_` attribute is set to\\n        'minkowski'.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    - `n_samples_fit_`: int\\n        Number of samples in the fitted data.\\n\\n    See Also\\n    --------\\n    - `NearestNeighbors`: Regression based on nearest neighbors.\\n    - `KNeighborsRegressor`: Regression based on k-nearest neighbors.\\n    - `KNeighborsClassifier`: Classifier based on the k-nearest neighbors.\\n    - `RadiusNeighborsClassifier`: Classifier based on neighbors within a given radius.\\n\\n    Notes\\n    -----\\n    See Nearest Neighbors: `neighbors` in the online documentation\\n    for a discussion of the choice of ``algorithm`` and ``leaf_size``.\\n\\n    https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\\n\\n    Examples\\n    --------\\n    >>> X = [[0], [1], [2], [3]]\\n    >>> y = [0, 0, 1, 1]\\n    >>> from sklearn.neighbors import RadiusNeighborsRegressor\\n    >>> neigh = RadiusNeighborsRegressor(radius=1.0)\\n    >>> neigh.fit(X, y)\\n    RadiusNeighborsRegressor(...)\\n    >>> print(neigh.predict([[1.5]]))\\n    [0.5]\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/random-forest-regressor\"} \":sklearn.regression/random-forest-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [17 2]:\\n\\n|                     :name |      :default |\\n|---------------------------|---------------|\\n| :min-weight-fraction-leaf |         0.000 |\\n|           :max-leaf-nodes |               |\\n|    :min-impurity-decrease |         0.000 |\\n|        :min-samples-split |         2.000 |\\n|                :bootstrap |          true |\\n|                :ccp-alpha |         0.000 |\\n|                   :n-jobs |               |\\n|             :random-state |               |\\n|                :oob-score |         false |\\n|         :min-samples-leaf |             1 |\\n|             :max-features |         1.000 |\\n|               :warm-start |         false |\\n|                :max-depth |               |\\n|             :n-estimators |           100 |\\n|              :max-samples |               |\\n|                :criterion | squared_error |\\n|                  :verbose |             0 |\\n\"]]] [:span [:p/markdown \"\\n    A random forest regressor.\\n\\n    A random forest is a meta estimator that fits a number of classifying\\n    decision trees on various sub-samples of the dataset and uses averaging\\n    to improve the predictive accuracy and control over-fitting.\\n    The sub-sample size is controlled with the `max_samples` parameter if\\n    `bootstrap=True` (default), otherwise the whole dataset is used to build\\n    each tree.\\n\\n    Read more in the User Guide: `forest`.\\n\\n    Parameters\\n    ----------\\n    - `n_estimators`: int, default=100\\n        The number of trees in the forest.\\n\\n        *Changed in 0.22*\\n           The default value of ``n_estimators`` changed from 10 to 100\\n           in 0.22.\\n\\n    - `criterion`: {\\\"squared_error\\\", \\\"absolute_error\\\", \\\"poisson\\\"},             default=\\\"squared_error\\\"\\n        The function to measure the quality of a split. Supported criteria\\n        are \\\"squared_error\\\" for the mean squared error, which is equal to\\n        variance reduction as feature selection criterion, \\\"absolute_error\\\"\\n        for the mean absolute error, and \\\"poisson\\\" which uses reduction in\\n        Poisson deviance to find splits.\\n        Training using \\\"absolute_error\\\" is significantly slower\\n        than when using \\\"squared_error\\\".\\n\\n        *Added in 0.18*\\n           Mean Absolute Error (MAE) criterion.\\n\\n        *Added in 1.0*\\n           Poisson criterion.\\n\\n        *Deprecated since 1.0*\\n            Criterion \\\"mse\\\" was deprecated in v1.0 and will be removed in\\n            version 1.2. Use `criterion=\\\"squared_error\\\"` which is equivalent.\\n\\n        *Deprecated since 1.0*\\n            Criterion \\\"mae\\\" was deprecated in v1.0 and will be removed in\\n            version 1.2. Use `criterion=\\\"absolute_error\\\"` which is equivalent.\\n\\n    - `max_depth`: int, default=None\\n        The maximum depth of the tree. If None, then nodes are expanded until\\n        all leaves are pure or until all leaves contain less than\\n        min_samples_split samples.\\n\\n    - `min_samples_split`: int or float, default=2\\n        The minimum number of samples required to split an internal node:\\n\\n        - If int, then consider `min_samples_split` as the minimum number.\\n        - If float, then `min_samples_split` is a fraction and\\n          `ceil(min_samples_split * n_samples)` are the minimum\\n          number of samples for each split.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n    - `min_samples_leaf`: int or float, default=1\\n        The minimum number of samples required to be at a leaf node.\\n        A split point at any depth will only be considered if it leaves at\\n        least ``min_samples_leaf`` training samples in each of the left and\\n        right branches.  This may have the effect of smoothing the model,\\n        especially in regression.\\n\\n        - If int, then consider `min_samples_leaf` as the minimum number.\\n        - If float, then `min_samples_leaf` is a fraction and\\n          `ceil(min_samples_leaf * n_samples)` are the minimum\\n          number of samples for each node.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n    - `min_weight_fraction_leaf`: float, default=0.0\\n        The minimum weighted fraction of the sum total of weights (of all\\n        the input samples) required to be at a leaf node. Samples have\\n        equal weight when sample_weight is not provided.\\n\\n    - `max_features`: {\\\"sqrt\\\", \\\"log2\\\", None}, int or float, default=1.0\\n        The number of features to consider when looking for the best split:\\n\\n        - If int, then consider `max_features` features at each split.\\n        - If float, then `max_features` is a fraction and\\n          `round(max_features * n_features)` features are considered at each\\n          split.\\n        - If \\\"auto\\\", then `max_features=n_features`.\\n        - If \\\"sqrt\\\", then `max_features=sqrt(n_features)`.\\n        - If \\\"log2\\\", then `max_features=log2(n_features)`.\\n        - If None or 1.0, then `max_features=n_features`.\\n\\n\\n---\\n**Note**\\n\\nThe default of 1.0 is equivalent to bagged trees and more\\nrandomness can be achieved by setting smaller values, e.g. 0.3.\\n\\nersionchanged:: 1.1\\nThe default of `max_features` changed from `\\\"auto\\\"` to 1.0.\\n\\neprecated:: 1.1\\nThe `\\\"auto\\\"` option was deprecated in 1.1 and will be removed\\nin 1.3.\\n\\n: the search for a split does not stop until at least one\\nd partition of the node samples is found, even if it requires to\\nctively inspect more than ``max_features`` features.\\n\\n_nodes : int, default=None\\n trees with ``max_leaf_nodes`` in best-first fashion.\\n nodes are defined as relative reduction in impurity.\\none then unlimited number of leaf nodes.\\n\\nrity_decrease : float, default=0.0\\nde will be split if this split induces a decrease of the impurity\\nter than or equal to this value.\\n\\nweighted impurity decrease equation is the following::\\n\\nN_t / N * (impurity - N_t_R / N_t * right_impurity\\n                    - N_t_L / N_t * left_impurity)\\n\\ne ``N`` is the total number of samples, ``N_t`` is the number of\\nles at the current node, ``N_t_L`` is the number of samples in the\\n child, and ``N_t_R`` is the number of samples in the right child.\\n\\n`, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\\n`sample_weight`` is passed.\\n\\nersionadded:: 0.19\\n\\np : bool, default=True\\nher bootstrap samples are used when building trees. If False, the\\ne dataset is used to build each tree.\\n\\ne : bool, default=False\\nher to use out-of-bag samples to estimate the generalization score.\\n available if bootstrap=True.\\n\\n int, default=None\\nnumber of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\\nh:`decision_path` and :meth:`apply` are all parallelized over the\\ns. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\\next. ``-1`` means using all processors. See :term:`Glossary\\nobs>` for more details.\\n\\ntate : int, RandomState instance or None, default=None\\nrols both the randomness of the bootstrapping of the samples used\\n building trees (if ``bootstrap=True``) and the sampling of the\\nures to consider when looking for the best split at each node\\n``max_features < n_features``).\\n:term:`Glossary <random_state>` for details.\\n\\n: int, default=0\\nrols the verbosity when fitting and predicting.\\n\\nrt : bool, default=False\\n set to ``True``, reuse the solution of the previous call to fit\\nadd more estimators to the ensemble, otherwise, just fit a whole\\nforest. See :term:`the Glossary <warm_start>`.\\n\\na : non-negative float, default=0.0\\nlexity parameter used for Minimal Cost-Complexity Pruning. The\\nree with the largest cost complexity that is smaller than\\np_alpha`` will be chosen. By default, no pruning is performed. See\\n:`minimal_cost_complexity_pruning` for details.\\n\\nersionadded:: 0.22\\n\\nles : int or float, default=None\\nootstrap is True, the number of samples to draw from X\\nrain each base estimator.\\n\\n None (default), then draw `X.shape[0]` samples.\\n int, then draw `max_samples` samples.\\n float, then draw `max_samples * X.shape[0]` samples. Thus,\\nax_samples` should be in the interval `(0.0, 1.0]`.\\n\\nersionadded:: 0.22\\n\\nes\\n--\\nimator_ : DecisionTreeRegressor\\nchild estimator template used to create the collection of fitted\\nestimators.\\n\\nrs_ : list of DecisionTreeRegressor\\ncollection of fitted sub-estimators.\\n\\nimportances_ : ndarray of shape (n_features,)\\nimpurity-based feature importances.\\nhigher, the more important the feature.\\nimportance of a feature is computed as the (normalized)\\nl reduction of the criterion brought by that feature.  It is also\\nn as the Gini importance.\\n\\ning: impurity-based feature importances can be misleading for\\n cardinality features (many unique values). See\\nc:`sklearn.inspection.permutation_importance` as an alternative.\\n\\nes_ : int\\nnumber of features when ``fit`` is performed.\\n\\neprecated:: 1.0\\nAttribute `n_features_` was deprecated in version 1.0 and will be\\nremoved in 1.2. Use `n_features_in_` instead.\\n\\nes_in_ : int\\ner of features seen during :term:`fit`.\\n\\nersionadded:: 0.24\\n\\nnames_in_ : ndarray of shape (`n_features_in_`,)\\ns of features seen during :term:`fit`. Defined only when `X`\\nfeature names that are all strings.\\n\\nersionadded:: 1.0\\n\\ns_ : int\\nnumber of outputs when ``fit`` is performed.\\n\\ne_ : float\\ne of the training dataset obtained using an out-of-bag estimate.\\n attribute exists only when ``oob_score`` is True.\\n\\niction_ : ndarray of shape (n_samples,) or (n_samples, n_outputs)\\niction computed with out-of-bag estimate on the training set.\\n attribute exists only when ``oob_score`` is True.\\n\\n\\n\\ntree.DecisionTreeRegressor : A decision tree regressor.\\nensemble.ExtraTreesRegressor : Ensemble of extremely randomized\\n regressors.\\n\\n\\n\\nult values for the parameters controlling the size of the trees\\nmax_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\\n trees which can potentially be very large on some data sets. To\\nemory consumption, the complexity and size of the trees should be\\ned by setting those parameter values.\\n\\nures are always randomly permuted at each split. Therefore,\\n found split may vary, even with the same training data,\\natures=n_features`` and ``bootstrap=False``, if the improvement\\nriterion is identical for several splits enumerated during the\\nf the best split. To obtain a deterministic behaviour during\\n ``random_state`` has to be fixed.\\n\\nult value ``max_features=\\\"auto\\\"`` uses ``n_features``\\nhan ``n_features / 3``. The latter was originally suggested in\\nreas the former was more recently justified empirically in [2].\\n\\nes\\n--\\n. Breiman, \\\"Random Forests\\\", Machine Learning, 45(1), 5-32, 2001.\\n\\n. Geurts, D. Ernst., and L. Wehenkel, \\\"Extremely randomized\\nrees\\\", Machine Learning, 63(1), 3-42, 2006.\\n\\n\\n\\n sklearn.ensemble import RandomForestRegressor\\n sklearn.datasets import make_regression\\n = make_regression(n_features=4, n_informative=2,\\n                   random_state=0, shuffle=False)\\n = RandomForestRegressor(max_depth=2, random_state=0)\\n.fit(X, y)\\nrestRegressor(...)\\nt(regr.predict([[0, 0, 0, 0]]))\\n7858]\\n\\n---\\n\"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/ransac-regressor\"} \":sklearn.regression/ransac-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [13 2]:\\n\\n|               :name |       :default |\\n|---------------------|----------------|\\n|      :is-data-valid |                |\\n|          :max-skips |       INFINITY |\\n|       :random-state |                |\\n|        :min-samples |                |\\n|   :stop-probability |         0.9900 |\\n|          :estimator |                |\\n|     :stop-n-inliers |       INFINITY |\\n|     :base-estimator |     deprecated |\\n|         :max-trials |            100 |\\n| :residual-threshold |                |\\n|     :is-model-valid |                |\\n|               :loss | absolute_error |\\n|         :stop-score |       INFINITY |\\n\"]]] [:span [:p/markdown \"RANSAC (RANdom SAmple Consensus) algorithm.\\n\\n    RANSAC is an iterative algorithm for the robust estimation of parameters\\n    from a subset of inliers from the complete data set.\\n\\n    Read more in the User Guide: `ransac_regression`.\\n\\n    Parameters\\n    ----------\\n    - `estimator`: object, default=None\\n        Base estimator object which implements the following methods:\\n\\n         * `fit(X, y)`: Fit model to given training data and target values.\\n         * `score(X, y)`: Returns the mean accuracy on the given test data,\\n           which is used for the stop criterion defined by `stop_score`.\\n           Additionally, the score is used to decide which of two equally\\n           large consensus sets is chosen as the better one.\\n         * `predict(X)`: Returns predicted values using the linear model,\\n           which is used to compute residual error using loss function.\\n\\n        If `estimator` is None, then\\n        `~sklearn.linear_model.LinearRegression` is used for\\n        target values of dtype float.\\n\\n        Note that the current implementation only supports regression\\n        estimators.\\n\\n    - `min_samples`: int (>= 1) or float ([0, 1]), default=None\\n        Minimum number of samples chosen randomly from original data. Treated\\n        as an absolute number of samples for `min_samples >= 1`, treated as a\\n        relative number `ceil(min_samples * X.shape[0])` for\\n        `min_samples < 1`. This is typically chosen as the minimal number of\\n        samples necessary to estimate the given `estimator`. By default a\\n        ``sklearn.linear_model.LinearRegression()`` estimator is assumed and\\n        `min_samples` is chosen as ``X.shape[1] + 1``. This parameter is highly\\n        dependent upon the model, so if a `estimator` other than\\n        `linear_model.LinearRegression` is used, the user is\\n        encouraged to provide a value.\\n\\n        *Deprecated since 1.0*\\n           Not setting `min_samples` explicitly will raise an error in version\\n           1.2 for models other than\\n           `~sklearn.linear_model.LinearRegression`. To keep the old\\n           default behavior, set `min_samples=X.shape[1] + 1` explicitly.\\n\\n    - `residual_threshold`: float, default=None\\n        Maximum residual for a data sample to be classified as an inlier.\\n        By default the threshold is chosen as the MAD (median absolute\\n        deviation) of the target values `y`. Points whose residuals are\\n        strictly equal to the threshold are considered as inliers.\\n\\n    - `is_data_valid`: callable, default=None\\n        This function is called with the randomly selected data before the\\n        model is fitted to it: `is_data_valid(X, y)`. If its return value is\\n        False the current randomly chosen sub-sample is skipped.\\n\\n    - `is_model_valid`: callable, default=None\\n        This function is called with the estimated model and the randomly\\n        selected data: `is_model_valid(model, X, y)`. If its return value is\\n        False the current randomly chosen sub-sample is skipped.\\n        Rejecting samples with this function is computationally costlier than\\n        with `is_data_valid`. `is_model_valid` should therefore only be used if\\n        the estimated model is needed for making the rejection decision.\\n\\n    - `max_trials`: int, default=100\\n        Maximum number of iterations for random sample selection.\\n\\n    - `max_skips`: int, default=np.inf\\n        Maximum number of iterations that can be skipped due to finding zero\\n        inliers or invalid data defined by ``is_data_valid`` or invalid models\\n        defined by ``is_model_valid``.\\n\\n        *Added in 0.19*\\n\\n    - `stop_n_inliers`: int, default=np.inf\\n        Stop iteration if at least this number of inliers are found.\\n\\n    - `stop_score`: float, default=np.inf\\n        Stop iteration if score is greater equal than this threshold.\\n\\n    - `stop_probability`: float in range [0, 1], default=0.99\\n        RANSAC iteration stops if at least one outlier-free set of the training\\n        data is sampled in RANSAC. This requires to generate at least N\\n        samples (iterations)\\n\\n```python\\nN >= log(1 - probability) / log(1 - e**m)\\n\\ne the probability (confidence) is typically set to high value such\\n.99 (the default) and e is the current fraction of inliers w.r.t.\\ntotal number of samples.\\n\\ntr, callable, default='absolute_error'\\nng inputs, 'absolute_error' and 'squared_error' are supported which\\n the absolute error and squared error per sample respectively.\\n\\n`loss`` is a callable, then it should be a function that takes\\narrays as inputs, the true and predicted value and returns a 1-D\\ny with the i-th value of the array corresponding to the loss\\n`X[i]``.\\n\\nhe loss on a sample is greater than the ``residual_threshold``,\\n this sample is classified as an outlier.\\n\\nersionadded:: 0.18\\n\\neprecated:: 1.0\\nThe loss 'squared_loss' was deprecated in v1.0 and will be removed\\nin version 1.2. Use `loss='squared_error'` which is equivalent.\\n\\neprecated:: 1.0\\nThe loss 'absolute_loss' was deprecated in v1.0 and will be removed\\nin version 1.2. Use `loss='absolute_error'` which is equivalent.\\n\\ntate : int, RandomState instance, default=None\\ngenerator used to initialize the centers.\\n an int for reproducible output across multiple function calls.\\n:term:`Glossary <random_state>`.\\n\\nimator : object, default=\\\"deprecated\\\"\\n`estimator` instead.\\n\\neprecated:: 1.1\\n`base_estimator` is deprecated and will be removed in 1.3.\\nUse `estimator` instead.\\n\\nes\\n--\\nr_ : object\\n fitted model (copy of the `estimator` object).\\n\\n_ : int\\ner of random selection trials until one of the stop criteria is\\n It is always ``<= max_trials``.\\n\\nask_ : bool array of shape [n_samples]\\nean mask of inliers classified as ``True``.\\n\\nno_inliers_ : int\\ner of iterations skipped due to finding zero inliers.\\n\\nersionadded:: 0.19\\n\\ninvalid_data_ : int\\ner of iterations skipped due to invalid data defined by\\n_data_valid``.\\n\\nersionadded:: 0.19\\n\\ninvalid_model_ : int\\ner of iterations skipped due to an invalid model defined by\\n_model_valid``.\\n\\nersionadded:: 0.19\\n\\nes_in_ : int\\ner of features seen during :term:`fit`.\\n\\nersionadded:: 0.24\\n\\nnames_in_ : ndarray of shape (`n_features_in_`,)\\ns of features seen during :term:`fit`. Defined only when `X`\\nfeature names that are all strings.\\n\\nersionadded:: 1.0\\n\\n\\n\\nressor : Linear regression model that is robust to outliers.\\nRegressor : Theil-Sen Estimator robust multivariate regression model.\\nssor : Fitted by minimizing a regularized empirical loss with SGD.\\n\\nes\\n--\\nttps://en.wikipedia.org/wiki/RANSAC\\nttps://www.sri.com/sites/default/files/publications/ransac-publication.pdf\\nttp://www.bmva.org/bmvc/2009/Papers/Paper355/Paper355.pdf\\n\\n\\n\\n sklearn.linear_model import RANSACRegressor\\n sklearn.datasets import make_regression\\n = make_regression(\\nn_samples=200, n_features=2, noise=4.0, random_state=0)\\n= RANSACRegressor(random_state=0).fit(X, y)\\nscore(X, y)\\n.\\npredict(X[:1,])\\n31.9417...])\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/ridge\"} \":sklearn.regression/ridge\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [9 2]:\\n\\n|          :name |   :default |\\n|----------------|------------|\\n|     :normalize | deprecated |\\n|      :positive |      false |\\n|           :tol |   0.001000 |\\n|        :solver |       auto |\\n|      :max-iter |            |\\n|  :random-state |            |\\n|        :copy-x |       true |\\n| :fit-intercept |       true |\\n|         :alpha |      1.000 |\\n\"]]] [:span [:p/markdown \"Linear least squares with l2 regularization.\\n\\n    Minimizes the objective function\\n\\n```python\\n||y - Xw||^2_2 + alpha * ||w||^2_2\\n\\nThis model solves a regression model where the loss function is\\nthe linear least squares function and regularization is given by\\nthe l2-norm. Also known as Ridge Regression or Tikhonov regularization.\\nThis estimator has built-in support for multi-variate regression\\n(i.e., when y is a 2d-array of shape (n_samples, n_targets)).\\n\\nRead more in the :ref:`User Guide <ridge_regression>`.\\n\\nParameters\\n----------\\nalpha : {float, ndarray of shape (n_targets,)}, default=1.0\\n    Constant that multiplies the L2 term, controlling regularization\\n    strength. `alpha` must be a non-negative float i.e. in `[0, inf)`.\\n\\n    When `alpha = 0`, the objective is equivalent to ordinary least\\n    squares, solved by the :class:`LinearRegression` object. For numerical\\n    reasons, using `alpha = 0` with the `Ridge` object is not advised.\\n    Instead, you should use the :class:`LinearRegression` object.\\n\\n    If an array is passed, penalties are assumed to be specific to the\\n    targets. Hence they must correspond in number.\\n\\nfit_intercept : bool, default=True\\n    Whether to fit the intercept for this model. If set\\n    to false, no intercept will be used in calculations\\n    (i.e. ``X`` and ``y`` are expected to be centered).\\n\\nnormalize : bool, default=False\\n    This parameter is ignored when ``fit_intercept`` is set to False.\\n    If True, the regressors X will be normalized before regression by\\n    subtracting the mean and dividing by the l2-norm.\\n    If you wish to standardize, please use\\n    :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\\n    on an estimator with ``normalize=False``.\\n\\n    .. deprecated:: 1.0\\n        ``normalize`` was deprecated in version 1.0 and\\n        will be removed in 1.2.\\n\\ncopy_X : bool, default=True\\n    If True, X will be copied; else, it may be overwritten.\\n\\nmax_iter : int, default=None\\n    Maximum number of iterations for conjugate gradient solver.\\n    For 'sparse_cg' and 'lsqr' solvers, the default value is determined\\n    by scipy.sparse.linalg. For 'sag' solver, the default value is 1000.\\n    For 'lbfgs' solver, the default value is 15000.\\n\\ntol : float, default=1e-3\\n    Precision of the solution.\\n\\nsolver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg',             'sag', 'saga', 'lbfgs'}, default='auto'\\n    Solver to use in the computational routines:\\n\\n    - 'auto' chooses the solver automatically based on the type of data.\\n\\n    - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\\n      coefficients. It is the most stable solver, in particular more stable\\n      for singular matrices than 'cholesky' at the cost of being slower.\\n\\n    - 'cholesky' uses the standard scipy.linalg.solve function to\\n      obtain a closed-form solution.\\n\\n    - 'sparse_cg' uses the conjugate gradient solver as found in\\n      scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\\n      more appropriate than 'cholesky' for large-scale data\\n      (possibility to set `tol` and `max_iter`).\\n\\n    - 'lsqr' uses the dedicated regularized least-squares routine\\n      scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\\n      procedure.\\n\\n    - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\\n      its improved, unbiased version named SAGA. Both methods also use an\\n      iterative procedure, and are often faster than other solvers when\\n      both n_samples and n_features are large. Note that 'sag' and\\n      'saga' fast convergence is only guaranteed on features with\\n      approximately the same scale. You can preprocess the data with a\\n      scaler from sklearn.preprocessing.\\n\\n    - 'lbfgs' uses L-BFGS-B algorithm implemented in\\n      `scipy.optimize.minimize`. It can be used only when `positive`\\n      is True.\\n\\n    All solvers except 'svd' support both dense and sparse data. However, only\\n    'lsqr', 'sag', 'sparse_cg', and 'lbfgs' support sparse input when\\n    `fit_intercept` is True.\\n\\n    .. versionadded:: 0.17\\n       Stochastic Average Gradient descent solver.\\n    .. versionadded:: 0.19\\n       SAGA solver.\\n\\npositive : bool, default=False\\n    When set to ``True``, forces the coefficients to be positive.\\n    Only 'lbfgs' solver is supported in this case.\\n\\nrandom_state : int, RandomState instance, default=None\\n    Used when ``solver`` == 'sag' or 'saga' to shuffle the data.\\n    See :term:`Glossary <random_state>` for details.\\n\\n    .. versionadded:: 0.17\\n       `random_state` to support Stochastic Average Gradient.\\n\\nAttributes\\n----------\\ncoef_ : ndarray of shape (n_features,) or (n_targets, n_features)\\n    Weight vector(s).\\n\\nintercept_ : float or ndarray of shape (n_targets,)\\n    Independent term in decision function. Set to 0.0 if\\n    ``fit_intercept = False``.\\n\\nn_iter_ : None or ndarray of shape (n_targets,)\\n    Actual number of iterations for each target. Available only for\\n    sag and lsqr solvers. Other solvers will return None.\\n\\n    .. versionadded:: 0.17\\n\\nn_features_in_ : int\\n    Number of features seen during :term:`fit`.\\n\\n    .. versionadded:: 0.24\\n\\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\\n    Names of features seen during :term:`fit`. Defined only when `X`\\n    has feature names that are all strings.\\n\\n    .. versionadded:: 1.0\\n\\nSee Also\\n--------\\nRidgeClassifier : Ridge classifier.\\nRidgeCV : Ridge regression with built-in cross validation.\\n:class:`~sklearn.kernel_ridge.KernelRidge` : Kernel ridge regression\\n    combines ridge regression with the kernel trick.\\n\\nNotes\\n-----\\nRegularization improves the conditioning of the problem and\\nreduces the variance of the estimates. Larger values specify stronger\\nregularization. Alpha corresponds to ``1 / (2C)`` in other linear\\nmodels such as :class:`~sklearn.linear_model.LogisticRegression` or\\n:class:`~sklearn.svm.LinearSVC`.\\n\\nExamples\\n--------\\n>>> from sklearn.linear_model import Ridge\\n>>> import numpy as np\\n>>> n_samples, n_features = 10, 5\\n>>> rng = np.random.RandomState(0)\\n>>> y = rng.randn(n_samples)\\n>>> X = rng.randn(n_samples, n_features)\\n>>> clf = Ridge(alpha=1.0)\\n>>> clf.fit(X, y)\\nRidge()\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/ridge-cv\"} \":sklearn.regression/ridge-cv\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [8 2]:\\n\\n|             :name |       :default |\\n|-------------------|----------------|\\n| :alpha-per-target |          false |\\n|           :alphas | [0.1 1.0 10.0] |\\n|               :cv |                |\\n|    :fit-intercept |           true |\\n|         :gcv-mode |                |\\n|        :normalize |     deprecated |\\n|          :scoring |                |\\n|  :store-cv-values |          false |\\n\"]]] [:span [:p/markdown \"Ridge regression with built-in cross-validation.\\n\\n    See glossary entry for `cross-validation estimator`.\\n\\n    By default, it performs efficient Leave-One-Out Cross-Validation.\\n\\n    Read more in the User Guide: `ridge_regression`.\\n\\n    Parameters\\n    ----------\\n    - `alphas`: ndarray of shape (n_alphas,), default=(0.1, 1.0, 10.0)\\n        Array of alpha values to try.\\n        Regularization strength; must be a positive float. Regularization\\n        improves the conditioning of the problem and reduces the variance of\\n        the estimates. Larger values specify stronger regularization.\\n        Alpha corresponds to ``1 / (2C)`` in other linear models such as\\n        `~sklearn.linear_model.LogisticRegression` or\\n        `~sklearn.svm.LinearSVC`.\\n        If using Leave-One-Out cross-validation, alphas must be positive.\\n\\n    - `fit_intercept`: bool, default=True\\n        Whether to calculate the intercept for this model. If set\\n        to false, no intercept will be used in calculations\\n        (i.e. data is expected to be centered).\\n\\n    - `normalize`: bool, default=False\\n        This parameter is ignored when ``fit_intercept`` is set to False.\\n        If True, the regressors X will be normalized before regression by\\n        subtracting the mean and dividing by the l2-norm.\\n        If you wish to standardize, please use\\n        `~sklearn.preprocessing.StandardScaler` before calling ``fit``\\n        on an estimator with ``normalize=False``.\\n\\n        *Deprecated since 1.0*\\n            ``normalize`` was deprecated in version 1.0 and will be removed in\\n            1.2.\\n\\n    - `scoring`: str, callable, default=None\\n        A string (see model evaluation documentation) or\\n        a scorer callable object / function with signature\\n        ``scorer(estimator, X, y)``.\\n        If None, the negative mean squared error if cv is 'auto' or None\\n        (i.e. when using leave-one-out cross-validation), and r2 score\\n        otherwise.\\n\\n    - `cv`: int, cross-validation generator or an iterable, default=None\\n        Determines the cross-validation splitting strategy.\\n        Possible inputs for cv are:\\n\\n        - None, to use the efficient Leave-One-Out cross-validation\\n        - integer, to specify the number of folds.\\n        - `CV splitter`,\\n        - An iterable yielding (train, test) splits as arrays of indices.\\n\\n        For integer/None inputs, if ``y`` is binary or multiclass,\\n        `~sklearn.model_selection.StratifiedKFold` is used, else,\\n        `~sklearn.model_selection.KFold` is used.\\n\\n        Refer User Guide: `cross_validation` for the various\\n        cross-validation strategies that can be used here.\\n\\n    - `gcv_mode`: {'auto', 'svd', 'eigen'}, default='auto'\\n        Flag indicating which strategy to use when performing\\n        Leave-One-Out Cross-Validation. Options are\\n\\n```python\\n'auto' : use 'svd' if n_samples > n_features, otherwise use 'eigen'\\n'svd' : force use of singular value decomposition of X when X is\\n    dense, eigenvalue decomposition of X^T.X when X is sparse.\\n'eigen' : force computation via eigendecomposition of X.X^T\\n\\n'auto' mode is the default and is intended to pick the cheaper\\non of the two depending on the shape of the training data.\\n\\n_values : bool, default=False\\n indicating if the cross-validation values corresponding to\\n alpha should be stored in the ``cv_values_`` attribute (see\\nw). This flag is only compatible with ``cv=None`` (i.e. using\\ne-One-Out Cross-Validation).\\n\\nr_target : bool, default=False\\n indicating whether to optimize the alpha value (picked from the\\nhas` parameter list) for each target separately (for multi-output\\nings: multiple prediction targets). When set to `True`, after\\ning, the `alpha_` attribute will contain a value for each target.\\n set to `False`, a single alpha is used for all targets.\\n\\nersionadded:: 0.24\\n\\nes\\n--\\ns_ : ndarray of shape (n_samples, n_alphas) or             shape (n_samples, n_targets, n_alphas), optional\\ns-validation values for each alpha (only available if\\nore_cv_values=True`` and ``cv=None``). After ``fit()`` has been\\ned, this attribute will contain the mean squared errors if\\nring is None` otherwise it will contain standardized per point\\niction values.\\n\\nndarray of shape (n_features) or (n_targets, n_features)\\nht vector(s).\\n\\nt_ : float or ndarray of shape (n_targets,)\\npendent term in decision function. Set to 0.0 if\\nt_intercept = False``.\\n\\n float or ndarray of shape (n_targets,)\\nmated regularization parameter, or, if ``alpha_per_target=True``,\\nestimated regularization parameter for each target.\\n\\nre_ : float or ndarray of shape (n_targets,)\\ne of base estimator with best alpha, or, if\\npha_per_target=True``, a score for each target.\\n\\nersionadded:: 0.23\\n\\nes_in_ : int\\ner of features seen during :term:`fit`.\\n\\nersionadded:: 0.24\\n\\nnames_in_ : ndarray of shape (`n_features_in_`,)\\ns of features seen during :term:`fit`. Defined only when `X`\\nfeature names that are all strings.\\n\\nersionadded:: 1.0\\n\\n\\n\\nRidge regression.\\nssifier : Classifier based on ridge regression on {-1, 1} labels.\\nssifierCV : Ridge classifier with built-in cross validation.\\n\\n\\n\\n sklearn.datasets import load_diabetes\\n sklearn.linear_model import RidgeCV\\n = load_diabetes(return_X_y=True)\\n= RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\\nscore(X, y)\\n.\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/sgd-regressor\"} \":sklearn.regression/sgd-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [19 2]:\\n\\n|                :name |      :default |\\n|----------------------|---------------|\\n|    :n-iter-no-change |             5 |\\n|       :learning-rate |    invscaling |\\n|             :average |         false |\\n|                 :tol |      0.001000 |\\n|      :early-stopping |         false |\\n|               :eta-0 |       0.01000 |\\n|             :shuffle |          true |\\n|             :penalty |            l2 |\\n|             :power-t |        0.2500 |\\n|            :max-iter |          1000 |\\n|        :random-state |               |\\n|       :fit-intercept |          true |\\n|               :alpha |     0.0001000 |\\n|          :warm-start |         false |\\n|           :l-1-ratio |        0.1500 |\\n| :validation-fraction |        0.1000 |\\n|                :loss | squared_error |\\n|             :verbose |             0 |\\n|             :epsilon |        0.1000 |\\n\"]]] [:span [:p/markdown \"Linear model fitted by minimizing a regularized empirical loss with SGD.\\n\\n    SGD stands for Stochastic Gradient Descent: the gradient of the loss is\\n    estimated each sample at a time and the model is updated along the way with\\n    a decreasing strength schedule (aka learning rate).\\n\\n    The regularizer is a penalty added to the loss function that shrinks model\\n    parameters towards the zero vector using either the squared euclidean norm\\n    L2 or the absolute norm L1 or a combination of both (Elastic Net). If the\\n    parameter update crosses the 0.0 value because of the regularizer, the\\n    update is truncated to 0.0 to allow for learning sparse models and achieve\\n    online feature selection.\\n\\n    This implementation works with data represented as dense numpy arrays of\\n    floating point values for the features.\\n\\n    Read more in the User Guide: `sgd`.\\n\\n    Parameters\\n    ----------\\n    - `loss`: str, default='squared_error'\\n        The loss function to be used. The possible values are 'squared_error',\\n        'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'\\n\\n        The 'squared_error' refers to the ordinary least squares fit.\\n        'huber' modifies 'squared_error' to focus less on getting outliers\\n        correct by switching from squared to linear loss past a distance of\\n        epsilon. 'epsilon_insensitive' ignores errors less than epsilon and is\\n        linear past that; this is the loss function used in SVR.\\n        'squared_epsilon_insensitive' is the same but becomes squared loss past\\n        a tolerance of epsilon.\\n\\n        More details about the losses formulas can be found in the\\n        User Guide: `sgd_mathematical_formulation`.\\n\\n        *Deprecated since 1.0*\\n            The loss 'squared_loss' was deprecated in v1.0 and will be removed\\n            in version 1.2. Use `loss='squared_error'` which is equivalent.\\n\\n    - `penalty`: {'l2', 'l1', 'elasticnet'}, default='l2'\\n        The penalty (aka regularization term) to be used. Defaults to 'l2'\\n        which is the standard regularizer for linear SVM models. 'l1' and\\n        'elasticnet' might bring sparsity to the model (feature selection)\\n        not achievable with 'l2'.\\n\\n    - `alpha`: float, default=0.0001\\n        Constant that multiplies the regularization term. The higher the\\n        value, the stronger the regularization.\\n        Also used to compute the learning rate when set to `learning_rate` is\\n        set to 'optimal'.\\n\\n    - `l1_ratio`: float, default=0.15\\n        The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\\n        l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\\n        Only used if `penalty` is 'elasticnet'.\\n\\n    - `fit_intercept`: bool, default=True\\n        Whether the intercept should be estimated or not. If False, the\\n        data is assumed to be already centered.\\n\\n    - `max_iter`: int, default=1000\\n        The maximum number of passes over the training data (aka epochs).\\n        It only impacts the behavior in the ``fit`` method, and not the\\n        `partial_fit` method.\\n\\n        *Added in 0.19*\\n\\n    - `tol`: float, default=1e-3\\n        The stopping criterion. If it is not None, training will stop\\n        when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive\\n        epochs.\\n        Convergence is checked against the training loss or the\\n        validation loss depending on the `early_stopping` parameter.\\n\\n        *Added in 0.19*\\n\\n    - `shuffle`: bool, default=True\\n        Whether or not the training data should be shuffled after each epoch.\\n\\n    - `verbose`: int, default=0\\n        The verbosity level.\\n\\n    - `epsilon`: float, default=0.1\\n        Epsilon in the epsilon-insensitive loss functions; only if `loss` is\\n        'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\\n        For 'huber', determines the threshold at which it becomes less\\n        important to get the prediction exactly right.\\n        For epsilon-insensitive, any differences between the current prediction\\n        and the correct label are ignored if they are less than this threshold.\\n\\n    - `random_state`: int, RandomState instance, default=None\\n        Used for shuffling the data, when ``shuffle`` is set to ``True``.\\n        Pass an int for reproducible output across multiple function calls.\\n        See `Glossary <random_state>`.\\n\\n    - `learning_rate`: str, default='invscaling'\\n        The learning rate schedule:\\n\\n        - 'constant': `eta = eta0`\\n        - 'optimal': `eta = 1.0 / (alpha * (t + t0))`\\n          where t0 is chosen by a heuristic proposed by Leon Bottou.\\n        - 'invscaling': `eta = eta0 / pow(t, power_t)`\\n        - 'adaptive': eta = eta0, as long as the training keeps decreasing.\\n          Each time n_iter_no_change consecutive epochs fail to decrease the\\n          training loss by tol or fail to increase validation score by tol if\\n          early_stopping is True, the current learning rate is divided by 5.\\n\\n            *Added in 0.20*\\n                Added 'adaptive' option\\n\\n    - `eta0`: float, default=0.01\\n        The initial learning rate for the 'constant', 'invscaling' or\\n        'adaptive' schedules. The default value is 0.01.\\n\\n    - `power_t`: float, default=0.25\\n        The exponent for inverse scaling learning rate.\\n\\n    - `early_stopping`: bool, default=False\\n        Whether to use early stopping to terminate training when validation\\n        score is not improving. If set to True, it will automatically set aside\\n        a fraction of training data as validation and terminate\\n        training when validation score returned by the `score` method is not\\n        improving by at least `tol` for `n_iter_no_change` consecutive\\n        epochs.\\n\\n        *Added in 0.20*\\n            Added 'early_stopping' option\\n\\n    - `validation_fraction`: float, default=0.1\\n        The proportion of training data to set aside as validation set for\\n        early stopping. Must be between 0 and 1.\\n        Only used if `early_stopping` is True.\\n\\n        *Added in 0.20*\\n            Added 'validation_fraction' option\\n\\n    - `n_iter_no_change`: int, default=5\\n        Number of iterations with no improvement to wait before stopping\\n        fitting.\\n        Convergence is checked against the training loss or the\\n        validation loss depending on the `early_stopping` parameter.\\n\\n        *Added in 0.20*\\n            Added 'n_iter_no_change' option\\n\\n    - `warm_start`: bool, default=False\\n        When set to True, reuse the solution of the previous call to fit as\\n        initialization, otherwise, just erase the previous solution.\\n        See `the Glossary <warm_start>`.\\n\\n        Repeatedly calling fit or partial_fit when warm_start is True can\\n        result in a different solution than when calling fit a single time\\n        because of the way the data is shuffled.\\n        If a dynamic learning rate is used, the learning rate is adapted\\n        depending on the number of samples already seen. Calling ``fit`` resets\\n        this counter, while ``partial_fit``  will result in increasing the\\n        existing counter.\\n\\n    - `average`: bool or int, default=False\\n        When set to True, computes the averaged SGD weights across all\\n        updates and stores the result in the ``coef_`` attribute. If set to\\n        an int greater than 1, averaging will begin once the total number of\\n        samples seen reaches `average`. So ``average=10`` will begin\\n        averaging after seeing 10 samples.\\n\\n    Attributes\\n    ----------\\n    - `coef_`: ndarray of shape (n_features,)\\n        Weights assigned to the features.\\n\\n    - `intercept_`: ndarray of shape (1,)\\n        The intercept term.\\n\\n    - `n_iter_`: int\\n        The actual number of iterations before reaching the stopping criterion.\\n\\n    - `t_`: int\\n        Number of weight updates performed during training.\\n        Same as ``(n_iter_ * n_samples)``.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    See Also\\n    --------\\n    - `HuberRegressor`: Linear regression model that is robust to outliers.\\n    - `Lars`: Least Angle Regression model.\\n    - `Lasso`: Linear Model trained with L1 prior as regularizer.\\n    - `RANSACRegressor`: RANSAC (RANdom SAmple Consensus) algorithm.\\n    - `Ridge`: Linear least squares with l2 regularization.\\n    - `sklearn.svm.SVR`: Epsilon-Support Vector Regression.\\n    - `TheilSenRegressor`: Theil-Sen Estimator robust multivariate regression model.\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> from sklearn.linear_model import SGDRegressor\\n    >>> from sklearn.pipeline import make_pipeline\\n    >>> from sklearn.preprocessing import StandardScaler\\n    >>> n_samples, n_features = 10, 5\\n    >>> rng = np.random.RandomState(0)\\n    >>> y = rng.randn(n_samples)\\n    >>> X = rng.randn(n_samples, n_features)\\n    >>> # Always scale the input. The most convenient way is to use a pipeline.\\n    >>> reg = make_pipeline(StandardScaler(),\\n    ...                     SGDRegressor(max_iter=1000, tol=1e-3))\\n    >>> reg.fit(X, y)\\n    Pipeline(steps=[('standardscaler', StandardScaler()),\\n                    ('sgdregressor', SGDRegressor())])\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/svr\"} \":sklearn.regression/svr\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [11 2]:\\n\\n|       :name | :default |\\n|-------------|----------|\\n|     :kernel |      rbf |\\n|      :gamma |    scale |\\n|     :degree |        3 |\\n|        :tol | 0.001000 |\\n|  :shrinking |     true |\\n|          :c |    1.000 |\\n|   :max-iter |       -1 |\\n|     :coef-0 |    0.000 |\\n| :cache-size |      200 |\\n|    :verbose |    false |\\n|    :epsilon |   0.1000 |\\n\"]]] [:span [:p/markdown \"Epsilon-Support Vector Regression.\\n\\n    The free parameters in the model are C and epsilon.\\n\\n    The implementation is based on libsvm. The fit time complexity\\n    is more than quadratic with the number of samples which makes it hard\\n    to scale to datasets with more than a couple of 10000 samples. For large\\n    datasets consider using `~sklearn.svm.LinearSVR` or\\n    `~sklearn.linear_model.SGDRegressor` instead, possibly after a\\n    `~sklearn.kernel_approximation.Nystroem` transformer.\\n\\n    Read more in the User Guide: `svm_regression`.\\n\\n    Parameters\\n    ----------\\n    - `kernel`: {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'} or callable,          default='rbf'\\n         Specifies the kernel type to be used in the algorithm.\\n         If none is given, 'rbf' will be used. If a callable is given it is\\n         used to precompute the kernel matrix.\\n\\n    - `degree`: int, default=3\\n        Degree of the polynomial kernel function ('poly').\\n        Ignored by all other kernels.\\n\\n    - `gamma`: {'scale', 'auto'} or float, default='scale'\\n        Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\\n\\n        - if ``gamma='scale'`` (default) is passed then it uses\\n          1 / (n_features * X.var()) as value of gamma,\\n        - if 'auto', uses 1 / n_features.\\n\\n        *Changed in 0.22*\\n           The default value of ``gamma`` changed from 'auto' to 'scale'.\\n\\n    - `coef0`: float, default=0.0\\n        Independent term in kernel function.\\n        It is only significant in 'poly' and 'sigmoid'.\\n\\n    - `tol`: float, default=1e-3\\n        Tolerance for stopping criterion.\\n\\n    - `C`: float, default=1.0\\n        Regularization parameter. The strength of the regularization is\\n        inversely proportional to C. Must be strictly positive.\\n        The penalty is a squared l2 penalty.\\n\\n    - `epsilon`: float, default=0.1\\n         Epsilon in the epsilon-SVR model. It specifies the epsilon-tube\\n         within which no penalty is associated in the training loss function\\n         with points predicted within a distance epsilon from the actual\\n         value.\\n\\n    - `shrinking`: bool, default=True\\n        Whether to use the shrinking heuristic.\\n        See the User Guide: `shrinking_svm`.\\n\\n    - `cache_size`: float, default=200\\n        Specify the size of the kernel cache (in MB).\\n\\n    - `verbose`: bool, default=False\\n        Enable verbose output. Note that this setting takes advantage of a\\n        per-process runtime setting in libsvm that, if enabled, may not work\\n        properly in a multithreaded context.\\n\\n    - `max_iter`: int, default=-1\\n        Hard limit on iterations within solver, or -1 for no limit.\\n\\n    Attributes\\n    ----------\\n    - `class_weight_`: ndarray of shape (n_classes,)\\n        Multipliers of parameter C for each class.\\n        Computed based on the ``class_weight`` parameter.\\n\\n    - `coef_`: ndarray of shape (1, n_features)\\n        Weights assigned to the features (coefficients in the primal\\n        problem). This is only available in the case of a linear kernel.\\n\\n        `coef_` is readonly property derived from `dual_coef_` and\\n        `support_vectors_`.\\n\\n    - `dual_coef_`: ndarray of shape (1, n_SV)\\n        Coefficients of the support vector in the decision function.\\n\\n    - `fit_status_`: int\\n        0 if correctly fitted, 1 otherwise (will raise warning)\\n\\n    - `intercept_`: ndarray of shape (1,)\\n        Constants in decision function.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    - `n_iter_`: int\\n        Number of iterations run by the optimization routine to fit the model.\\n\\n        *Added in 1.1*\\n\\n    - `n_support_`: ndarray of shape (n_classes,), dtype=int32\\n        Number of support vectors for each class.\\n\\n    - `shape_fit_`: tuple of int of shape (n_dimensions_of_X,)\\n        Array dimensions of training vector ``X``.\\n\\n    - `support_`: ndarray of shape (n_SV,)\\n        Indices of support vectors.\\n\\n    - `support_vectors_`: ndarray of shape (n_SV, n_features)\\n        Support vectors.\\n\\n    See Also\\n    --------\\n    - `NuSVR`: Support Vector Machine for regression implemented using libsvm\\n        using a parameter to control the number of support vectors.\\n\\n    - `LinearSVR`: Scalable Linear Support Vector Machine for regression\\n        implemented using liblinear.\\n\\n    References\\n    ----------\\n - [1] [LIBSVM: A Library for Support Vector Machines\\n        ](http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf)\\n\\n - [2] [Platt, John (1999). \\\"Probabilistic outputs for support vector\\n        machines and comparison to regularizedlikelihood methods.\\\"\\n        ](http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639)\\n\\n    Examples\\n    --------\\n    >>> from sklearn.svm import SVR\\n    >>> from sklearn.pipeline import make_pipeline\\n    >>> from sklearn.preprocessing import StandardScaler\\n    >>> import numpy as np\\n    >>> n_samples, n_features = 10, 5\\n    >>> rng = np.random.RandomState(0)\\n    >>> y = rng.randn(n_samples)\\n    >>> X = rng.randn(n_samples, n_features)\\n    >>> regr = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))\\n    >>> regr.fit(X, y)\\n    Pipeline(steps=[('standardscaler', StandardScaler()),\\n                    ('svr', SVR(epsilon=0.2))])\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/theil-sen-regressor\"} \":sklearn.regression/theil-sen-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [9 2]:\\n\\n|              :name |  :default |\\n|--------------------|-----------|\\n| :max-subpopulation | 1.000E+04 |\\n|               :tol |  0.001000 |\\n|      :n-subsamples |           |\\n|          :max-iter |     300.0 |\\n|            :n-jobs |           |\\n|      :random-state |           |\\n|            :copy-x |      true |\\n|     :fit-intercept |      true |\\n|           :verbose |     false |\\n\"]]] [:span [:p/markdown \"Theil-Sen Estimator: robust multivariate regression model.\\n\\n    The algorithm calculates least square solutions on subsets with size\\n    n_subsamples of the samples in X. Any value of n_subsamples between the\\n    number of features and samples leads to an estimator with a compromise\\n    between robustness and efficiency. Since the number of least square\\n    solutions is \\\"n_samples choose n_subsamples\\\", it can be extremely large\\n    and can therefore be limited with max_subpopulation. If this limit is\\n    reached, the subsets are chosen randomly. In a final step, the spatial\\n    median (or L1 median) is calculated of all least square solutions.\\n\\n    Read more in the User Guide: `theil_sen_regression`.\\n\\n    Parameters\\n    ----------\\n    - `fit_intercept`: bool, default=True\\n        Whether to calculate the intercept for this model. If set\\n        to false, no intercept will be used in calculations.\\n\\n    - `copy_X`: bool, default=True\\n        If True, X will be copied; else, it may be overwritten.\\n\\n    - `max_subpopulation`: int, default=1e4\\n        Instead of computing with a set of cardinality 'n choose k', where n is\\n        the number of samples and k is the number of subsamples (at least\\n        number of features), consider only a stochastic subpopulation of a\\n        given maximal size if 'n choose k' is larger than max_subpopulation.\\n        For other than small problem sizes this parameter will determine\\n        memory usage and runtime if n_subsamples is not changed. Note that the\\n        data type should be int but floats such as 1e4 can be accepted too.\\n\\n    - `n_subsamples`: int, default=None\\n        Number of samples to calculate the parameters. This is at least the\\n        number of features (plus 1 if fit_intercept=True) and the number of\\n        samples as a maximum. A lower number leads to a higher breakdown\\n        point and a low efficiency while a high number leads to a low\\n        breakdown point and a high efficiency. If None, take the\\n        minimum number of subsamples leading to maximal robustness.\\n        If n_subsamples is set to n_samples, Theil-Sen is identical to least\\n        squares.\\n\\n    - `max_iter`: int, default=300\\n        Maximum number of iterations for the calculation of spatial median.\\n\\n    - `tol`: float, default=1e-3\\n        Tolerance when calculating spatial median.\\n\\n    - `random_state`: int, RandomState instance or None, default=None\\n        A random number generator instance to define the state of the random\\n        permutations generator. Pass an int for reproducible output across\\n        multiple function calls.\\n        See `Glossary <random_state>`.\\n\\n    - `n_jobs`: int, default=None\\n        Number of CPUs to use during the cross validation.\\n        ``None`` means 1 unless in a `joblib.parallel_backend` context.\\n        ``-1`` means using all processors. See `Glossary <n_jobs>`\\n        for more details.\\n\\n    - `verbose`: bool, default=False\\n        Verbose mode when fitting the model.\\n\\n    Attributes\\n    ----------\\n    - `coef_`: ndarray of shape (n_features,)\\n        Coefficients of the regression model (median of distribution).\\n\\n    - `intercept_`: float\\n        Estimated intercept of regression model.\\n\\n    - `breakdown_`: float\\n        Approximated breakdown point.\\n\\n    - `n_iter_`: int\\n        Number of iterations needed for the spatial median.\\n\\n    - `n_subpopulation_`: int\\n        Number of combinations taken into account from 'n choose k', where n is\\n        the number of samples and k is the number of subsamples.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    See Also\\n    --------\\n    - `HuberRegressor`: Linear regression model that is robust to outliers.\\n    - `RANSACRegressor`: RANSAC (RANdom SAmple Consensus) algorithm.\\n    - `SGDRegressor`: Fitted by minimizing a regularized empirical loss with SGD.\\n\\n    References\\n    ----------\\n    - Theil-Sen Estimators in a Multiple Linear Regression Model, 2009\\n      Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang\\n      http://home.olemiss.edu/~xdang/papers/MTSE.pdf\\n\\n    Examples\\n    --------\\n    >>> from sklearn.linear_model import TheilSenRegressor\\n    >>> from sklearn.datasets import make_regression\\n    >>> X, y = make_regression(\\n    ...     n_samples=200, n_features=2, noise=4.0, random_state=0)\\n    >>> reg = TheilSenRegressor(random_state=0).fit(X, y)\\n    >>> reg.score(X, y)\\n    0.9884...\\n    >>> reg.predict(X[:1,])\\n    array([-31.5871...])\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/transformed-target-regressor\"} \":sklearn.regression/transformed-target-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [5 2]:\\n\\n|          :name | :default |\\n|----------------|----------|\\n| :check-inverse |     true |\\n|          :func |          |\\n|  :inverse-func |          |\\n|     :regressor |          |\\n|   :transformer |          |\\n\"]]] [:span [:p/markdown \"Meta-estimator to regress on a transformed target.\\n\\n    Useful for applying a non-linear transformation to the target `y` in\\n    regression problems. This transformation can be given as a Transformer\\n    such as the `~sklearn.preprocessing.QuantileTransformer` or as a\\n    function and its inverse such as `np.log` and `np.exp`.\\n\\n    The computation during `fit` is\\n\\n```python\\nregressor.fit(X, func(y))\\n\\n\\n\\nregressor.fit(X, transformer.transform(y))\\n\\ncomputation during :meth:`predict` is::\\n\\ninverse_func(regressor.predict(X))\\n\\n\\n\\ntransformer.inverse_transform(regressor.predict(X))\\n\\n more in the :ref:`User Guide <transformed_target_regressor>`.\\n\\nersionadded:: 0.20\\n\\nmeters\\n------\\nessor : object, default=None\\nRegressor object such as derived from\\n:class:`~sklearn.base.RegressorMixin`. This regressor will\\nautomatically be cloned each time prior to fitting. If `regressor is\\nNone`, :class:`~sklearn.linear_model.LinearRegression` is created and used.\\n\\nsformer : object, default=None\\nEstimator object such as derived from\\n:class:`~sklearn.base.TransformerMixin`. Cannot be set at the same time\\nas `func` and `inverse_func`. If `transformer is None` as well as\\n`func` and `inverse_func`, the transformer will be an identity\\ntransformer. Note that the transformer will be cloned during fitting.\\nAlso, the transformer is restricting `y` to be a numpy array.\\n\\n : function, default=None\\nFunction to apply to `y` before passing to :meth:`fit`. Cannot be set\\nat the same time as `transformer`. The function needs to return a\\n2-dimensional array. If `func is None`, the function used will be the\\nidentity function.\\n\\nrse_func : function, default=None\\nFunction to apply to the prediction of the regressor. Cannot be set at\\nthe same time as `transformer`. The function needs to return a\\n2-dimensional array. The inverse function is used to return\\npredictions to the same space of the original training labels.\\n\\nk_inverse : bool, default=True\\nWhether to check that `transform` followed by `inverse_transform`\\nor `func` followed by `inverse_func` leads to the original targets.\\n\\nibutes\\n------\\nessor_ : object\\nFitted regressor.\\n\\nsformer_ : object\\nTransformer used in :meth:`fit` and :meth:`predict`.\\n\\natures_in_ : int\\nNumber of features seen during :term:`fit`. Only defined if the\\nunderlying regressor exposes such an attribute when fit.\\n\\n.. versionadded:: 0.24\\n\\nure_names_in_ : ndarray of shape (`n_features_in_`,)\\nNames of features seen during :term:`fit`. Defined only when `X`\\nhas feature names that are all strings.\\n\\n.. versionadded:: 1.0\\n\\nAlso\\n----\\narn.preprocessing.FunctionTransformer : Construct a transformer from an\\narbitrary callable.\\n\\ns\\n-\\nrnally, the target `y` is always converted into a 2-dimensional array\\ne used by scikit-learn transformers. At the time of prediction, the\\nut will be reshaped to a have the same number of dimensions as `y`.\\n\\n:ref:`examples/compose/plot_transformed_target.py\\nx_glr_auto_examples_compose_plot_transformed_target.py>`.\\n\\nples\\n----\\nimport numpy as np\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.compose import TransformedTargetRegressor\\ntt = TransformedTargetRegressor(regressor=LinearRegression(),\\n                                func=np.log, inverse_func=np.exp)\\nX = np.arange(4).reshape(-1, 1)\\ny = np.exp(2 * X).ravel()\\ntt.fit(X, y)\\nsformedTargetRegressor(...)\\ntt.score(X, y)\\n\\ntt.regressor_.coef_\\ny([2.])\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.regression/tweedie-regressor\"} \":sklearn.regression/tweedie-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [8 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|         :alpha |     1.000 |\\n| :fit-intercept |      true |\\n|          :link |      auto |\\n|      :max-iter |       100 |\\n|         :power |     0.000 |\\n|           :tol | 0.0001000 |\\n|       :verbose |         0 |\\n|    :warm-start |     false |\\n\"]]] [:span [:p/markdown \"Generalized Linear Model with a Tweedie distribution.\\n\\n    This estimator can be used to model different GLMs depending on the\\n    ``power`` parameter, which determines the underlying distribution.\\n\\n    Read more in the User Guide: `Generalized_linear_regression`.\\n\\n    *Added in 0.23*\\n\\n    Parameters\\n    ----------\\n    - `power`: float, default=0\\n            The power determines the underlying target distribution according\\n            to the following table:\\n\\n            | Power |      Distribution      |\\n            | ----- | ---------------------- |\\n            | 0     | Normal                 |\\n            | 1     | Poisson                |\\n            | (1,2) | Compound Poisson Gamma |\\n            | 2     | Gamma                  |\\n            | 3     | Inverse Gaussian       |\\n\\n            For ``0 < power < 1``, no distribution exists.\\n\\n    - `alpha`: float, default=1\\n        Constant that multiplies the penalty term and thus determines the\\n        regularization strength. ``alpha = 0`` is equivalent to unpenalized\\n        GLMs. In this case, the design matrix `X` must have full column rank\\n        (no collinearities).\\n        Values must be in the range `[0.0, inf)`.\\n\\n    - `fit_intercept`: bool, default=True\\n        Specifies if a constant (a.k.a. bias or intercept) should be\\n        added to the linear predictor (X @ coef + intercept).\\n\\n    - `link`: {'auto', 'identity', 'log'}, default='auto'\\n        The link function of the GLM, i.e. mapping from linear predictor\\n        `X @ coeff + intercept` to prediction `y_pred`. Option 'auto' sets\\n        the link depending on the chosen `power` parameter as follows:\\n\\n        - 'identity' for ``power <= 0``, e.g. for the Normal distribution\\n        - 'log' for ``power > 0``, e.g. for Poisson, Gamma and Inverse Gaussian\\n          distributions\\n\\n    - `max_iter`: int, default=100\\n        The maximal number of iterations for the solver.\\n        Values must be in the range `[1, inf)`.\\n\\n    - `tol`: float, default=1e-4\\n        Stopping criterion. For the lbfgs solver,\\n        the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol``\\n        where ``g_j`` is the j-th component of the gradient (derivative) of\\n        the objective function.\\n        Values must be in the range `(0.0, inf)`.\\n\\n    - `warm_start`: bool, default=False\\n        If set to ``True``, reuse the solution of the previous call to ``fit``\\n        as initialization for ``coef_`` and ``intercept_`` .\\n\\n    - `verbose`: int, default=0\\n        For the lbfgs solver set verbose to any positive number for verbosity.\\n        Values must be in the range `[0, inf)`.\\n\\n    Attributes\\n    ----------\\n    - `coef_`: array of shape (n_features,)\\n        Estimated coefficients for the linear predictor (`X @ coef_ +\\n        intercept_`) in the GLM.\\n\\n    - `intercept_`: float\\n        Intercept (a.k.a. bias) added to linear predictor.\\n\\n    - `n_iter_`: int\\n        Actual number of iterations used in the solver.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    See Also\\n    --------\\n    - `PoissonRegressor`: Generalized Linear Model with a Poisson distribution.\\n    - `GammaRegressor`: Generalized Linear Model with a Gamma distribution.\\n\\n    Examples\\n    --------\\n    >>> from sklearn import linear_model\\n    >>> clf = linear_model.TweedieRegressor()\\n    >>> X = [[1, 2], [2, 3], [3, 4], [4, 3]]\\n    >>> y = [2, 3.5, 5, 5.5]\\n    >>> clf.fit(X, y)\\n    TweedieRegressor()\\n    >>> clf.score(X, y)\\n    0.839...\\n    >>> clf.coef_\\n    array([0.599..., 0.299...])\\n    >>> clf.intercept_\\n    1.600...\\n    >>> clf.predict([[1, 1], [3, 4]])\\n    array([2.500..., 4.599...])\\n    \"]] [:hr] [:hr]])], \"1525\" [:div [:p] nil nil [:p/markdown \"## Sklearn classification\"]], \"1523\" [:div [:p] nil nil [:ul ([:li [:a {:href \"#:sklearn.classification/ada-boost-classifier\"} \":sklearn.classification/ada-boost-classifier\"]] [:li [:a {:href \"#:sklearn.classification/bagging-classifier\"} \":sklearn.classification/bagging-classifier\"]] [:li [:a {:href \"#:sklearn.classification/bernoulli-nb\"} \":sklearn.classification/bernoulli-nb\"]] [:li [:a {:href \"#:sklearn.classification/calibrated-classifier-cv\"} \":sklearn.classification/calibrated-classifier-cv\"]] [:li [:a {:href \"#:sklearn.classification/categorical-nb\"} \":sklearn.classification/categorical-nb\"]] [:li [:a {:href \"#:sklearn.classification/complement-nb\"} \":sklearn.classification/complement-nb\"]] [:li [:a {:href \"#:sklearn.classification/decision-tree-classifier\"} \":sklearn.classification/decision-tree-classifier\"]] [:li [:a {:href \"#:sklearn.classification/dummy-classifier\"} \":sklearn.classification/dummy-classifier\"]] [:li [:a {:href \"#:sklearn.classification/extra-tree-classifier\"} \":sklearn.classification/extra-tree-classifier\"]] [:li [:a {:href \"#:sklearn.classification/extra-trees-classifier\"} \":sklearn.classification/extra-trees-classifier\"]] [:li [:a {:href \"#:sklearn.classification/gaussian-nb\"} \":sklearn.classification/gaussian-nb\"]] [:li [:a {:href \"#:sklearn.classification/gaussian-process-classifier\"} \":sklearn.classification/gaussian-process-classifier\"]] [:li [:a {:href \"#:sklearn.classification/gradient-boosting-classifier\"} \":sklearn.classification/gradient-boosting-classifier\"]] [:li [:a {:href \"#:sklearn.classification/hist-gradient-boosting-classifier\"} \":sklearn.classification/hist-gradient-boosting-classifier\"]] [:li [:a {:href \"#:sklearn.classification/k-neighbors-classifier\"} \":sklearn.classification/k-neighbors-classifier\"]] [:li [:a {:href \"#:sklearn.classification/label-propagation\"} \":sklearn.classification/label-propagation\"]] [:li [:a {:href \"#:sklearn.classification/label-spreading\"} \":sklearn.classification/label-spreading\"]] [:li [:a {:href \"#:sklearn.classification/linear-discriminant-analysis\"} \":sklearn.classification/linear-discriminant-analysis\"]] [:li [:a {:href \"#:sklearn.classification/linear-svc\"} \":sklearn.classification/linear-svc\"]] [:li [:a {:href \"#:sklearn.classification/logistic-regression\"} \":sklearn.classification/logistic-regression\"]] [:li [:a {:href \"#:sklearn.classification/logistic-regression-cv\"} \":sklearn.classification/logistic-regression-cv\"]] [:li [:a {:href \"#:sklearn.classification/mlp-classifier\"} \":sklearn.classification/mlp-classifier\"]] [:li [:a {:href \"#:sklearn.classification/multinomial-nb\"} \":sklearn.classification/multinomial-nb\"]] [:li [:a {:href \"#:sklearn.classification/nearest-centroid\"} \":sklearn.classification/nearest-centroid\"]] [:li [:a {:href \"#:sklearn.classification/nu-svc\"} \":sklearn.classification/nu-svc\"]] [:li [:a {:href \"#:sklearn.classification/passive-aggressive-classifier\"} \":sklearn.classification/passive-aggressive-classifier\"]] [:li [:a {:href \"#:sklearn.classification/perceptron\"} \":sklearn.classification/perceptron\"]] [:li [:a {:href \"#:sklearn.classification/quadratic-discriminant-analysis\"} \":sklearn.classification/quadratic-discriminant-analysis\"]] [:li [:a {:href \"#:sklearn.classification/radius-neighbors-classifier\"} \":sklearn.classification/radius-neighbors-classifier\"]] [:li [:a {:href \"#:sklearn.classification/random-forest-classifier\"} \":sklearn.classification/random-forest-classifier\"]] [:li [:a {:href \"#:sklearn.classification/ridge-classifier\"} \":sklearn.classification/ridge-classifier\"]] [:li [:a {:href \"#:sklearn.classification/ridge-classifier-cv\"} \":sklearn.classification/ridge-classifier-cv\"]] [:li [:a {:href \"#:sklearn.classification/sgd-classifier\"} \":sklearn.classification/sgd-classifier\"]] [:li [:a {:href \"#:sklearn.classification/svc\"} \":sklearn.classification/svc\"]] [:li [:a {:href \"#:sklearn.regression/ada-boost-regressor\"} \":sklearn.regression/ada-boost-regressor\"]] [:li [:a {:href \"#:sklearn.regression/ard-regression\"} \":sklearn.regression/ard-regression\"]] [:li [:a {:href \"#:sklearn.regression/bagging-regressor\"} \":sklearn.regression/bagging-regressor\"]] [:li [:a {:href \"#:sklearn.regression/bayesian-ridge\"} \":sklearn.regression/bayesian-ridge\"]] [:li [:a {:href \"#:sklearn.regression/cca\"} \":sklearn.regression/cca\"]] [:li [:a {:href \"#:sklearn.regression/decision-tree-regressor\"} \":sklearn.regression/decision-tree-regressor\"]] [:li [:a {:href \"#:sklearn.regression/dummy-regressor\"} \":sklearn.regression/dummy-regressor\"]] [:li [:a {:href \"#:sklearn.regression/elastic-net\"} \":sklearn.regression/elastic-net\"]] [:li [:a {:href \"#:sklearn.regression/elastic-net-cv\"} \":sklearn.regression/elastic-net-cv\"]] [:li [:a {:href \"#:sklearn.regression/extra-tree-regressor\"} \":sklearn.regression/extra-tree-regressor\"]] [:li [:a {:href \"#:sklearn.regression/extra-trees-regressor\"} \":sklearn.regression/extra-trees-regressor\"]] [:li [:a {:href \"#:sklearn.regression/gamma-regressor\"} \":sklearn.regression/gamma-regressor\"]] [:li [:a {:href \"#:sklearn.regression/gaussian-process-regressor\"} \":sklearn.regression/gaussian-process-regressor\"]] [:li [:a {:href \"#:sklearn.regression/gradient-boosting-regressor\"} \":sklearn.regression/gradient-boosting-regressor\"]] [:li [:a {:href \"#:sklearn.regression/hist-gradient-boosting-regressor\"} \":sklearn.regression/hist-gradient-boosting-regressor\"]] [:li [:a {:href \"#:sklearn.regression/huber-regressor\"} \":sklearn.regression/huber-regressor\"]] [:li [:a {:href \"#:sklearn.regression/isotonic-regression\"} \":sklearn.regression/isotonic-regression\"]] [:li [:a {:href \"#:sklearn.regression/k-neighbors-regressor\"} \":sklearn.regression/k-neighbors-regressor\"]] [:li [:a {:href \"#:sklearn.regression/kernel-ridge\"} \":sklearn.regression/kernel-ridge\"]] [:li [:a {:href \"#:sklearn.regression/lars\"} \":sklearn.regression/lars\"]] [:li [:a {:href \"#:sklearn.regression/lars-cv\"} \":sklearn.regression/lars-cv\"]] [:li [:a {:href \"#:sklearn.regression/lasso\"} \":sklearn.regression/lasso\"]] [:li [:a {:href \"#:sklearn.regression/lasso-cv\"} \":sklearn.regression/lasso-cv\"]] [:li [:a {:href \"#:sklearn.regression/lasso-lars\"} \":sklearn.regression/lasso-lars\"]] [:li [:a {:href \"#:sklearn.regression/lasso-lars-cv\"} \":sklearn.regression/lasso-lars-cv\"]] [:li [:a {:href \"#:sklearn.regression/lasso-lars-ic\"} \":sklearn.regression/lasso-lars-ic\"]] [:li [:a {:href \"#:sklearn.regression/linear-regression\"} \":sklearn.regression/linear-regression\"]] [:li [:a {:href \"#:sklearn.regression/linear-svr\"} \":sklearn.regression/linear-svr\"]] [:li [:a {:href \"#:sklearn.regression/mlp-regressor\"} \":sklearn.regression/mlp-regressor\"]] [:li [:a {:href \"#:sklearn.regression/multi-task-elastic-net\"} \":sklearn.regression/multi-task-elastic-net\"]] [:li [:a {:href \"#:sklearn.regression/multi-task-elastic-net-cv\"} \":sklearn.regression/multi-task-elastic-net-cv\"]] [:li [:a {:href \"#:sklearn.regression/multi-task-lasso\"} \":sklearn.regression/multi-task-lasso\"]] [:li [:a {:href \"#:sklearn.regression/multi-task-lasso-cv\"} \":sklearn.regression/multi-task-lasso-cv\"]] [:li [:a {:href \"#:sklearn.regression/nu-svr\"} \":sklearn.regression/nu-svr\"]] [:li [:a {:href \"#:sklearn.regression/orthogonal-matching-pursuit\"} \":sklearn.regression/orthogonal-matching-pursuit\"]] [:li [:a {:href \"#:sklearn.regression/orthogonal-matching-pursuit-cv\"} \":sklearn.regression/orthogonal-matching-pursuit-cv\"]] [:li [:a {:href \"#:sklearn.regression/passive-aggressive-regressor\"} \":sklearn.regression/passive-aggressive-regressor\"]] [:li [:a {:href \"#:sklearn.regression/pls-canonical\"} \":sklearn.regression/pls-canonical\"]] [:li [:a {:href \"#:sklearn.regression/pls-regression\"} \":sklearn.regression/pls-regression\"]] [:li [:a {:href \"#:sklearn.regression/poisson-regressor\"} \":sklearn.regression/poisson-regressor\"]] [:li [:a {:href \"#:sklearn.regression/quantile-regressor\"} \":sklearn.regression/quantile-regressor\"]] [:li [:a {:href \"#:sklearn.regression/radius-neighbors-regressor\"} \":sklearn.regression/radius-neighbors-regressor\"]] [:li [:a {:href \"#:sklearn.regression/random-forest-regressor\"} \":sklearn.regression/random-forest-regressor\"]] [:li [:a {:href \"#:sklearn.regression/ransac-regressor\"} \":sklearn.regression/ransac-regressor\"]] [:li [:a {:href \"#:sklearn.regression/ridge\"} \":sklearn.regression/ridge\"]] [:li [:a {:href \"#:sklearn.regression/ridge-cv\"} \":sklearn.regression/ridge-cv\"]] [:li [:a {:href \"#:sklearn.regression/sgd-regressor\"} \":sklearn.regression/sgd-regressor\"]] [:li [:a {:href \"#:sklearn.regression/svr\"} \":sklearn.regression/svr\"]] [:li [:a {:href \"#:sklearn.regression/theil-sen-regressor\"} \":sklearn.regression/theil-sen-regressor\"]] [:li [:a {:href \"#:sklearn.regression/transformed-target-regressor\"} \":sklearn.regression/transformed-target-regressor\"]] [:li [:a {:href \"#:sklearn.regression/tweedie-regressor\"} \":sklearn.regression/tweedie-regressor\"]])]], \"1493\" [:div [:p] nil nil [:p/markdown \"Example: logistic regression\"]], \"1503\" [:div [:p] [:div [:p/code {:code \"(def fitted-ctx\\n  (pipe {:metamorph/data ds\\n         :metamorph/mode :fit}))\", :bg-class \"bg-light\"}]] nil nil], \"1509\" [:div [:p] nil nil [:p/markdown \"Access model details via python interop (libpython-clj)\"]], \"1517\" [:div [:p] nil nil [:dl ([:span [:dt :n_features_in_] [:dd \"2\"]] [:span [:dt :coef_] [:dd \"[[-4.80685757e-01 -4.80685757e-01]\\n [-2.51891384e-06 -2.51891384e-06]\\n [ 4.80688276e-01  4.80688276e-01]]\"]] [:span [:dt :intercept_] [:dd \"[ 0.87336885  0.17600816 -1.04937701]\"]] [:span [:dt :n_iter_] [:dd \"[12]\"]] [:span [:dt :classes_] [:dd \"[0. 1. 2.]\"]])]], \"1485\" [:div [:p] nil nil [:p/markdown \"# sklearn-clj\"]], \"1529\" [:div [:p] nil nil [:p/markdown \"## Sklearn regression\"]], \"1513\" [:div [:p] nil nil [:p/markdown \"All model attributes are as well in the context\"]], \"1527\" [:div [:p] nil nil ([:div [:h3 {:id \":sklearn.classification/ada-boost-classifier\"} \":sklearn.classification/ada-boost-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [5 2]:\\n\\n|           :name | :default |\\n|-----------------|----------|\\n|      :algorithm |  SAMME.R |\\n| :base-estimator |          |\\n|  :learning-rate |    1.000 |\\n|   :n-estimators |       50 |\\n|   :random-state |          |\\n\"]]] [:span [:p/markdown \"An AdaBoost classifier.\\n\\n    An AdaBoost [1] classifier is a meta-estimator that begins by fitting a\\n    classifier on the original dataset and then fits additional copies of the\\n    classifier on the same dataset but where the weights of incorrectly\\n    classified instances are adjusted such that subsequent classifiers focus\\n    more on difficult cases.\\n\\n    This class implements the algorithm known as AdaBoost-SAMME [2].\\n\\n    Read more in the User Guide: `adaboost`.\\n\\n    *Added in 0.14*\\n\\n    Parameters\\n    ----------\\n    - `base_estimator`: object, default=None\\n        The base estimator from which the boosted ensemble is built.\\n        Support for sample weighting is required, as well as proper\\n        ``classes_`` and ``n_classes_`` attributes. If ``None``, then\\n        the base estimator is `~sklearn.tree.DecisionTreeClassifier`\\n        initialized with `max_depth=1`.\\n\\n    - `n_estimators`: int, default=50\\n        The maximum number of estimators at which boosting is terminated.\\n        In case of perfect fit, the learning procedure is stopped early.\\n        Values must be in the range `[1, inf)`.\\n\\n    - `learning_rate`: float, default=1.0\\n        Weight applied to each classifier at each boosting iteration. A higher\\n        learning rate increases the contribution of each classifier. There is\\n        a trade-off between the `learning_rate` and `n_estimators` parameters.\\n        Values must be in the range `(0.0, inf)`.\\n\\n    - `algorithm`: {'SAMME', 'SAMME.R'}, default='SAMME.R'\\n        If 'SAMME.R' then use the SAMME.R real boosting algorithm.\\n        ``base_estimator`` must support calculation of class probabilities.\\n        If 'SAMME' then use the SAMME discrete boosting algorithm.\\n        The SAMME.R algorithm typically converges faster than SAMME,\\n        achieving a lower test error with fewer boosting iterations.\\n\\n    - `random_state`: int, RandomState instance or None, default=None\\n        Controls the random seed given at each `base_estimator` at each\\n        boosting iteration.\\n        Thus, it is only used when `base_estimator` exposes a `random_state`.\\n        Pass an int for reproducible output across multiple function calls.\\n        See `Glossary <random_state>`.\\n\\n    Attributes\\n    ----------\\n    - `base_estimator_`: estimator\\n        The base estimator from which the ensemble is grown.\\n\\n    - `estimators_`: list of classifiers\\n        The collection of fitted sub-estimators.\\n\\n    - `classes_`: ndarray of shape (n_classes,)\\n        The classes labels.\\n\\n    - `n_classes_`: int\\n        The number of classes.\\n\\n    - `estimator_weights_`: ndarray of floats\\n        Weights for each estimator in the boosted ensemble.\\n\\n    - `estimator_errors_`: ndarray of floats\\n        Classification error for each estimator in the boosted\\n        ensemble.\\n\\n    - `feature_importances_`: ndarray of shape (n_features,)\\n        The impurity-based feature importances if supported by the\\n        ``base_estimator`` (when based on decision trees).\\n\\n        Warning: impurity-based feature importances can be misleading for\\n        high cardinality features (many unique values). See\\n        `sklearn.inspection.permutation_importance` as an alternative.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    See Also\\n    --------\\n    - `AdaBoostRegressor`: An AdaBoost regressor that begins by fitting a\\n        regressor on the original dataset and then fits additional copies of\\n        the regressor on the same dataset but where the weights of instances\\n        are adjusted according to the error of the current prediction.\\n\\n    - `GradientBoostingClassifier`: GB builds an additive model in a forward\\n        stage-wise fashion. Regression trees are fit on the negative gradient\\n        of the binomial or multinomial deviance loss function. Binary\\n        classification is a special case where only a single regression tree is\\n        induced.\\n\\n    - `sklearn.tree.DecisionTreeClassifier`: A non-parametric supervised learning\\n        method used for classification.\\n        Creates a model that predicts the value of a target variable by\\n        learning simple decision rules inferred from the data features.\\n\\n    References\\n    ----------\\n - [1] Y. Freund, R. Schapire, \\\"A Decision-Theoretic Generalization of\\n           on-Line Learning and an Application to Boosting\\\", 1995.\\n\\n - [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, \\\"Multi-class AdaBoost\\\", 2009.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.ensemble import AdaBoostClassifier\\n    >>> from sklearn.datasets import make_classification\\n    >>> X, y = make_classification(n_samples=1000, n_features=4,\\n    ...                            n_informative=2, n_redundant=0,\\n    ...                            random_state=0, shuffle=False)\\n    >>> clf = AdaBoostClassifier(n_estimators=100, random_state=0)\\n    >>> clf.fit(X, y)\\n    AdaBoostClassifier(n_estimators=100, random_state=0)\\n    >>> clf.predict([[0, 0, 0, 0]])\\n    array([1])\\n    >>> clf.score(X, y)\\n    0.983...\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.classification/bagging-classifier\"} \":sklearn.classification/bagging-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [11 2]:\\n\\n|               :name | :default |\\n|---------------------|----------|\\n|          :bootstrap |     true |\\n| :bootstrap-features |    false |\\n|             :n-jobs |          |\\n|       :random-state |          |\\n|          :oob-score |    false |\\n|     :base-estimator |          |\\n|       :max-features |    1.000 |\\n|         :warm-start |    false |\\n|       :n-estimators |       10 |\\n|        :max-samples |    1.000 |\\n|            :verbose |        0 |\\n\"]]] [:span [:p/markdown \"A Bagging classifier.\\n\\n    A Bagging classifier is an ensemble meta-estimator that fits base\\n    classifiers each on random subsets of the original dataset and then\\n    aggregate their individual predictions (either by voting or by averaging)\\n    to form a final prediction. Such a meta-estimator can typically be used as\\n    a way to reduce the variance of a black-box estimator (e.g., a decision\\n    tree), by introducing randomization into its construction procedure and\\n    then making an ensemble out of it.\\n\\n    This algorithm encompasses several works from the literature. When random\\n    subsets of the dataset are drawn as random subsets of the samples, then\\n    this algorithm is known as Pasting [1]_. If samples are drawn with\\n    replacement, then the method is known as Bagging [2]_. When random subsets\\n    of the dataset are drawn as random subsets of the features, then the method\\n    is known as Random Subspaces [3]_. Finally, when base estimators are built\\n    on subsets of both samples and features, then the method is known as\\n    Random Patches [4]_.\\n\\n    Read more in the User Guide: `bagging`.\\n\\n    *Added in 0.15*\\n\\n    Parameters\\n    ----------\\n    - `base_estimator`: object, default=None\\n        The base estimator to fit on random subsets of the dataset.\\n        If None, then the base estimator is a\\n        `~sklearn.tree.DecisionTreeClassifier`.\\n\\n    - `n_estimators`: int, default=10\\n        The number of base estimators in the ensemble.\\n\\n    - `max_samples`: int or float, default=1.0\\n        The number of samples to draw from X to train each base estimator (with\\n        replacement by default, see `bootstrap` for more details).\\n\\n        - If int, then draw `max_samples` samples.\\n        - If float, then draw `max_samples * X.shape[0]` samples.\\n\\n    - `max_features`: int or float, default=1.0\\n        The number of features to draw from X to train each base estimator (\\n        without replacement by default, see `bootstrap_features` for more\\n        details).\\n\\n        - If int, then draw `max_features` features.\\n        - If float, then draw `max_features * X.shape[1]` features.\\n\\n    - `bootstrap`: bool, default=True\\n        Whether samples are drawn with replacement. If False, sampling\\n        without replacement is performed.\\n\\n    - `bootstrap_features`: bool, default=False\\n        Whether features are drawn with replacement.\\n\\n    - `oob_score`: bool, default=False\\n        Whether to use out-of-bag samples to estimate\\n        the generalization error. Only available if bootstrap=True.\\n\\n    - `warm_start`: bool, default=False\\n        When set to True, reuse the solution of the previous call to fit\\n        and add more estimators to the ensemble, otherwise, just fit\\n        a whole new ensemble. See `the Glossary <warm_start>`.\\n\\n        *Added in 0.17*\\n           *warm_start* constructor parameter.\\n\\n    - `n_jobs`: int, default=None\\n        The number of jobs to run in parallel for both `fit` and\\n        `predict`. ``None`` means 1 unless in a\\n        `joblib.parallel_backend` context. ``-1`` means using all\\n        processors. See `Glossary <n_jobs>` for more details.\\n\\n    - `random_state`: int, RandomState instance or None, default=None\\n        Controls the random resampling of the original dataset\\n        (sample wise and feature wise).\\n        If the base estimator accepts a `random_state` attribute, a different\\n        seed is generated for each instance in the ensemble.\\n        Pass an int for reproducible output across multiple function calls.\\n        See `Glossary <random_state>`.\\n\\n    - `verbose`: int, default=0\\n        Controls the verbosity when fitting and predicting.\\n\\n    Attributes\\n    ----------\\n    - `base_estimator_`: estimator\\n        The base estimator from which the ensemble is grown.\\n\\n    - `n_features_`: int\\n        The number of features when `fit` is performed.\\n\\n        *Deprecated since 1.0*\\n            Attribute `n_features_` was deprecated in version 1.0 and will be\\n            removed in 1.2. Use `n_features_in_` instead.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    - `estimators_`: list of estimators\\n        The collection of fitted base estimators.\\n\\n    - `estimators_samples_`: list of arrays\\n        The subset of drawn samples (i.e., the in-bag samples) for each base\\n        estimator. Each subset is defined by an array of the indices selected.\\n\\n    - `estimators_features_`: list of arrays\\n        The subset of drawn features for each base estimator.\\n\\n    - `classes_`: ndarray of shape (n_classes,)\\n        The classes labels.\\n\\n    - `n_classes_`: int or list\\n        The number of classes.\\n\\n    - `oob_score_`: float\\n        Score of the training dataset obtained using an out-of-bag estimate.\\n        This attribute exists only when ``oob_score`` is True.\\n\\n    - `oob_decision_function_`: ndarray of shape (n_samples, n_classes)\\n        Decision function computed with out-of-bag estimate on the training\\n        set. If n_estimators is small it might be possible that a data point\\n        was never left out during the bootstrap. In this case,\\n        `oob_decision_function_` might contain NaN. This attribute exists\\n        only when ``oob_score`` is True.\\n\\n    See Also\\n    --------\\n    - `BaggingRegressor`: A Bagging regressor.\\n\\n    References\\n    ----------\\n\\n - [1] L. Breiman, \\\"Pasting small votes for classification in large\\n           databases and on-line\\\", Machine Learning, 36(1), 85-103, 1999.\\n\\n - [2] L. Breiman, \\\"Bagging predictors\\\", Machine Learning, 24(2), 123-140,\\n           1996.\\n\\n - [3] T. Ho, \\\"The random subspace method for constructing decision\\n           forests\\\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\\n           1998.\\n\\n - [4] G. Louppe and P. Geurts, \\\"Ensembles on Random Patches\\\", Machine\\n           Learning and Knowledge Discovery in Databases, 346-361, 2012.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.svm import SVC\\n    >>> from sklearn.ensemble import BaggingClassifier\\n    >>> from sklearn.datasets import make_classification\\n    >>> X, y = make_classification(n_samples=100, n_features=4,\\n    ...                            n_informative=2, n_redundant=0,\\n    ...                            random_state=0, shuffle=False)\\n    >>> clf = BaggingClassifier(base_estimator=SVC(),\\n    ...                         n_estimators=10, random_state=0).fit(X, y)\\n    >>> clf.predict([[0, 0, 0, 0]])\\n    array([1])\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.classification/bernoulli-nb\"} \":sklearn.classification/bernoulli-nb\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [4 2]:\\n\\n|        :name | :default |\\n|--------------|----------|\\n|       :alpha |    1.000 |\\n|    :binarize |    0.000 |\\n| :class-prior |          |\\n|   :fit-prior |     true |\\n\"]]] [:span [:p/markdown \"Naive Bayes classifier for multivariate Bernoulli models.\\n\\n    Like MultinomialNB, this classifier is suitable for discrete data. The\\n    difference is that while MultinomialNB works with occurrence counts,\\n    BernoulliNB is designed for binary/boolean features.\\n\\n    Read more in the User Guide: `bernoulli_naive_bayes`.\\n\\n    Parameters\\n    ----------\\n    - `alpha`: float, default=1.0\\n        Additive (Laplace/Lidstone) smoothing parameter\\n        (0 for no smoothing).\\n\\n    - `binarize`: float or None, default=0.0\\n        Threshold for binarizing (mapping to booleans) of sample features.\\n        If None, input is presumed to already consist of binary vectors.\\n\\n    - `fit_prior`: bool, default=True\\n        Whether to learn class prior probabilities or not.\\n        If false, a uniform prior will be used.\\n\\n    - `class_prior`: array-like of shape (n_classes,), default=None\\n        Prior probabilities of the classes. If specified, the priors are not\\n        adjusted according to the data.\\n\\n    Attributes\\n    ----------\\n    - `class_count_`: ndarray of shape (n_classes,)\\n        Number of samples encountered for each class during fitting. This\\n        value is weighted by the sample weight when provided.\\n\\n    - `class_log_prior_`: ndarray of shape (n_classes,)\\n        Log probability of each class (smoothed).\\n\\n    - `classes_`: ndarray of shape (n_classes,)\\n        Class labels known to the classifier\\n\\n    - `feature_count_`: ndarray of shape (n_classes, n_features)\\n        Number of samples encountered for each (class, feature)\\n        during fitting. This value is weighted by the sample weight when\\n        provided.\\n\\n    - `feature_log_prob_`: ndarray of shape (n_classes, n_features)\\n        Empirical log probability of features given a class, P(x_i|y).\\n\\n    - `n_features_`: int\\n        Number of features of each sample.\\n\\n        *Deprecated since 1.0*\\n            Attribute `n_features_` was deprecated in version 1.0 and will be\\n            removed in 1.2. Use `n_features_in_` instead.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    See Also\\n    --------\\n    - `CategoricalNB`: Naive Bayes classifier for categorical features.\\n    - `ComplementNB`: The Complement Naive Bayes classifier\\n        described in Rennie et al. (2003).\\n    - `GaussianNB`: Gaussian Naive Bayes (GaussianNB).\\n    - `MultinomialNB`: Naive Bayes classifier for multinomial models.\\n\\n    References\\n    ----------\\n    C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\\n    Information Retrieval. Cambridge University Press, pp. 234-265.\\n    https://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html\\n\\n    A. McCallum and K. Nigam (1998). A comparison of event models for naive\\n    Bayes text classification. Proc. AAAI/ICML-98 Workshop on Learning for\\n    Text Categorization, pp. 41-48.\\n\\n    V. Metsis, I. Androutsopoulos and G. Paliouras (2006). Spam filtering with\\n    naive Bayes -- Which naive Bayes? 3rd Conf. on Email and Anti-Spam (CEAS).\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> rng = np.random.RandomState(1)\\n    >>> X = rng.randint(5, size=(6, 100))\\n    >>> Y = np.array([1, 2, 3, 4, 4, 5])\\n    >>> from sklearn.naive_bayes import BernoulliNB\\n    >>> clf = BernoulliNB()\\n    >>> clf.fit(X, Y)\\n    BernoulliNB()\\n    >>> print(clf.predict(X[2:3]))\\n    [3]\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.classification/calibrated-classifier-cv\"} \":sklearn.classification/calibrated-classifier-cv\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [5 2]:\\n\\n|           :name | :default |\\n|-----------------|----------|\\n| :base-estimator |          |\\n|             :cv |          |\\n|       :ensemble |     true |\\n|         :method |  sigmoid |\\n|         :n-jobs |          |\\n\"]]] [:span [:p/markdown \"Probability calibration with isotonic regression or logistic regression.\\n\\n    This class uses cross-validation to both estimate the parameters of a\\n    classifier and subsequently calibrate a classifier. With default\\n    `ensemble=True`, for each cv split it\\n    fits a copy of the base estimator to the training subset, and calibrates it\\n    using the testing subset. For prediction, predicted probabilities are\\n    averaged across these individual calibrated classifiers. When\\n    `ensemble=False`, cross-validation is used to obtain unbiased predictions,\\n    via `~sklearn.model_selection.cross_val_predict`, which are then\\n    used for calibration. For prediction, the base estimator, trained using all\\n    the data, is used. This is the method implemented when `probabilities=True`\\n    for `sklearn.svm` estimators.\\n\\n    Already fitted classifiers can be calibrated via the parameter\\n    `cv=\\\"prefit\\\"`. In this case, no cross-validation is used and all provided\\n    data is used for calibration. The user has to take care manually that data\\n    for model fitting and calibration are disjoint.\\n\\n    The calibration is based on the `decision_function` method of the\\n    `base_estimator` if it exists, else on `predict_proba`.\\n\\n    Read more in the User Guide: `calibration`.\\n\\n    Parameters\\n    ----------\\n    - `base_estimator`: estimator instance, default=None\\n        The classifier whose output need to be calibrated to provide more\\n        accurate `predict_proba` outputs. The default classifier is\\n        a `~sklearn.svm.LinearSVC`.\\n\\n    - `method`: {'sigmoid', 'isotonic'}, default='sigmoid'\\n        The method to use for calibration. Can be 'sigmoid' which\\n        corresponds to Platt's method (i.e. a logistic regression model) or\\n        'isotonic' which is a non-parametric approach. It is not advised to\\n        use isotonic calibration with too few calibration samples\\n        ``(<<1000)`` since it tends to overfit.\\n\\n    - `cv`: int, cross-validation generator, iterable or \\\"prefit\\\",             default=None\\n        Determines the cross-validation splitting strategy.\\n        Possible inputs for cv are:\\n\\n        - None, to use the default 5-fold cross-validation,\\n        - integer, to specify the number of folds.\\n        - `CV splitter`,\\n        - An iterable yielding (train, test) splits as arrays of indices.\\n\\n        For integer/None inputs, if ``y`` is binary or multiclass,\\n        `~sklearn.model_selection.StratifiedKFold` is used. If ``y`` is\\n        neither binary nor multiclass, `~sklearn.model_selection.KFold`\\n        is used.\\n\\n        Refer to the User Guide: `cross_validation` for the various\\n        cross-validation strategies that can be used here.\\n\\n        If \\\"prefit\\\" is passed, it is assumed that `base_estimator` has been\\n        fitted already and all data is used for calibration.\\n\\n        *Changed in 0.22*\\n            ``cv`` default value if None changed from 3-fold to 5-fold.\\n\\n    - `n_jobs`: int, default=None\\n        Number of jobs to run in parallel.\\n        ``None`` means 1 unless in a `joblib.parallel_backend` context.\\n        ``-1`` means using all processors.\\n\\n        Base estimator clones are fitted in parallel across cross-validation\\n        iterations. Therefore parallelism happens only when `cv != \\\"prefit\\\"`.\\n\\n        See `Glossary <n_jobs>` for more details.\\n\\n        *Added in 0.24*\\n\\n    - `ensemble`: bool, default=True\\n        Determines how the calibrator is fitted when `cv` is not `'prefit'`.\\n        Ignored if `cv='prefit'`.\\n\\n        If `True`, the `base_estimator` is fitted using training data, and\\n        calibrated using testing data, for each `cv` fold. The final estimator\\n        is an ensemble of `n_cv` fitted classifier and calibrator pairs, where\\n        `n_cv` is the number of cross-validation folds. The output is the\\n        average predicted probabilities of all pairs.\\n\\n        If `False`, `cv` is used to compute unbiased predictions, via\\n        `~sklearn.model_selection.cross_val_predict`, which are then\\n        used for calibration. At prediction time, the classifier used is the\\n        `base_estimator` trained on all the data.\\n        Note that this method is also internally implemented  in\\n        `sklearn.svm` estimators with the `probabilities=True` parameter.\\n\\n        *Added in 0.24*\\n\\n    Attributes\\n    ----------\\n    - `classes_`: ndarray of shape (n_classes,)\\n        The class labels.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`. Only defined if the\\n        underlying base_estimator exposes such an attribute when fit.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Only defined if the\\n        underlying base_estimator exposes such an attribute when fit.\\n\\n        *Added in 1.0*\\n\\n    - `calibrated_classifiers_`: list (len() equal to cv or 1 if `cv=\\\"prefit\\\"`             or `ensemble=False`)\\n        The list of classifier and calibrator pairs.\\n\\n        - When `cv=\\\"prefit\\\"`, the fitted `base_estimator` and fitted\\n          calibrator.\\n        - When `cv` is not \\\"prefit\\\" and `ensemble=True`, `n_cv` fitted\\n          `base_estimator` and calibrator pairs. `n_cv` is the number of\\n          cross-validation folds.\\n        - When `cv` is not \\\"prefit\\\" and `ensemble=False`, the `base_estimator`,\\n          fitted on all the data, and fitted calibrator.\\n\\n        *Changed in 0.24*\\n            Single calibrated classifier case when `ensemble=False`.\\n\\n    See Also\\n    --------\\n    - `calibration_curve`: Compute true and predicted probabilities\\n        for a calibration curve.\\n\\n    References\\n    ----------\\n - [1] Obtaining calibrated probability estimates from decision trees\\n           and naive Bayesian classifiers, B. Zadrozny & C. Elkan, ICML 2001\\n\\n - [2] Transforming Classifier Scores into Accurate Multiclass\\n           Probability Estimates, B. Zadrozny & C. Elkan, (KDD 2002)\\n\\n - [3] Probabilistic Outputs for Support Vector Machines and Comparisons to\\n           Regularized Likelihood Methods, J. Platt, (1999)\\n\\n - [4] Predicting Good Probabilities with Supervised Learning,\\n           A. Niculescu-Mizil & R. Caruana, ICML 2005\\n\\n    Examples\\n    --------\\n    >>> from sklearn.datasets import make_classification\\n    >>> from sklearn.naive_bayes import GaussianNB\\n    >>> from sklearn.calibration import CalibratedClassifierCV\\n    >>> X, y = make_classification(n_samples=100, n_features=2,\\n    ...                            n_redundant=0, random_state=42)\\n    >>> base_clf = GaussianNB()\\n    >>> calibrated_clf = CalibratedClassifierCV(base_estimator=base_clf, cv=3)\\n    >>> calibrated_clf.fit(X, y)\\n    CalibratedClassifierCV(base_estimator=GaussianNB(), cv=3)\\n    >>> len(calibrated_clf.calibrated_classifiers_)\\n    3\\n    >>> calibrated_clf.predict_proba(X)[:5, :]\\n    array([[0.110..., 0.889...],\\n           [0.072..., 0.927...],\\n           [0.928..., 0.071...],\\n           [0.928..., 0.071...],\\n           [0.071..., 0.928...]])\\n    >>> from sklearn.model_selection import train_test_split\\n    >>> X, y = make_classification(n_samples=100, n_features=2,\\n    ...                            n_redundant=0, random_state=42)\\n    >>> X_train, X_calib, y_train, y_calib = train_test_split(\\n    ...        X, y, random_state=42\\n    ... )\\n    >>> base_clf = GaussianNB()\\n    >>> base_clf.fit(X_train, y_train)\\n    GaussianNB()\\n    >>> calibrated_clf = CalibratedClassifierCV(\\n    ...     base_estimator=base_clf,\\n    ...     cv=\\\"prefit\\\"\\n    ... )\\n    >>> calibrated_clf.fit(X_calib, y_calib)\\n    CalibratedClassifierCV(base_estimator=GaussianNB(), cv='prefit')\\n    >>> len(calibrated_clf.calibrated_classifiers_)\\n    1\\n    >>> calibrated_clf.predict_proba([[-0.5, 0.5]])\\n    array([[0.936..., 0.063...]])\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.classification/categorical-nb\"} \":sklearn.classification/categorical-nb\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [4 2]:\\n\\n|           :name | :default |\\n|-----------------|----------|\\n|          :alpha |    1.000 |\\n|    :class-prior |          |\\n|      :fit-prior |     true |\\n| :min-categories |          |\\n\"]]] [:span [:p/markdown \"Naive Bayes classifier for categorical features.\\n\\n    The categorical Naive Bayes classifier is suitable for classification with\\n    discrete features that are categorically distributed. The categories of\\n    each feature are drawn from a categorical distribution.\\n\\n    Read more in the User Guide: `categorical_naive_bayes`.\\n\\n    Parameters\\n    ----------\\n    - `alpha`: float, default=1.0\\n        Additive (Laplace/Lidstone) smoothing parameter\\n        (0 for no smoothing).\\n\\n    - `fit_prior`: bool, default=True\\n        Whether to learn class prior probabilities or not.\\n        If false, a uniform prior will be used.\\n\\n    - `class_prior`: array-like of shape (n_classes,), default=None\\n        Prior probabilities of the classes. If specified, the priors are not\\n        adjusted according to the data.\\n\\n    - `min_categories`: int or array-like of shape (n_features,), default=None\\n        Minimum number of categories per feature.\\n\\n        - integer: Sets the minimum number of categories per feature to\\n          `n_categories` for each features.\\n        - array-like: shape (n_features,) where `n_categories[i]` holds the\\n          minimum number of categories for the ith column of the input.\\n        - None (default): Determines the number of categories automatically\\n          from the training data.\\n\\n        *Added in 0.24*\\n\\n    Attributes\\n    ----------\\n    - `category_count_`: list of arrays of shape (n_features,)\\n        Holds arrays of shape (n_classes, n_categories of respective feature)\\n        for each feature. Each array provides the number of samples\\n        encountered for each class and category of the specific feature.\\n\\n    - `class_count_`: ndarray of shape (n_classes,)\\n        Number of samples encountered for each class during fitting. This\\n        value is weighted by the sample weight when provided.\\n\\n    - `class_log_prior_`: ndarray of shape (n_classes,)\\n        Smoothed empirical log probability for each class.\\n\\n    - `classes_`: ndarray of shape (n_classes,)\\n        Class labels known to the classifier\\n\\n    - `feature_log_prob_`: list of arrays of shape (n_features,)\\n        Holds arrays of shape (n_classes, n_categories of respective feature)\\n        for each feature. Each array provides the empirical log probability\\n        of categories given the respective feature and class, ``P(x_i|y)``.\\n\\n    - `n_features_`: int\\n        Number of features of each sample.\\n\\n        *Deprecated since 1.0*\\n            Attribute `n_features_` was deprecated in version 1.0 and will be\\n            removed in 1.2. Use `n_features_in_` instead.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    - `n_categories_`: ndarray of shape (n_features,), dtype=np.int64\\n        Number of categories for each feature. This value is\\n        inferred from the data or set by the minimum number of categories.\\n\\n        *Added in 0.24*\\n\\n    See Also\\n    --------\\n    - `BernoulliNB`: Naive Bayes classifier for multivariate Bernoulli models.\\n    - `ComplementNB`: Complement Naive Bayes classifier.\\n    - `GaussianNB`: Gaussian Naive Bayes.\\n    - `MultinomialNB`: Naive Bayes classifier for multinomial models.\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> rng = np.random.RandomState(1)\\n    >>> X = rng.randint(5, size=(6, 100))\\n    >>> y = np.array([1, 2, 3, 4, 5, 6])\\n    >>> from sklearn.naive_bayes import CategoricalNB\\n    >>> clf = CategoricalNB()\\n    >>> clf.fit(X, y)\\n    CategoricalNB()\\n    >>> print(clf.predict(X[2:3]))\\n    [3]\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.classification/complement-nb\"} \":sklearn.classification/complement-nb\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [4 2]:\\n\\n|        :name | :default |\\n|--------------|----------|\\n|       :alpha |    1.000 |\\n| :class-prior |          |\\n|   :fit-prior |     true |\\n|        :norm |    false |\\n\"]]] [:span [:p/markdown \"The Complement Naive Bayes classifier described in Rennie et al. (2003).\\n\\n    The Complement Naive Bayes classifier was designed to correct the \\\"severe\\n    assumptions\\\" made by the standard Multinomial Naive Bayes classifier. It is\\n    particularly suited for imbalanced data sets.\\n\\n    Read more in the User Guide: `complement_naive_bayes`.\\n\\n    *Added in 0.20*\\n\\n    Parameters\\n    ----------\\n    - `alpha`: float, default=1.0\\n        Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).\\n\\n    - `fit_prior`: bool, default=True\\n        Only used in edge case with a single class in the training set.\\n\\n    - `class_prior`: array-like of shape (n_classes,), default=None\\n        Prior probabilities of the classes. Not used.\\n\\n    - `norm`: bool, default=False\\n        Whether or not a second normalization of the weights is performed. The\\n        default behavior mirrors the implementations found in Mahout and Weka,\\n        which do not follow the full algorithm described in Table 9 of the\\n        paper.\\n\\n    Attributes\\n    ----------\\n    - `class_count_`: ndarray of shape (n_classes,)\\n        Number of samples encountered for each class during fitting. This\\n        value is weighted by the sample weight when provided.\\n\\n    - `class_log_prior_`: ndarray of shape (n_classes,)\\n        Smoothed empirical log probability for each class. Only used in edge\\n        case with a single class in the training set.\\n\\n    - `classes_`: ndarray of shape (n_classes,)\\n        Class labels known to the classifier\\n\\n    - `feature_all_`: ndarray of shape (n_features,)\\n        Number of samples encountered for each feature during fitting. This\\n        value is weighted by the sample weight when provided.\\n\\n    - `feature_count_`: ndarray of shape (n_classes, n_features)\\n        Number of samples encountered for each (class, feature) during fitting.\\n        This value is weighted by the sample weight when provided.\\n\\n    - `feature_log_prob_`: ndarray of shape (n_classes, n_features)\\n        Empirical weights for class complements.\\n\\n    - `n_features_`: int\\n        Number of features of each sample.\\n\\n        *Deprecated since 1.0*\\n            Attribute `n_features_` was deprecated in version 1.0 and will be\\n            removed in 1.2. Use `n_features_in_` instead.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    See Also\\n    --------\\n    - `BernoulliNB`: Naive Bayes classifier for multivariate Bernoulli models.\\n    - `CategoricalNB`: Naive Bayes classifier for categorical features.\\n    - `GaussianNB`: Gaussian Naive Bayes.\\n    - `MultinomialNB`: Naive Bayes classifier for multinomial models.\\n\\n    References\\n    ----------\\n    Rennie, J. D., Shih, L., Teevan, J., & Karger, D. R. (2003).\\n    Tackling the poor assumptions of naive bayes text classifiers. In ICML\\n    (Vol. 3, pp. 616-623).\\n    https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> rng = np.random.RandomState(1)\\n    >>> X = rng.randint(5, size=(6, 100))\\n    >>> y = np.array([1, 2, 3, 4, 5, 6])\\n    >>> from sklearn.naive_bayes import ComplementNB\\n    >>> clf = ComplementNB()\\n    >>> clf.fit(X, y)\\n    ComplementNB()\\n    >>> print(clf.predict(X[2:3]))\\n    [3]\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.classification/decision-tree-classifier\"} \":sklearn.classification/decision-tree-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [12 2]:\\n\\n|                     :name | :default |\\n|---------------------------|----------|\\n| :min-weight-fraction-leaf |    0.000 |\\n|           :max-leaf-nodes |          |\\n|    :min-impurity-decrease |    0.000 |\\n|        :min-samples-split |    2.000 |\\n|                :ccp-alpha |    0.000 |\\n|                 :splitter |     best |\\n|             :random-state |          |\\n|         :min-samples-leaf |        1 |\\n|             :max-features |          |\\n|                :max-depth |          |\\n|             :class-weight |          |\\n|                :criterion |     gini |\\n\"]]] [:span [:p/markdown \"A decision tree classifier.\\n\\n    Read more in the User Guide: `tree`.\\n\\n    Parameters\\n    ----------\\n    - `criterion`: {\\\"gini\\\", \\\"entropy\\\", \\\"log_loss\\\"}, default=\\\"gini\\\"\\n        The function to measure the quality of a split. Supported criteria are\\n        \\\"gini\\\" for the Gini impurity and \\\"log_loss\\\" and \\\"entropy\\\" both for the\\n        Shannon information gain, see :ref:`tree_mathematical_formulation`.\\n\\n    - `splitter`: {\\\"best\\\", \\\"random\\\"}, default=\\\"best\\\"\\n        The strategy used to choose the split at each node. Supported\\n        strategies are \\\"best\\\" to choose the best split and \\\"random\\\" to choose\\n        the best random split.\\n\\n    - `max_depth`: int, default=None\\n        The maximum depth of the tree. If None, then nodes are expanded until\\n        all leaves are pure or until all leaves contain less than\\n        min_samples_split samples.\\n\\n    - `min_samples_split`: int or float, default=2\\n        The minimum number of samples required to split an internal node:\\n\\n        - If int, then consider `min_samples_split` as the minimum number.\\n        - If float, then `min_samples_split` is a fraction and\\n          `ceil(min_samples_split * n_samples)` are the minimum\\n          number of samples for each split.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n    - `min_samples_leaf`: int or float, default=1\\n        The minimum number of samples required to be at a leaf node.\\n        A split point at any depth will only be considered if it leaves at\\n        least ``min_samples_leaf`` training samples in each of the left and\\n        right branches.  This may have the effect of smoothing the model,\\n        especially in regression.\\n\\n        - If int, then consider `min_samples_leaf` as the minimum number.\\n        - If float, then `min_samples_leaf` is a fraction and\\n          `ceil(min_samples_leaf * n_samples)` are the minimum\\n          number of samples for each node.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n    - `min_weight_fraction_leaf`: float, default=0.0\\n        The minimum weighted fraction of the sum total of weights (of all\\n        the input samples) required to be at a leaf node. Samples have\\n        equal weight when sample_weight is not provided.\\n\\n    - `max_features`: int, float or {\\\"auto\\\", \\\"sqrt\\\", \\\"log2\\\"}, default=None\\n        The number of features to consider when looking for the best split:\\n\\n            - If int, then consider `max_features` features at each split.\\n            - If float, then `max_features` is a fraction and\\n              `int(max_features * n_features)` features are considered at each\\n              split.\\n            - If \\\"auto\\\", then `max_features=sqrt(n_features)`.\\n            - If \\\"sqrt\\\", then `max_features=sqrt(n_features)`.\\n            - If \\\"log2\\\", then `max_features=log2(n_features)`.\\n            - If None, then `max_features=n_features`.\\n\\n            *Deprecated since 1.1*\\n                The `\\\"auto\\\"` option was deprecated in 1.1 and will be removed\\n                in 1.3.\\n\\n        Note: the search for a split does not stop until at least one\\n        valid partition of the node samples is found, even if it requires to\\n        effectively inspect more than ``max_features`` features.\\n\\n    - `random_state`: int, RandomState instance or None, default=None\\n        Controls the randomness of the estimator. The features are always\\n        randomly permuted at each split, even if ``splitter`` is set to\\n        ``\\\"best\\\"``. When ``max_features < n_features``, the algorithm will\\n        select ``max_features`` at random at each split before finding the best\\n        split among them. But the best found split may vary across different\\n        runs, even if ``max_features=n_features``. That is the case, if the\\n        improvement of the criterion is identical for several splits and one\\n        split has to be selected at random. To obtain a deterministic behaviour\\n        during fitting, ``random_state`` has to be fixed to an integer.\\n        See `Glossary <random_state>` for details.\\n\\n    - `max_leaf_nodes`: int, default=None\\n        Grow a tree with ``max_leaf_nodes`` in best-first fashion.\\n        Best nodes are defined as relative reduction in impurity.\\n        If None then unlimited number of leaf nodes.\\n\\n    - `min_impurity_decrease`: float, default=0.0\\n        A node will be split if this split induces a decrease of the impurity\\n        greater than or equal to this value.\\n\\n        The weighted impurity decrease equation is the following\\n\\n```python\\nN_t / N * (impurity - N_t_R / N_t * right_impurity\\n                    - N_t_L / N_t * left_impurity)\\n\\ne ``N`` is the total number of samples, ``N_t`` is the number of\\nles at the current node, ``N_t_L`` is the number of samples in the\\n child, and ``N_t_R`` is the number of samples in the right child.\\n\\n`, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\\n`sample_weight`` is passed.\\n\\nersionadded:: 0.19\\n\\night : dict, list of dict or \\\"balanced\\\", default=None\\nhts associated with classes in the form ``{class_label: weight}``.\\none, all classes are supposed to have weight one. For\\ni-output problems, a list of dicts can be provided in the same\\nr as the columns of y.\\n\\n that for multioutput (including multilabel) weights should be\\nned for each class of every column in its own dict. For example,\\nfour-class multilabel classification weights should be\\n 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\\n1}, {2:5}, {3:1}, {4:1}].\\n\\n\\\"balanced\\\" mode uses the values of y to automatically adjust\\nhts inversely proportional to class frequencies in the input data\\n`n_samples / (n_classes * np.bincount(y))``\\n\\nmulti-output, the weights of each column of y will be multiplied.\\n\\n that these weights will be multiplied with sample_weight (passed\\nugh the fit method) if sample_weight is specified.\\n\\na : non-negative float, default=0.0\\nlexity parameter used for Minimal Cost-Complexity Pruning. The\\nree with the largest cost complexity that is smaller than\\np_alpha`` will be chosen. By default, no pruning is performed. See\\n:`minimal_cost_complexity_pruning` for details.\\n\\nersionadded:: 0.22\\n\\nes\\n--\\n : ndarray of shape (n_classes,) or list of ndarray\\nclasses labels (single output problem),\\n list of arrays of class labels (multi-output problem).\\n\\nimportances_ : ndarray of shape (n_features,)\\nimpurity-based feature importances.\\nhigher, the more important the feature.\\nimportance of a feature is computed as the (normalized)\\nl reduction of the criterion brought by that feature.  It is also\\nn as the Gini importance [4]_.\\n\\ning: impurity-based feature importances can be misleading for\\n cardinality features (many unique values). See\\nc:`sklearn.inspection.permutation_importance` as an alternative.\\n\\nures_ : int\\ninferred value of max_features.\\n\\ns_ : int or list of int\\nnumber of classes (for single output problems),\\n list containing the number of classes for each\\nut (for multi-output problems).\\n\\nes_ : int\\nnumber of features when ``fit`` is performed.\\n\\neprecated:: 1.0\\nn_features_` is deprecated in 1.0 and will be removed in\\n.2. Use `n_features_in_` instead.\\n\\nes_in_ : int\\ner of features seen during :term:`fit`.\\n\\nersionadded:: 0.24\\n\\nnames_in_ : ndarray of shape (`n_features_in_`,)\\ns of features seen during :term:`fit`. Defined only when `X`\\nfeature names that are all strings.\\n\\nersionadded:: 1.0\\n\\ns_ : int\\nnumber of outputs when ``fit`` is performed.\\n\\nTree instance\\nunderlying Tree object. Please refer to\\nlp(sklearn.tree._tree.Tree)`` for attributes of Tree object and\\n:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\\nbasic usage of these attributes.\\n\\n\\n\\nTreeRegressor : A decision tree regressor.\\n\\n\\n\\nult values for the parameters controlling the size of the trees\\nmax_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\\n trees which can potentially be very large on some data sets. To\\nemory consumption, the complexity and size of the trees should be\\ned by setting those parameter values.\\n\\nh:`predict` method operates using the :func:`numpy.argmax`\\n on the outputs of :meth:`predict_proba`. This means that in\\n highest predicted probabilities are tied, the classifier will\\nthe tied class with the lowest index in :term:`classes_`.\\n\\nes\\n--\\n\\nttps://en.wikipedia.org/wiki/Decision_tree_learning\\n\\n. Breiman, J. Friedman, R. Olshen, and C. Stone, \\\"Classification\\nnd Regression Trees\\\", Wadsworth, Belmont, CA, 1984.\\n\\n. Hastie, R. Tibshirani and J. Friedman. \\\"Elements of Statistical\\nearning\\\", Springer, 2009.\\n\\n. Breiman, and A. Cutler, \\\"Random Forests\\\",\\nttps://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\\n\\n\\n\\n sklearn.datasets import load_iris\\n sklearn.model_selection import cross_val_score\\n sklearn.tree import DecisionTreeClassifier\\n= DecisionTreeClassifier(random_state=0)\\n = load_iris()\\ns_val_score(clf, iris.data, iris.target, cv=10)\\n                        # doctest: +SKIP\\n\\n1.     ,  0.93...,  0.86...,  0.93...,  0.93...,\\n0.93...,  0.93...,  1.     ,  0.93...,  1.      ])\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.classification/dummy-classifier\"} \":sklearn.classification/dummy-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [3 2]:\\n\\n|         :name | :default |\\n|---------------|----------|\\n|     :constant |          |\\n| :random-state |          |\\n|     :strategy |    prior |\\n\"]]] [:span [:p/markdown \"DummyClassifier makes predictions that ignore the input features.\\n\\n    This classifier serves as a simple baseline to compare against other more\\n    complex classifiers.\\n\\n    The specific behavior of the baseline is selected with the `strategy`\\n    parameter.\\n\\n    All strategies make predictions that ignore the input feature values passed\\n    as the `X` argument to `fit` and `predict`. The predictions, however,\\n    typically depend on values observed in the `y` parameter passed to `fit`.\\n\\n    Note that the \\\"stratified\\\" and \\\"uniform\\\" strategies lead to\\n    non-deterministic predictions that can be rendered deterministic by setting\\n    the `random_state` parameter if needed. The other strategies are naturally\\n    deterministic and, once fit, always return a the same constant prediction\\n    for any value of `X`.\\n\\n    Read more in the User Guide: `dummy_estimators`.\\n\\n    *Added in 0.13*\\n\\n    Parameters\\n    ----------\\n    - `strategy`: {\\\"most_frequent\\\", \\\"prior\\\", \\\"stratified\\\", \\\"uniform\\\",             \\\"constant\\\"}, default=\\\"prior\\\"\\n        Strategy to use to generate predictions.\\n\\n        * \\\"most_frequent\\\": the `predict` method always returns the most\\n          frequent class label in the observed `y` argument passed to `fit`.\\n          The `predict_proba` method returns the matching one-hot encoded\\n          vector.\\n        * \\\"prior\\\": the `predict` method always returns the most frequent\\n          class label in the observed `y` argument passed to `fit` (like\\n          \\\"most_frequent\\\"). ``predict_proba`` always returns the empirical\\n          class distribution of `y` also known as the empirical class prior\\n          distribution.\\n        * \\\"stratified\\\": the `predict_proba` method randomly samples one-hot\\n          vectors from a multinomial distribution parametrized by the empirical\\n          class prior probabilities.\\n          The `predict` method returns the class label which got probability\\n          one in the one-hot vector of `predict_proba`.\\n          Each sampled row of both methods is therefore independent and\\n          identically distributed.\\n        * \\\"uniform\\\": generates predictions uniformly at random from the list\\n          of unique classes observed in `y`, i.e. each class has equal\\n          probability.\\n        * \\\"constant\\\": always predicts a constant label that is provided by\\n          the user. This is useful for metrics that evaluate a non-majority\\n          class.\\n\\n          *Changed in 0.24*\\n             The default value of `strategy` has changed to \\\"prior\\\" in version\\n             0.24.\\n\\n    - `random_state`: int, RandomState instance or None, default=None\\n        Controls the randomness to generate the predictions when\\n        ``strategy='stratified'`` or ``strategy='uniform'``.\\n        Pass an int for reproducible output across multiple function calls.\\n        See `Glossary <random_state>`.\\n\\n    - `constant`: int or str or array-like of shape (n_outputs,), default=None\\n        The explicit constant as predicted by the \\\"constant\\\" strategy. This\\n        parameter is useful only for the \\\"constant\\\" strategy.\\n\\n    Attributes\\n    ----------\\n    - `classes_`: ndarray of shape (n_classes,) or list of such arrays\\n        Unique class labels observed in `y`. For multi-output classification\\n        problems, this attribute is a list of arrays as each output has an\\n        independent set of possible classes.\\n\\n    - `n_classes_`: int or list of int\\n        Number of label for each output.\\n\\n    - `class_prior_`: ndarray of shape (n_classes,) or list of such arrays\\n        Frequency of each class observed in `y`. For multioutput classification\\n        problems, this is computed independently for each output.\\n\\n    - `n_outputs_`: int\\n        Number of outputs.\\n\\n    - `n_features_in_`: `None`\\n        Always set to `None`.\\n\\n        *Added in 0.24*\\n        *Deprecated since 1.0*\\n            Will be removed in 1.0\\n\\n    - `sparse_output_`: bool\\n        True if the array returned from predict is to be in sparse CSC format.\\n        Is automatically set to True if the input `y` is passed in sparse\\n        format.\\n\\n    See Also\\n    --------\\n    - `DummyRegressor`: Regressor that makes predictions using simple rules.\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> from sklearn.dummy import DummyClassifier\\n    >>> X = np.array([-1, 1, 1, 1])\\n    >>> y = np.array([0, 1, 1, 1])\\n    >>> dummy_clf = DummyClassifier(strategy=\\\"most_frequent\\\")\\n    >>> dummy_clf.fit(X, y)\\n    DummyClassifier(strategy='most_frequent')\\n    >>> dummy_clf.predict(X)\\n    array([1, 1, 1, 1])\\n    >>> dummy_clf.score(X, y)\\n    0.75\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.classification/extra-tree-classifier\"} \":sklearn.classification/extra-tree-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [12 2]:\\n\\n|                     :name | :default |\\n|---------------------------|----------|\\n| :min-weight-fraction-leaf |    0.000 |\\n|           :max-leaf-nodes |          |\\n|    :min-impurity-decrease |    0.000 |\\n|        :min-samples-split |    2.000 |\\n|                :ccp-alpha |    0.000 |\\n|                 :splitter |   random |\\n|             :random-state |          |\\n|         :min-samples-leaf |        1 |\\n|             :max-features |     sqrt |\\n|                :max-depth |          |\\n|             :class-weight |          |\\n|                :criterion |     gini |\\n\"]]] [:span [:p/markdown \"An extremely randomized tree classifier.\\n\\n    Extra-trees differ from classic decision trees in the way they are built.\\n    When looking for the best split to separate the samples of a node into two\\n    groups, random splits are drawn for each of the `max_features` randomly\\n    selected features and the best split among those is chosen. When\\n    `max_features` is set 1, this amounts to building a totally random\\n    decision tree.\\n\\n    Warning: Extra-trees should only be used within ensemble methods.\\n\\n    Read more in the User Guide: `tree`.\\n\\n    Parameters\\n    ----------\\n    - `criterion`: {\\\"gini\\\", \\\"entropy\\\", \\\"log_loss\\\"}, default=\\\"gini\\\"\\n        The function to measure the quality of a split. Supported criteria are\\n        \\\"gini\\\" for the Gini impurity and \\\"log_loss\\\" and \\\"entropy\\\" both for the\\n        Shannon information gain, see :ref:`tree_mathematical_formulation`.\\n\\n    - `splitter`: {\\\"random\\\", \\\"best\\\"}, default=\\\"random\\\"\\n        The strategy used to choose the split at each node. Supported\\n        strategies are \\\"best\\\" to choose the best split and \\\"random\\\" to choose\\n        the best random split.\\n\\n    - `max_depth`: int, default=None\\n        The maximum depth of the tree. If None, then nodes are expanded until\\n        all leaves are pure or until all leaves contain less than\\n        min_samples_split samples.\\n\\n    - `min_samples_split`: int or float, default=2\\n        The minimum number of samples required to split an internal node:\\n\\n        - If int, then consider `min_samples_split` as the minimum number.\\n        - If float, then `min_samples_split` is a fraction and\\n          `ceil(min_samples_split * n_samples)` are the minimum\\n          number of samples for each split.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n    - `min_samples_leaf`: int or float, default=1\\n        The minimum number of samples required to be at a leaf node.\\n        A split point at any depth will only be considered if it leaves at\\n        least ``min_samples_leaf`` training samples in each of the left and\\n        right branches.  This may have the effect of smoothing the model,\\n        especially in regression.\\n\\n        - If int, then consider `min_samples_leaf` as the minimum number.\\n        - If float, then `min_samples_leaf` is a fraction and\\n          `ceil(min_samples_leaf * n_samples)` are the minimum\\n          number of samples for each node.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n    - `min_weight_fraction_leaf`: float, default=0.0\\n        The minimum weighted fraction of the sum total of weights (of all\\n        the input samples) required to be at a leaf node. Samples have\\n        equal weight when sample_weight is not provided.\\n\\n    - `max_features`: int, float, {\\\"auto\\\", \\\"sqrt\\\", \\\"log2\\\"} or None, default=\\\"sqrt\\\"\\n        The number of features to consider when looking for the best split:\\n\\n            - If int, then consider `max_features` features at each split.\\n            - If float, then `max_features` is a fraction and\\n              `int(max_features * n_features)` features are considered at each\\n              split.\\n            - If \\\"auto\\\", then `max_features=sqrt(n_features)`.\\n            - If \\\"sqrt\\\", then `max_features=sqrt(n_features)`.\\n            - If \\\"log2\\\", then `max_features=log2(n_features)`.\\n            - If None, then `max_features=n_features`.\\n\\n            *Changed in 1.1*\\n                The default of `max_features` changed from `\\\"auto\\\"` to `\\\"sqrt\\\"`.\\n\\n            *Deprecated since 1.1*\\n                The `\\\"auto\\\"` option was deprecated in 1.1 and will be removed\\n                in 1.3.\\n\\n        Note: the search for a split does not stop until at least one\\n        valid partition of the node samples is found, even if it requires to\\n        effectively inspect more than ``max_features`` features.\\n\\n    - `random_state`: int, RandomState instance or None, default=None\\n        Used to pick randomly the `max_features` used at each split.\\n        See `Glossary <random_state>` for details.\\n\\n    - `max_leaf_nodes`: int, default=None\\n        Grow a tree with ``max_leaf_nodes`` in best-first fashion.\\n        Best nodes are defined as relative reduction in impurity.\\n        If None then unlimited number of leaf nodes.\\n\\n    - `min_impurity_decrease`: float, default=0.0\\n        A node will be split if this split induces a decrease of the impurity\\n        greater than or equal to this value.\\n\\n        The weighted impurity decrease equation is the following\\n\\n```python\\nN_t / N * (impurity - N_t_R / N_t * right_impurity\\n                    - N_t_L / N_t * left_impurity)\\n\\ne ``N`` is the total number of samples, ``N_t`` is the number of\\nles at the current node, ``N_t_L`` is the number of samples in the\\n child, and ``N_t_R`` is the number of samples in the right child.\\n\\n`, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\\n`sample_weight`` is passed.\\n\\nersionadded:: 0.19\\n\\night : dict, list of dict or \\\"balanced\\\", default=None\\nhts associated with classes in the form ``{class_label: weight}``.\\none, all classes are supposed to have weight one. For\\ni-output problems, a list of dicts can be provided in the same\\nr as the columns of y.\\n\\n that for multioutput (including multilabel) weights should be\\nned for each class of every column in its own dict. For example,\\nfour-class multilabel classification weights should be\\n 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\\n1}, {2:5}, {3:1}, {4:1}].\\n\\n\\\"balanced\\\" mode uses the values of y to automatically adjust\\nhts inversely proportional to class frequencies in the input data\\n`n_samples / (n_classes * np.bincount(y))``\\n\\nmulti-output, the weights of each column of y will be multiplied.\\n\\n that these weights will be multiplied with sample_weight (passed\\nugh the fit method) if sample_weight is specified.\\n\\na : non-negative float, default=0.0\\nlexity parameter used for Minimal Cost-Complexity Pruning. The\\nree with the largest cost complexity that is smaller than\\np_alpha`` will be chosen. By default, no pruning is performed. See\\n:`minimal_cost_complexity_pruning` for details.\\n\\nersionadded:: 0.22\\n\\nes\\n--\\n : ndarray of shape (n_classes,) or list of ndarray\\nclasses labels (single output problem),\\n list of arrays of class labels (multi-output problem).\\n\\nures_ : int\\ninferred value of max_features.\\n\\ns_ : int or list of int\\nnumber of classes (for single output problems),\\n list containing the number of classes for each\\nut (for multi-output problems).\\n\\nimportances_ : ndarray of shape (n_features,)\\nimpurity-based feature importances.\\nhigher, the more important the feature.\\nimportance of a feature is computed as the (normalized)\\nl reduction of the criterion brought by that feature.  It is also\\nn as the Gini importance.\\n\\ning: impurity-based feature importances can be misleading for\\n cardinality features (many unique values). See\\nc:`sklearn.inspection.permutation_importance` as an alternative.\\n\\nes_ : int\\nnumber of features when ``fit`` is performed.\\n\\neprecated:: 1.0\\nn_features_` is deprecated in 1.0 and will be removed in\\n.2. Use `n_features_in_` instead.\\n\\nes_in_ : int\\ner of features seen during :term:`fit`.\\n\\nersionadded:: 0.24\\n\\nnames_in_ : ndarray of shape (`n_features_in_`,)\\ns of features seen during :term:`fit`. Defined only when `X`\\nfeature names that are all strings.\\n\\nersionadded:: 1.0\\n\\ns_ : int\\nnumber of outputs when ``fit`` is performed.\\n\\nTree instance\\nunderlying Tree object. Please refer to\\nlp(sklearn.tree._tree.Tree)`` for attributes of Tree object and\\n:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\\nbasic usage of these attributes.\\n\\n\\n\\neRegressor : An extremely randomized tree regressor.\\nensemble.ExtraTreesClassifier : An extra-trees classifier.\\nensemble.ExtraTreesRegressor : An extra-trees regressor.\\nensemble.RandomForestClassifier : A random forest classifier.\\nensemble.RandomForestRegressor : A random forest regressor.\\nensemble.RandomTreesEmbedding : An ensemble of\\nlly random trees.\\n\\n\\n\\nult values for the parameters controlling the size of the trees\\nmax_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\\n trees which can potentially be very large on some data sets. To\\nemory consumption, the complexity and size of the trees should be\\ned by setting those parameter values.\\n\\nes\\n--\\n\\n. Geurts, D. Ernst., and L. Wehenkel, \\\"Extremely randomized trees\\\",\\nachine Learning, 63(1), 3-42, 2006.\\n\\n\\n\\n sklearn.datasets import load_iris\\n sklearn.model_selection import train_test_split\\n sklearn.ensemble import BaggingClassifier\\n sklearn.tree import ExtraTreeClassifier\\n = load_iris(return_X_y=True)\\nain, X_test, y_train, y_test = train_test_split(\\n, y, random_state=0)\\na_tree = ExtraTreeClassifier(random_state=0)\\n= BaggingClassifier(extra_tree, random_state=0).fit(\\n_train, y_train)\\nscore(X_test, y_test)\\n.\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.classification/extra-trees-classifier\"} \":sklearn.classification/extra-trees-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [18 2]:\\n\\n|                     :name | :default |\\n|---------------------------|----------|\\n| :min-weight-fraction-leaf |    0.000 |\\n|           :max-leaf-nodes |          |\\n|    :min-impurity-decrease |    0.000 |\\n|        :min-samples-split |    2.000 |\\n|                :bootstrap |    false |\\n|                :ccp-alpha |    0.000 |\\n|                   :n-jobs |          |\\n|             :random-state |          |\\n|                :oob-score |    false |\\n|         :min-samples-leaf |        1 |\\n|             :max-features |     sqrt |\\n|               :warm-start |    false |\\n|                :max-depth |          |\\n|             :class-weight |          |\\n|             :n-estimators |      100 |\\n|              :max-samples |          |\\n|                :criterion |     gini |\\n|                  :verbose |        0 |\\n\"]]] [:span [:p/markdown \"\\n    An extra-trees classifier.\\n\\n    This class implements a meta estimator that fits a number of\\n    randomized decision trees (a.k.a. extra-trees) on various sub-samples\\n    of the dataset and uses averaging to improve the predictive accuracy\\n    and control over-fitting.\\n\\n    Read more in the User Guide: `forest`.\\n\\n    Parameters\\n    ----------\\n    - `n_estimators`: int, default=100\\n        The number of trees in the forest.\\n\\n        *Changed in 0.22*\\n           The default value of ``n_estimators`` changed from 10 to 100\\n           in 0.22.\\n\\n    - `criterion`: {\\\"gini\\\", \\\"entropy\\\", \\\"log_loss\\\"}, default=\\\"gini\\\"\\n        The function to measure the quality of a split. Supported criteria are\\n        \\\"gini\\\" for the Gini impurity and \\\"log_loss\\\" and \\\"entropy\\\" both for the\\n        Shannon information gain, see :ref:`tree_mathematical_formulation`.\\n        Note: This parameter is tree-specific.\\n\\n    - `max_depth`: int, default=None\\n        The maximum depth of the tree. If None, then nodes are expanded until\\n        all leaves are pure or until all leaves contain less than\\n        min_samples_split samples.\\n\\n    - `min_samples_split`: int or float, default=2\\n        The minimum number of samples required to split an internal node:\\n\\n        - If int, then consider `min_samples_split` as the minimum number.\\n        - If float, then `min_samples_split` is a fraction and\\n          `ceil(min_samples_split * n_samples)` are the minimum\\n          number of samples for each split.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n    - `min_samples_leaf`: int or float, default=1\\n        The minimum number of samples required to be at a leaf node.\\n        A split point at any depth will only be considered if it leaves at\\n        least ``min_samples_leaf`` training samples in each of the left and\\n        right branches.  This may have the effect of smoothing the model,\\n        especially in regression.\\n\\n        - If int, then consider `min_samples_leaf` as the minimum number.\\n        - If float, then `min_samples_leaf` is a fraction and\\n          `ceil(min_samples_leaf * n_samples)` are the minimum\\n          number of samples for each node.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n    - `min_weight_fraction_leaf`: float, default=0.0\\n        The minimum weighted fraction of the sum total of weights (of all\\n        the input samples) required to be at a leaf node. Samples have\\n        equal weight when sample_weight is not provided.\\n\\n    - `max_features`: {\\\"sqrt\\\", \\\"log2\\\", None}, int or float, default=\\\"sqrt\\\"\\n        The number of features to consider when looking for the best split:\\n\\n        - If int, then consider `max_features` features at each split.\\n        - If float, then `max_features` is a fraction and\\n          `round(max_features * n_features)` features are considered at each\\n          split.\\n        - If \\\"auto\\\", then `max_features=sqrt(n_features)`.\\n        - If \\\"sqrt\\\", then `max_features=sqrt(n_features)`.\\n        - If \\\"log2\\\", then `max_features=log2(n_features)`.\\n        - If None, then `max_features=n_features`.\\n\\n        *Changed in 1.1*\\n            The default of `max_features` changed from `\\\"auto\\\"` to `\\\"sqrt\\\"`.\\n\\n        *Deprecated since 1.1*\\n            The `\\\"auto\\\"` option was deprecated in 1.1 and will be removed\\n            in 1.3.\\n\\n        Note: the search for a split does not stop until at least one\\n        valid partition of the node samples is found, even if it requires to\\n        effectively inspect more than ``max_features`` features.\\n\\n    - `max_leaf_nodes`: int, default=None\\n        Grow trees with ``max_leaf_nodes`` in best-first fashion.\\n        Best nodes are defined as relative reduction in impurity.\\n        If None then unlimited number of leaf nodes.\\n\\n    - `min_impurity_decrease`: float, default=0.0\\n        A node will be split if this split induces a decrease of the impurity\\n        greater than or equal to this value.\\n\\n        The weighted impurity decrease equation is the following\\n\\n```python\\nN_t / N * (impurity - N_t_R / N_t * right_impurity\\n                    - N_t_L / N_t * left_impurity)\\n\\ne ``N`` is the total number of samples, ``N_t`` is the number of\\nles at the current node, ``N_t_L`` is the number of samples in the\\n child, and ``N_t_R`` is the number of samples in the right child.\\n\\n`, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\\n`sample_weight`` is passed.\\n\\nersionadded:: 0.19\\n\\np : bool, default=False\\nher bootstrap samples are used when building trees. If False, the\\ne dataset is used to build each tree.\\n\\ne : bool, default=False\\nher to use out-of-bag samples to estimate the generalization score.\\n available if bootstrap=True.\\n\\n int, default=None\\nnumber of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\\nh:`decision_path` and :meth:`apply` are all parallelized over the\\ns. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\\next. ``-1`` means using all processors. See :term:`Glossary\\nobs>` for more details.\\n\\ntate : int, RandomState instance or None, default=None\\nrols 3 sources of randomness:\\n\\ne bootstrapping of the samples used when building trees\\nf ``bootstrap=True``)\\ne sampling of the features to consider when looking for the best\\nlit at each node (if ``max_features < n_features``)\\ne draw of the splits for each of the `max_features`\\n\\n:term:`Glossary <random_state>` for details.\\n\\n: int, default=0\\nrols the verbosity when fitting and predicting.\\n\\nrt : bool, default=False\\n set to ``True``, reuse the solution of the previous call to fit\\nadd more estimators to the ensemble, otherwise, just fit a whole\\nforest. See :term:`the Glossary <warm_start>`.\\n\\night : {\\\"balanced\\\", \\\"balanced_subsample\\\"}, dict or list of dicts,             default=None\\nhts associated with classes in the form ``{class_label: weight}``.\\not given, all classes are supposed to have weight one. For\\ni-output problems, a list of dicts can be provided in the same\\nr as the columns of y.\\n\\n that for multioutput (including multilabel) weights should be\\nned for each class of every column in its own dict. For example,\\nfour-class multilabel classification weights should be\\n 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\\n1}, {2:5}, {3:1}, {4:1}].\\n\\n\\\"balanced\\\" mode uses the values of y to automatically adjust\\nhts inversely proportional to class frequencies in the input data\\n`n_samples / (n_classes * np.bincount(y))``\\n\\n\\\"balanced_subsample\\\" mode is the same as \\\"balanced\\\" except that\\nhts are computed based on the bootstrap sample for every tree\\nn.\\n\\nmulti-output, the weights of each column of y will be multiplied.\\n\\n that these weights will be multiplied with sample_weight (passed\\nugh the fit method) if sample_weight is specified.\\n\\na : non-negative float, default=0.0\\nlexity parameter used for Minimal Cost-Complexity Pruning. The\\nree with the largest cost complexity that is smaller than\\np_alpha`` will be chosen. By default, no pruning is performed. See\\n:`minimal_cost_complexity_pruning` for details.\\n\\nersionadded:: 0.22\\n\\nles : int or float, default=None\\nootstrap is True, the number of samples to draw from X\\nrain each base estimator.\\n\\n None (default), then draw `X.shape[0]` samples.\\n int, then draw `max_samples` samples.\\n float, then draw `max_samples * X.shape[0]` samples. Thus,\\nax_samples` should be in the interval `(0.0, 1.0]`.\\n\\nersionadded:: 0.22\\n\\nes\\n--\\nimator_ : ExtraTreesClassifier\\nchild estimator template used to create the collection of fitted\\nestimators.\\n\\nrs_ : list of DecisionTreeClassifier\\ncollection of fitted sub-estimators.\\n\\n : ndarray of shape (n_classes,) or a list of such arrays\\nclasses labels (single output problem), or a list of arrays of\\ns labels (multi-output problem).\\n\\ns_ : int or list\\nnumber of classes (single output problem), or a list containing the\\ner of classes for each output (multi-output problem).\\n\\nimportances_ : ndarray of shape (n_features,)\\nimpurity-based feature importances.\\nhigher, the more important the feature.\\nimportance of a feature is computed as the (normalized)\\nl reduction of the criterion brought by that feature.  It is also\\nn as the Gini importance.\\n\\ning: impurity-based feature importances can be misleading for\\n cardinality features (many unique values). See\\nc:`sklearn.inspection.permutation_importance` as an alternative.\\n\\nes_ : int\\nnumber of features when ``fit`` is performed.\\n\\neprecated:: 1.0\\nAttribute `n_features_` was deprecated in version 1.0 and will be\\nremoved in 1.2. Use `n_features_in_` instead.\\n\\nes_in_ : int\\ner of features seen during :term:`fit`.\\n\\nersionadded:: 0.24\\n\\nnames_in_ : ndarray of shape (`n_features_in_`,)\\ns of features seen during :term:`fit`. Defined only when `X`\\nfeature names that are all strings.\\n\\nersionadded:: 1.0\\n\\ns_ : int\\nnumber of outputs when ``fit`` is performed.\\n\\ne_ : float\\ne of the training dataset obtained using an out-of-bag estimate.\\n attribute exists only when ``oob_score`` is True.\\n\\nsion_function_ : ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)\\nsion function computed with out-of-bag estimate on the training\\n If n_estimators is small it might be possible that a data point\\nnever left out during the bootstrap. In this case,\\n_decision_function_` might contain NaN. This attribute exists\\n when ``oob_score`` is True.\\n\\n\\n\\nesRegressor : An extra-trees regressor with random splits.\\nrestClassifier : A random forest classifier with optimal splits.\\nrestRegressor : Ensemble regressor using trees with optimal splits.\\n\\n\\n\\nult values for the parameters controlling the size of the trees\\nmax_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\\n trees which can potentially be very large on some data sets. To\\nemory consumption, the complexity and size of the trees should be\\ned by setting those parameter values.\\n\\nes\\n--\\n. Geurts, D. Ernst., and L. Wehenkel, \\\"Extremely randomized\\nrees\\\", Machine Learning, 63(1), 3-42, 2006.\\n\\n\\n\\n sklearn.ensemble import ExtraTreesClassifier\\n sklearn.datasets import make_classification\\n = make_classification(n_features=4, random_state=0)\\n= ExtraTreesClassifier(n_estimators=100, random_state=0)\\nfit(X, y)\\nesClassifier(random_state=0)\\npredict([[0, 0, 0, 0]])\\n])\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.classification/gaussian-nb\"} \":sklearn.classification/gaussian-nb\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [2 2]:\\n\\n|          :name | :default |\\n|----------------|---------:|\\n|        :priors |          |\\n| :var-smoothing |  1.0E-09 |\\n\"]]] [:span [:p/markdown \"\\n    Gaussian Naive Bayes (GaussianNB).\\n\\n    Can perform online updates to model parameters via `partial_fit`.\\n    For details on algorithm used to update feature means and variance online,\\n    see Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:\\n\\n        http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf\\n\\n    Read more in the User Guide: `gaussian_naive_bayes`.\\n\\n    Parameters\\n    ----------\\n    - `priors`: array-like of shape (n_classes,)\\n        Prior probabilities of the classes. If specified, the priors are not\\n        adjusted according to the data.\\n\\n    - `var_smoothing`: float, default=1e-9\\n        Portion of the largest variance of all features that is added to\\n        variances for calculation stability.\\n\\n        *Added in 0.20*\\n\\n    Attributes\\n    ----------\\n    - `class_count_`: ndarray of shape (n_classes,)\\n        number of training samples observed in each class.\\n\\n    - `class_prior_`: ndarray of shape (n_classes,)\\n        probability of each class.\\n\\n    - `classes_`: ndarray of shape (n_classes,)\\n        class labels known to the classifier.\\n\\n    - `epsilon_`: float\\n        absolute additive value to variances.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    - `sigma_`: ndarray of shape (n_classes, n_features)\\n        Variance of each feature per class.\\n\\n        *Deprecated since 1.0*\\n           `sigma_` is deprecated in 1.0 and will be removed in 1.2.\\n           Use `var_` instead.\\n\\n    - `var_`: ndarray of shape (n_classes, n_features)\\n        Variance of each feature per class.\\n\\n        *Added in 1.0*\\n\\n    - `theta_`: ndarray of shape (n_classes, n_features)\\n        mean of each feature per class.\\n\\n    See Also\\n    --------\\n    - `BernoulliNB`: Naive Bayes classifier for multivariate Bernoulli models.\\n    - `CategoricalNB`: Naive Bayes classifier for categorical features.\\n    - `ComplementNB`: Complement Naive Bayes classifier.\\n    - `MultinomialNB`: Naive Bayes classifier for multinomial models.\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\\n    >>> Y = np.array([1, 1, 1, 2, 2, 2])\\n    >>> from sklearn.naive_bayes import GaussianNB\\n    >>> clf = GaussianNB()\\n    >>> clf.fit(X, Y)\\n    GaussianNB()\\n    >>> print(clf.predict([[-0.8, -1]]))\\n    [1]\\n    >>> clf_pf = GaussianNB()\\n    >>> clf_pf.partial_fit(X, Y, np.unique(Y))\\n    GaussianNB()\\n    >>> print(clf_pf.predict([[-0.8, -1]]))\\n    [1]\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.classification/gaussian-process-classifier\"} \":sklearn.classification/gaussian-process-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [9 2]:\\n\\n|                 :name |      :default |\\n|-----------------------|---------------|\\n|               :kernel |               |\\n|            :optimizer | fmin_l_bfgs_b |\\n|          :multi-class |   one_vs_rest |\\n|               :n-jobs |               |\\n|         :random-state |               |\\n|     :max-iter-predict |           100 |\\n|         :copy-x-train |          true |\\n| :n-restarts-optimizer |             0 |\\n|           :warm-start |         false |\\n\"]]] [:span [:p/markdown \"Gaussian process classification (GPC) based on Laplace approximation.\\n\\n    The implementation is based on Algorithm 3.1, 3.2, and 5.1 of\\n    Gaussian Processes for Machine Learning (GPML) by Rasmussen and\\n    Williams.\\n\\n    Internally, the Laplace approximation is used for approximating the\\n    non-Gaussian posterior by a Gaussian.\\n\\n    Currently, the implementation is restricted to using the logistic link\\n    function. For multi-class classification, several binary one-versus rest\\n    classifiers are fitted. Note that this class thus does not implement\\n    a true multi-class Laplace approximation.\\n\\n    Read more in the User Guide: `gaussian_process`.\\n\\n    *Added in 0.18*\\n\\n    Parameters\\n    ----------\\n    - `kernel`: kernel instance, default=None\\n        The kernel specifying the covariance function of the GP. If None is\\n        passed, the kernel \\\"1.0 * RBF(1.0)\\\" is used as default. Note that\\n        the kernel's hyperparameters are optimized during fitting. Also kernel\\n        cannot be a `CompoundKernel`.\\n\\n    - `optimizer`: 'fmin_l_bfgs_b' or callable, default='fmin_l_bfgs_b'\\n        Can either be one of the internally supported optimizers for optimizing\\n        the kernel's parameters, specified by a string, or an externally\\n        defined optimizer passed as a callable. If a callable is passed, it\\n        must have the  signature\\n\\n```python\\ndef optimizer(obj_func, initial_theta, bounds):\\n    # * 'obj_func' is the objective function to be maximized, which\\n    #   takes the hyperparameters theta as parameter and an\\n    #   optional flag eval_gradient, which determines if the\\n    #   gradient is returned additionally to the function value\\n    # * 'initial_theta': the initial value for theta, which can be\\n    #   used by local optimizers\\n    # * 'bounds': the bounds on the values of theta\\n    ....\\n    # Returned are the best found hyperparameters theta and\\n    # the corresponding value of the target function.\\n    return theta_opt, func_min\\n\\ndefault, the 'L-BFGS-B' algorithm from scipy.optimize.minimize\\nsed. If None is passed, the kernel's parameters are kept fixed.\\nlable internal optimizers are::\\n\\n'fmin_l_bfgs_b'\\n\\nts_optimizer : int, default=0\\nnumber of restarts of the optimizer for finding the kernel's\\nmeters which maximize the log-marginal likelihood. The first run\\nhe optimizer is performed from the kernel's initial parameters,\\nremaining ones (if any) from thetas sampled log-uniform randomly\\n the space of allowed theta-values. If greater than 0, all bounds\\n be finite. Note that n_restarts_optimizer=0 implies that one\\nis performed.\\n\\n_predict : int, default=100\\nmaximum number of iterations in Newton's method for approximating\\nposterior during predict. Smaller values will reduce computation\\n at the cost of worse results.\\n\\nrt : bool, default=False\\narm-starts are enabled, the solution of the last Newton iteration\\nhe Laplace approximation of the posterior mode is used as\\nialization for the next call of _posterior_mode(). This can speed\\nonvergence when _posterior_mode is called several times on similar\\nlems as in hyperparameter optimization. See :term:`the Glossary\\nm_start>`.\\n\\nrain : bool, default=True\\nrue, a persistent copy of the training data is stored in the\\nct. Otherwise, just a reference to the training data is stored,\\nh might cause predictions to change if the data is modified\\nrnally.\\n\\ntate : int, RandomState instance or None, default=None\\nrmines random number generation used to initialize the centers.\\n an int for reproducible results across multiple function calls.\\n:term:`Glossary <random_state>`.\\n\\nass : {'one_vs_rest', 'one_vs_one'}, default='one_vs_rest'\\nifies how multi-class classification problems are handled.\\norted are 'one_vs_rest' and 'one_vs_one'. In 'one_vs_rest',\\nbinary Gaussian process classifier is fitted for each class, which\\nrained to separate this class from the rest. In 'one_vs_one', one\\nry Gaussian process classifier is fitted for each pair of classes,\\nh is trained to separate these two classes. The predictions of\\ne binary predictors are combined into multi-class predictions.\\n that 'one_vs_one' does not support predicting probability\\nmates.\\n\\n int, default=None\\nnumber of jobs to use for the computation: the specified\\niclass problems are computed in parallel.\\nne`` means 1 unless in a :obj:`joblib.parallel_backend` context.\\n`` means using all processors. See :term:`Glossary <n_jobs>`\\nmore details.\\n\\nes\\n--\\nimator_ : ``Estimator`` instance\\nestimator instance that defines the likelihood function\\ng the observed data.\\n\\n: kernel instance\\nkernel used for prediction. In case of binary classification,\\nstructure of the kernel is the same as the one passed as parameter\\nwith optimized hyperparameters. In case of multi-class\\nsification, a CompoundKernel is returned which consists of the\\nerent kernels used in the one-versus-rest classifiers.\\n\\ninal_likelihood_value_ : float\\nlog-marginal-likelihood of ``self.kernel_.theta``\\n\\n : array-like of shape (n_classes,)\\nue class labels.\\n\\ns_ : int\\nnumber of classes in the training data\\n\\nes_in_ : int\\ner of features seen during :term:`fit`.\\n\\nersionadded:: 0.24\\n\\nnames_in_ : ndarray of shape (`n_features_in_`,)\\ns of features seen during :term:`fit`. Defined only when `X`\\nfeature names that are all strings.\\n\\nersionadded:: 1.0\\n\\n\\n\\nProcessRegressor : Gaussian process regression (GPR).\\n\\n\\n\\n sklearn.datasets import load_iris\\n sklearn.gaussian_process import GaussianProcessClassifier\\n sklearn.gaussian_process.kernels import RBF\\n = load_iris(return_X_y=True)\\nel = 1.0 * RBF(1.0)\\n= GaussianProcessClassifier(kernel=kernel,\\n    random_state=0).fit(X, y)\\nscore(X, y)\\n.\\npredict_proba(X[:2,:])\\n0.83548752, 0.03228706, 0.13222543],\\n0.79064206, 0.06525643, 0.14410151]])\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.classification/gradient-boosting-classifier\"} \":sklearn.classification/gradient-boosting-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [20 2]:\\n\\n|                     :name |     :default |\\n|---------------------------|--------------|\\n|         :n-iter-no-change |              |\\n|            :learning-rate |       0.1000 |\\n| :min-weight-fraction-leaf |        0.000 |\\n|           :max-leaf-nodes |              |\\n|    :min-impurity-decrease |        0.000 |\\n|        :min-samples-split |        2.000 |\\n|                      :tol |    0.0001000 |\\n|                :subsample |        1.000 |\\n|                :ccp-alpha |        0.000 |\\n|             :random-state |              |\\n|         :min-samples-leaf |        1.000 |\\n|             :max-features |              |\\n|                     :init |              |\\n|               :warm-start |        false |\\n|                :max-depth |            3 |\\n|      :validation-fraction |       0.1000 |\\n|             :n-estimators |          100 |\\n|                :criterion | friedman_mse |\\n|                     :loss |     log_loss |\\n|                  :verbose |            0 |\\n\"]]] [:span [:p/markdown \"Gradient Boosting for classification.\\n\\n    GB builds an additive model in a\\n    forward stage-wise fashion; it allows for the optimization of\\n    arbitrary differentiable loss functions. In each stage ``n_classes_``\\n    regression trees are fit on the negative gradient of the loss function,\\n    e.g. binary or multiclass log loss. Binary classification\\n    is a special case where only a single regression tree is induced.\\n\\n    Read more in the User Guide: `gradient_boosting`.\\n\\n    Parameters\\n    ----------\\n    - `loss`: {'log_loss', 'deviance', 'exponential'}, default='log_loss'\\n        The loss function to be optimized. 'log_loss' refers to binomial and\\n        multinomial deviance, the same as used in logistic regression.\\n        It is a good choice for classification with probabilistic outputs.\\n        For loss 'exponential', gradient boosting recovers the AdaBoost algorithm.\\n\\n        *Deprecated since 1.1*\\n            The loss 'deviance' was deprecated in v1.1 and will be removed in\\n            version 1.3. Use `loss='log_loss'` which is equivalent.\\n\\n    - `learning_rate`: float, default=0.1\\n        Learning rate shrinks the contribution of each tree by `learning_rate`.\\n        There is a trade-off between learning_rate and n_estimators.\\n        Values must be in the range `(0.0, inf)`.\\n\\n    - `n_estimators`: int, default=100\\n        The number of boosting stages to perform. Gradient boosting\\n        is fairly robust to over-fitting so a large number usually\\n        results in better performance.\\n        Values must be in the range `[1, inf)`.\\n\\n    - `subsample`: float, default=1.0\\n        The fraction of samples to be used for fitting the individual base\\n        learners. If smaller than 1.0 this results in Stochastic Gradient\\n        Boosting. `subsample` interacts with the parameter `n_estimators`.\\n        Choosing `subsample < 1.0` leads to a reduction of variance\\n        and an increase in bias.\\n        Values must be in the range `(0.0, 1.0]`.\\n\\n    - `criterion`: {'friedman_mse', 'squared_error', 'mse'},             default='friedman_mse'\\n        The function to measure the quality of a split. Supported criteria are\\n        'friedman_mse' for the mean squared error with improvement score by\\n        Friedman, 'squared_error' for mean squared error. The default value of\\n        'friedman_mse' is generally the best as it can provide a better\\n        approximation in some cases.\\n\\n        *Added in 0.18*\\n\\n        *Deprecated since 1.0*\\n            Criterion 'mse' was deprecated in v1.0 and will be removed in\\n            version 1.2. Use `criterion='squared_error'` which is equivalent.\\n\\n    - `min_samples_split`: int or float, default=2\\n        The minimum number of samples required to split an internal node:\\n\\n        - If int, values must be in the range `[2, inf)`.\\n        - If float, values must be in the range `(0.0, 1.0]` and `min_samples_split`\\n          will be `ceil(min_samples_split * n_samples)`.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n    - `min_samples_leaf`: int or float, default=1\\n        The minimum number of samples required to be at a leaf node.\\n        A split point at any depth will only be considered if it leaves at\\n        least ``min_samples_leaf`` training samples in each of the left and\\n        right branches.  This may have the effect of smoothing the model,\\n        especially in regression.\\n\\n        - If int, values must be in the range `[1, inf)`.\\n        - If float, values must be in the range `(0.0, 1.0]` and `min_samples_leaf`\\n          will be `ceil(min_samples_leaf * n_samples)`.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n    - `min_weight_fraction_leaf`: float, default=0.0\\n        The minimum weighted fraction of the sum total of weights (of all\\n        the input samples) required to be at a leaf node. Samples have\\n        equal weight when sample_weight is not provided.\\n        Values must be in the range `[0.0, 0.5]`.\\n\\n    - `max_depth`: int, default=3\\n        The maximum depth of the individual regression estimators. The maximum\\n        depth limits the number of nodes in the tree. Tune this parameter\\n        for best performance; the best value depends on the interaction\\n        of the input variables.\\n        Values must be in the range `[1, inf)`.\\n\\n    - `min_impurity_decrease`: float, default=0.0\\n        A node will be split if this split induces a decrease of the impurity\\n        greater than or equal to this value.\\n        Values must be in the range `[0.0, inf)`.\\n\\n        The weighted impurity decrease equation is the following\\n\\n```python\\nN_t / N * (impurity - N_t_R / N_t * right_impurity\\n                    - N_t_L / N_t * left_impurity)\\n\\ne ``N`` is the total number of samples, ``N_t`` is the number of\\nles at the current node, ``N_t_L`` is the number of samples in the\\n child, and ``N_t_R`` is the number of samples in the right child.\\n\\n`, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\\n`sample_weight`` is passed.\\n\\nersionadded:: 0.19\\n\\nstimator or 'zero', default=None\\nstimator object that is used to compute the initial predictions.\\nit`` has to provide :meth:`fit` and :meth:`predict_proba`. If\\no', the initial raw predictions are set to zero. By default, a\\nmmyEstimator`` predicting the classes priors is used.\\n\\ntate : int, RandomState instance or None, default=None\\nrols the random seed given to each Tree estimator at each\\nting iteration.\\nddition, it controls the random permutation of the features at\\n split (see Notes for more details).\\nlso controls the random splitting of the training data to obtain a\\ndation set if `n_iter_no_change` is not None.\\n an int for reproducible output across multiple function calls.\\n:term:`Glossary <random_state>`.\\n\\nures : {'auto', 'sqrt', 'log2'}, int or float, default=None\\nnumber of features to consider when looking for the best split:\\n\\n int, values must be in the range `[1, inf)`.\\n float, values must be in the range `(0.0, 1.0]` and the features\\nnsidered at each split will be `int(max_features * n_features)`.\\n 'auto', then `max_features=sqrt(n_features)`.\\n 'sqrt', then `max_features=sqrt(n_features)`.\\n 'log2', then `max_features=log2(n_features)`.\\n None, then `max_features=n_features`.\\n\\nsing `max_features < n_features` leads to a reduction of variance\\nan increase in bias.\\n\\n: the search for a split does not stop until at least one\\nd partition of the node samples is found, even if it requires to\\nctively inspect more than ``max_features`` features.\\n\\n: int, default=0\\nle verbose output. If 1 then it prints progress and performance\\n in a while (the more trees the lower the frequency). If greater\\n 1 then it prints progress and performance for every tree.\\nes must be in the range `[0, inf)`.\\n\\n_nodes : int, default=None\\n trees with ``max_leaf_nodes`` in best-first fashion.\\n nodes are defined as relative reduction in impurity.\\nes must be in the range `[2, inf)`.\\nNone`, then unlimited number of leaf nodes.\\n\\nrt : bool, default=False\\n set to ``True``, reuse the solution of the previous call to fit\\nadd more estimators to the ensemble, otherwise, just erase the\\nious solution. See :term:`the Glossary <warm_start>`.\\n\\non_fraction : float, default=0.1\\nproportion of training data to set aside as validation set for\\ny stopping. Values must be in the range `(0.0, 1.0)`.\\n used if ``n_iter_no_change`` is set to an integer.\\n\\nersionadded:: 0.20\\n\\no_change : int, default=None\\niter_no_change`` is used to decide if early stopping will be used\\nerminate training when validation score is not improving. By\\nult it is set to None to disable early stopping. If set to a\\ner, it will set aside ``validation_fraction`` size of the training\\n as validation and terminate training when validation score is not\\noving in all of the previous ``n_iter_no_change`` numbers of\\nations. The split is stratified.\\nes must be in the range `[1, inf)`.\\n\\nersionadded:: 0.20\\n\\noat, default=1e-4\\nrance for the early stopping. When the loss is not improving\\nt least tol for ``n_iter_no_change`` iterations (if set to a\\ner), the training stops.\\nes must be in the range `(0.0, inf)`.\\n\\nersionadded:: 0.20\\n\\na : non-negative float, default=0.0\\nlexity parameter used for Minimal Cost-Complexity Pruning. The\\nree with the largest cost complexity that is smaller than\\np_alpha`` will be chosen. By default, no pruning is performed.\\nes must be in the range `[0.0, inf)`.\\n:ref:`minimal_cost_complexity_pruning` for details.\\n\\nersionadded:: 0.22\\n\\nes\\n--\\ntors_ : int\\nnumber of estimators as selected by early stopping (if\\niter_no_change`` is specified). Otherwise it is set to\\nestimators``.\\n\\nersionadded:: 0.20\\n\\nimportances_ : ndarray of shape (n_features,)\\nimpurity-based feature importances.\\nhigher, the more important the feature.\\nimportance of a feature is computed as the (normalized)\\nl reduction of the criterion brought by that feature.  It is also\\nn as the Gini importance.\\n\\ning: impurity-based feature importances can be misleading for\\n cardinality features (many unique values). See\\nc:`sklearn.inspection.permutation_importance` as an alternative.\\n\\novement_ : ndarray of shape (n_estimators,)\\nimprovement in loss (= deviance) on the out-of-bag samples\\ntive to the previous iteration.\\nb_improvement_[0]`` is the improvement in\\n of the first stage over the ``init`` estimator.\\n available if ``subsample < 1.0``\\n\\nore_ : ndarray of shape (n_estimators,)\\ni-th score ``train_score_[i]`` is the deviance (= loss) of the\\nl at iteration ``i`` on the in-bag sample.\\n`subsample == 1`` this is the deviance on the training data.\\n\\nLossFunction\\nconcrete ``LossFunction`` object.\\n\\neprecated:: 1.1\\n Attribute `loss_` was deprecated in version 1.1 and will be\\nremoved in 1.3.\\n\\nestimator\\nestimator that provides the initial predictions.\\nvia the ``init`` argument or ``loss.init_estimator``.\\n\\nrs_ : ndarray of DecisionTreeRegressor of             shape (n_estimators, ``loss_.K``)\\ncollection of fitted sub-estimators. ``loss_.K`` is 1 for binary\\nsification, otherwise n_classes.\\n\\n : ndarray of shape (n_classes,)\\nclasses labels.\\n\\nes_ : int\\nnumber of data features.\\n\\neprecated:: 1.0\\nAttribute `n_features_` was deprecated in version 1.0 and will be\\nremoved in 1.2. Use `n_features_in_` instead.\\n\\nes_in_ : int\\ner of features seen during :term:`fit`.\\n\\nersionadded:: 0.24\\n\\nnames_in_ : ndarray of shape (`n_features_in_`,)\\ns of features seen during :term:`fit`. Defined only when `X`\\nfeature names that are all strings.\\n\\nersionadded:: 1.0\\n\\ns_ : int\\nnumber of classes.\\n\\nures_ : int\\ninferred value of max_features.\\n\\n\\n\\nientBoostingClassifier : Histogram-based Gradient Boosting\\nsification Tree.\\ntree.DecisionTreeClassifier : A decision tree classifier.\\nrestClassifier : A meta-estimator that fits a number of decision\\n classifiers on various sub-samples of the dataset and uses\\naging to improve the predictive accuracy and control over-fitting.\\nClassifier : A meta-estimator that begins by fitting a classifier\\nhe original dataset and then fits additional copies of the\\nsifier on the same dataset where the weights of incorrectly\\nsified instances are adjusted such that subsequent classifiers\\ns more on difficult cases.\\n\\n\\n\\nures are always randomly permuted at each split. Therefore,\\n found split may vary, even with the same training data and\\natures=n_features``, if the improvement of the criterion is\\nl for several splits enumerated during the search of the best\\no obtain a deterministic behaviour during fitting,\\n_state`` has to be fixed.\\n\\nes\\n--\\nman, Greedy Function Approximation: A Gradient Boosting\\n The Annals of Statistics, Vol. 29, No. 5, 2001.\\n\\nman, Stochastic Gradient Boosting, 1999\\n\\ne, R. Tibshirani and J. Friedman.\\n of Statistical Learning Ed. 2, Springer, 2009.\\n\\n\\n\\nowing example shows how to fit a gradient boosting classifier with\\nsion stumps as weak learners.\\n\\n sklearn.datasets import make_hastie_10_2\\n sklearn.ensemble import GradientBoostingClassifier\\n\\n = make_hastie_10_2(random_state=0)\\nain, X_test = X[:2000], X[2000:]\\nain, y_test = y[:2000], y[2000:]\\n\\n= GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\\nmax_depth=1, random_state=0).fit(X_train, y_train)\\nscore(X_test, y_test)\\n\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.classification/hist-gradient-boosting-classifier\"} \":sklearn.classification/hist-gradient-boosting-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [18 2]:\\n\\n|                 :name |  :default |\\n|-----------------------|-----------|\\n|     :n-iter-no-change |     10.00 |\\n|        :learning-rate |    0.1000 |\\n|       :max-leaf-nodes |     31.00 |\\n|              :scoring |      loss |\\n|                  :tol | 1.000E-07 |\\n|       :early-stopping |      auto |\\n|             :max-iter |       100 |\\n|         :random-state |           |\\n|             :max-bins |       255 |\\n|     :min-samples-leaf |        20 |\\n|        :monotonic-cst |           |\\n|           :warm-start |     false |\\n|            :max-depth |           |\\n|  :validation-fraction |    0.1000 |\\n|                 :loss |  log_loss |\\n|              :verbose |         0 |\\n| :categorical-features |           |\\n|   :l-2-regularization |     0.000 |\\n\"]]] [:span [:p/markdown \"Histogram-based Gradient Boosting Classification Tree.\\n\\n    This estimator is much faster than\\n    `GradientBoostingClassifier<sklearn.ensemble.GradientBoostingClassifier>`\\n    for big datasets (n_samples >= 10 000).\\n\\n    This estimator has native support for missing values (NaNs). During\\n    training, the tree grower learns at each split point whether samples\\n    with missing values should go to the left or right child, based on the\\n    potential gain. When predicting, samples with missing values are\\n    assigned to the left or right child consequently. If no missing values\\n    were encountered for a given feature during training, then samples with\\n    missing values are mapped to whichever child has the most samples.\\n\\n    This implementation is inspired by\\n    [LightGBM ](https://github.com/Microsoft/LightGBM).\\n\\n    Read more in the User Guide: `histogram_based_gradient_boosting`.\\n\\n    *Added in 0.21*\\n\\n    Parameters\\n    ----------\\n    - `loss`: {'log_loss', 'auto', 'binary_crossentropy', 'categorical_crossentropy'},             default='log_loss'\\n        The loss function to use in the boosting process.\\n\\n        For binary classification problems, 'log_loss' is also known as logistic loss,\\n        binomial deviance or binary crossentropy. Internally, the model fits one tree\\n        per boosting iteration and uses the logistic sigmoid function (expit) as\\n        inverse link function to compute the predicted positive class probability.\\n\\n        For multiclass classification problems, 'log_loss' is also known as multinomial\\n        deviance or categorical crossentropy. Internally, the model fits one tree per\\n        boosting iteration and per class and uses the softmax function as inverse link\\n        function to compute the predicted probabilities of the classes.\\n\\n        *Deprecated since 1.1*\\n            The loss arguments 'auto', 'binary_crossentropy' and\\n            'categorical_crossentropy' were deprecated in v1.1 and will be removed in\\n            version 1.3. Use `loss='log_loss'` which is equivalent.\\n\\n    - `learning_rate`: float, default=0.1\\n        The learning rate, also known as *shrinkage*. This is used as a\\n        multiplicative factor for the leaves values. Use ``1`` for no\\n        shrinkage.\\n    - `max_iter`: int, default=100\\n        The maximum number of iterations of the boosting process, i.e. the\\n        maximum number of trees for binary classification. For multiclass\\n        classification, `n_classes` trees per iteration are built.\\n    - `max_leaf_nodes`: int or None, default=31\\n        The maximum number of leaves for each tree. Must be strictly greater\\n        than 1. If None, there is no maximum limit.\\n    - `max_depth`: int or None, default=None\\n        The maximum depth of each tree. The depth of a tree is the number of\\n        edges to go from the root to the deepest leaf.\\n        Depth isn't constrained by default.\\n    - `min_samples_leaf`: int, default=20\\n        The minimum number of samples per leaf. For small datasets with less\\n        than a few hundred samples, it is recommended to lower this value\\n        since only very shallow trees would be built.\\n    - `l2_regularization`: float, default=0\\n        The L2 regularization parameter. Use 0 for no regularization.\\n    - `max_bins`: int, default=255\\n        The maximum number of bins to use for non-missing values. Before\\n        training, each feature of the input array `X` is binned into\\n        integer-valued bins, which allows for a much faster training stage.\\n        Features with a small number of unique values may use less than\\n        ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin\\n        is always reserved for missing values. Must be no larger than 255.\\n    - `categorical_features`: array-like of {bool, int} of shape (n_features)             or shape (n_categorical_features,), default=None\\n        Indicates the categorical features.\\n\\n        - None : no feature will be considered categorical.\\n        - boolean array-like : boolean mask indicating categorical features.\\n        - integer array-like : integer indices indicating categorical\\n          features.\\n\\n        For each categorical feature, there must be at most `max_bins` unique\\n        categories, and each categorical value must be in [0, max_bins -1].\\n\\n        Read more in the User Guide: `categorical_support_gbdt`.\\n\\n        *Added in 0.24*\\n\\n    - `monotonic_cst`: array-like of int of shape (n_features), default=None\\n        Indicates the monotonic constraint to enforce on each feature. -1, 1\\n        and 0 respectively correspond to a negative constraint, positive\\n        constraint and no constraint. Read more in the User Guide: `monotonic_cst_gbdt`.\\n\\n        *Added in 0.23*\\n\\n    - `warm_start`: bool, default=False\\n        When set to ``True``, reuse the solution of the previous call to fit\\n        and add more estimators to the ensemble. For results to be valid, the\\n        estimator should be re-trained on the same data only.\\n        See `the Glossary <warm_start>`.\\n    - `early_stopping`: 'auto' or bool, default='auto'\\n        If 'auto', early stopping is enabled if the sample size is larger than\\n        10000. If True, early stopping is enabled, otherwise early stopping is\\n        disabled.\\n\\n        *Added in 0.23*\\n\\n    - `scoring`: str or callable or None, default='loss'\\n        Scoring parameter to use for early stopping. It can be a single\\n        string (see :ref:`scoring_parameter`) or a callable (see\\n        :ref:`scoring`). If None, the estimator's default scorer\\n        is used. If ``scoring='loss'``, early stopping is checked\\n        w.r.t the loss value. Only used if early stopping is performed.\\n    - `validation_fraction`: int or float or None, default=0.1\\n        Proportion (or absolute size) of training data to set aside as\\n        validation data for early stopping. If None, early stopping is done on\\n        the training data. Only used if early stopping is performed.\\n    - `n_iter_no_change`: int, default=10\\n        Used to determine when to \\\"early stop\\\". The fitting process is\\n        stopped when none of the last ``n_iter_no_change`` scores are better\\n        than the ``n_iter_no_change - 1`` -th-to-last one, up to some\\n        tolerance. Only used if early stopping is performed.\\n    - `tol`: float, default=1e-7\\n        The absolute tolerance to use when comparing scores. The higher the\\n        tolerance, the more likely we are to early stop: higher tolerance\\n        means that it will be harder for subsequent iterations to be\\n        considered an improvement upon the reference score.\\n    - `verbose`: int, default=0\\n        The verbosity level. If not zero, print some information about the\\n        fitting process.\\n    - `random_state`: int, RandomState instance or None, default=None\\n        Pseudo-random number generator to control the subsampling in the\\n        binning process, and the train/validation data split if early stopping\\n        is enabled.\\n        Pass an int for reproducible output across multiple function calls.\\n        See `Glossary <random_state>`.\\n\\n    Attributes\\n    ----------\\n    - `classes_`: array, shape = (n_classes,)\\n        Class labels.\\n    - `do_early_stopping_`: bool\\n        Indicates whether early stopping is used during training.\\n    - `n_iter_`: int\\n        The number of iterations as selected by early stopping, depending on\\n        the `early_stopping` parameter. Otherwise it corresponds to max_iter.\\n    - `n_trees_per_iteration_`: int\\n        The number of tree that are built at each iteration. This is equal to 1\\n        for binary classification, and to ``n_classes`` for multiclass\\n        classification.\\n    - `train_score_`: ndarray, shape (n_iter_+1,)\\n        The scores at each iteration on the training data. The first entry\\n        is the score of the ensemble before the first iteration. Scores are\\n        computed according to the ``scoring`` parameter. If ``scoring`` is\\n        not 'loss', scores are computed on a subset of at most 10 000\\n        samples. Empty if no early stopping.\\n    - `validation_score_`: ndarray, shape (n_iter_+1,)\\n        The scores at each iteration on the held-out validation data. The\\n        first entry is the score of the ensemble before the first iteration.\\n        Scores are computed according to the ``scoring`` parameter. Empty if\\n        no early stopping or if ``validation_fraction`` is None.\\n    - `is_categorical_`: ndarray, shape (n_features, ) or None\\n        Boolean mask for the categorical features. ``None`` if there are no\\n        categorical features.\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    See Also\\n    --------\\n    - `GradientBoostingClassifier`: Exact gradient boosting method that does not\\n        scale as good on datasets with a large number of samples.\\n    - `sklearn.tree.DecisionTreeClassifier`: A decision tree classifier.\\n    - `RandomForestClassifier`: A meta-estimator that fits a number of decision\\n        tree classifiers on various sub-samples of the dataset and uses\\n        averaging to improve the predictive accuracy and control over-fitting.\\n    - `AdaBoostClassifier`: A meta-estimator that begins by fitting a classifier\\n        on the original dataset and then fits additional copies of the\\n        classifier on the same dataset where the weights of incorrectly\\n        classified instances are adjusted such that subsequent classifiers\\n        focus more on difficult cases.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.ensemble import HistGradientBoostingClassifier\\n    >>> from sklearn.datasets import load_iris\\n    >>> X, y = load_iris(return_X_y=True)\\n    >>> clf = HistGradientBoostingClassifier().fit(X, y)\\n    >>> clf.score(X, y)\\n    1.0\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.classification/k-neighbors-classifier\"} \":sklearn.classification/k-neighbors-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [8 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|     :algorithm |      auto |\\n|     :leaf-size |        30 |\\n|        :metric | minkowski |\\n| :metric-params |           |\\n|        :n-jobs |           |\\n|   :n-neighbors |         5 |\\n|             :p |         2 |\\n|       :weights |   uniform |\\n\"]]] [:span [:p/markdown \"Classifier implementing the k-nearest neighbors vote.\\n\\n    Read more in the User Guide: `classification`.\\n\\n    Parameters\\n    ----------\\n    - `n_neighbors`: int, default=5\\n        Number of neighbors to use by default for `kneighbors` queries.\\n\\n    - `weights`: {'uniform', 'distance'} or callable, default='uniform'\\n        Weight function used in prediction.  Possible values:\\n\\n        - 'uniform' : uniform weights.  All points in each neighborhood\\n          are weighted equally.\\n        - 'distance' : weight points by the inverse of their distance.\\n          in this case, closer neighbors of a query point will have a\\n          greater influence than neighbors which are further away.\\n        - [callable] : a user-defined function which accepts an\\n          array of distances, and returns an array of the same shape\\n          containing the weights.\\n\\n    - `algorithm`: {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\\n        Algorithm used to compute the nearest neighbors:\\n\\n        - 'ball_tree' will use `BallTree`\\n        - 'kd_tree' will use `KDTree`\\n        - 'brute' will use a brute-force search.\\n        - 'auto' will attempt to decide the most appropriate algorithm\\n          based on the values passed to `fit` method.\\n\\n        Note: fitting on sparse input will override the setting of\\n        this parameter, using brute force.\\n\\n    - `leaf_size`: int, default=30\\n        Leaf size passed to BallTree or KDTree.  This can affect the\\n        speed of the construction and query, as well as the memory\\n        required to store the tree.  The optimal value depends on the\\n        nature of the problem.\\n\\n    - `p`: int, default=2\\n        Power parameter for the Minkowski metric. When p = 1, this is\\n        equivalent to using manhattan_distance (l1), and euclidean_distance\\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\\n\\n    - `metric`: str or callable, default='minkowski'\\n        The distance metric to use for the tree.  The default metric is\\n        minkowski, and with p=2 is equivalent to the standard Euclidean\\n        metric. For a list of available metrics, see the documentation of\\n        `~sklearn.metrics.DistanceMetric` and the metrics listed in\\n        `sklearn.metrics.pairwise.PAIRWISE_DISTANCE_FUNCTIONS`. Note that the\\n        \\\"cosine\\\" metric uses `~sklearn.metrics.pairwise.cosine_distances`.\\n        If metric is \\\"precomputed\\\", X is assumed to be a distance matrix and\\n        must be square during fit. X may be a `sparse graph`,\\n        in which case only \\\"nonzero\\\" elements may be considered neighbors.\\n\\n    - `metric_params`: dict, default=None\\n        Additional keyword arguments for the metric function.\\n\\n    - `n_jobs`: int, default=None\\n        The number of parallel jobs to run for neighbors search.\\n        ``None`` means 1 unless in a `joblib.parallel_backend` context.\\n        ``-1`` means using all processors. See `Glossary <n_jobs>`\\n        for more details.\\n        Doesn't affect `fit` method.\\n\\n    Attributes\\n    ----------\\n    - `classes_`: array of shape (n_classes,)\\n        Class labels known to the classifier\\n\\n    - `effective_metric_`: str or callble\\n        The distance metric used. It will be same as the `metric` parameter\\n        or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\\n        'minkowski' and `p` parameter set to 2.\\n\\n    - `effective_metric_params_`: dict\\n        Additional keyword arguments for the metric function. For most metrics\\n        will be same with `metric_params` parameter, but may also contain the\\n        `p` parameter value if the `effective_metric_` attribute is set to\\n        'minkowski'.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    - `n_samples_fit_`: int\\n        Number of samples in the fitted data.\\n\\n    - `outputs_2d_`: bool\\n        False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit\\n        otherwise True.\\n\\n    See Also\\n    --------\\n    RadiusNeighborsClassifier: Classifier based on neighbors within a fixed radius.\\n    KNeighborsRegressor: Regression based on k-nearest neighbors.\\n    RadiusNeighborsRegressor: Regression based on neighbors within a fixed radius.\\n    NearestNeighbors: Unsupervised learner for implementing neighbor searches.\\n\\n    Notes\\n    -----\\n    See Nearest Neighbors: `neighbors` in the online documentation\\n    for a discussion of the choice of ``algorithm`` and ``leaf_size``.\\n\\n\\n---\\n**Warning**\\n\\nRegarding the Nearest Neighbors algorithms, if it is found that two\\nneighbors, neighbor `k+1` and `k`, have identical distances\\nbut different labels, the results will depend on the ordering of the\\ntraining data.\\n\\nps://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\\n\\nmples\\n-----\\n X = [[0], [1], [2], [3]]\\n y = [0, 0, 1, 1]\\n from sklearn.neighbors import KNeighborsClassifier\\n neigh = KNeighborsClassifier(n_neighbors=3)\\n neigh.fit(X, y)\\nighborsClassifier(...)\\n print(neigh.predict([[1.1]]))\\n\\n print(neigh.predict_proba([[0.9]]))\\n.666... 0.333...]]\\n\\n---\\n\"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.classification/label-propagation\"} \":sklearn.classification/label-propagation\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [6 2]:\\n\\n|        :name | :default |\\n|--------------|----------|\\n|       :gamma |       20 |\\n|      :kernel |      rbf |\\n|    :max-iter |     1000 |\\n|      :n-jobs |          |\\n| :n-neighbors |        7 |\\n|         :tol | 0.001000 |\\n\"]]] [:span [:p/markdown \"Label Propagation classifier.\\n\\n    Read more in the User Guide: `label_propagation`.\\n\\n    Parameters\\n    ----------\\n    - `kernel`: {'knn', 'rbf'} or callable, default='rbf'\\n        String identifier for kernel function to use or the kernel function\\n        itself. Only 'rbf' and 'knn' strings are valid inputs. The function\\n        passed should take two inputs, each of shape (n_samples, n_features),\\n        and return a (n_samples, n_samples) shaped weight matrix.\\n\\n    - `gamma`: float, default=20\\n        Parameter for rbf kernel.\\n\\n    - `n_neighbors`: int, default=7\\n        Parameter for knn kernel which need to be strictly positive.\\n\\n    - `max_iter`: int, default=1000\\n        Change maximum number of iterations allowed.\\n\\n    - `tol`: float, 1e-3\\n        Convergence tolerance: threshold to consider the system at steady\\n        state.\\n\\n    - `n_jobs`: int, default=None\\n        The number of parallel jobs to run.\\n        ``None`` means 1 unless in a `joblib.parallel_backend` context.\\n        ``-1`` means using all processors. See `Glossary <n_jobs>`\\n        for more details.\\n\\n    Attributes\\n    ----------\\n    - `X_`: ndarray of shape (n_samples, n_features)\\n        Input array.\\n\\n    - `classes_`: ndarray of shape (n_classes,)\\n        The distinct labels used in classifying instances.\\n\\n    - `label_distributions_`: ndarray of shape (n_samples, n_classes)\\n        Categorical distribution for each item.\\n\\n    - `transduction_`: ndarray of shape (n_samples)\\n        Label assigned to each item via the transduction.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    - `n_iter_`: int\\n        Number of iterations run.\\n\\n    See Also\\n    --------\\n    - `BaseLabelPropagation`: Base class for label propagation module.\\n    - `LabelSpreading`: Alternate label propagation strategy more robust to noise.\\n\\n    References\\n    ----------\\n    Xiaojin Zhu and Zoubin Ghahramani. Learning from labeled and unlabeled data\\n    with label propagation. Technical Report CMU-CALD-02-107, Carnegie Mellon\\n    University, 2002 http://pages.cs.wisc.edu/~jerryzhu/pub/CMU-CALD-02-107.pdf\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> from sklearn import datasets\\n    >>> from sklearn.semi_supervised import LabelPropagation\\n    >>> label_prop_model = LabelPropagation()\\n    >>> iris = datasets.load_iris()\\n    >>> rng = np.random.RandomState(42)\\n    >>> random_unlabeled_points = rng.rand(len(iris.target)) < 0.3\\n    >>> labels = np.copy(iris.target)\\n    >>> labels[random_unlabeled_points] = -1\\n    >>> label_prop_model.fit(iris.data, labels)\\n    LabelPropagation(...)\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.classification/label-spreading\"} \":sklearn.classification/label-spreading\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [7 2]:\\n\\n|        :name | :default |\\n|--------------|----------|\\n|       :alpha |   0.2000 |\\n|       :gamma |    20.00 |\\n|      :kernel |      rbf |\\n|    :max-iter |       30 |\\n|      :n-jobs |          |\\n| :n-neighbors |        7 |\\n|         :tol | 0.001000 |\\n\"]]] [:span [:p/markdown \"LabelSpreading model for semi-supervised learning.\\n\\n    This model is similar to the basic Label Propagation algorithm,\\n    but uses affinity matrix based on the normalized graph Laplacian\\n    and soft clamping across the labels.\\n\\n    Read more in the User Guide: `label_propagation`.\\n\\n    Parameters\\n    ----------\\n    - `kernel`: {'knn', 'rbf'} or callable, default='rbf'\\n        String identifier for kernel function to use or the kernel function\\n        itself. Only 'rbf' and 'knn' strings are valid inputs. The function\\n        passed should take two inputs, each of shape (n_samples, n_features),\\n        and return a (n_samples, n_samples) shaped weight matrix.\\n\\n    - `gamma`: float, default=20\\n      Parameter for rbf kernel.\\n\\n    - `n_neighbors`: int, default=7\\n      Parameter for knn kernel which is a strictly positive integer.\\n\\n    - `alpha`: float, default=0.2\\n      Clamping factor. A value in (0, 1) that specifies the relative amount\\n      that an instance should adopt the information from its neighbors as\\n      opposed to its initial label.\\n      alpha=0 means keeping the initial label information; alpha=1 means\\n      replacing all initial information.\\n\\n    - `max_iter`: int, default=30\\n      Maximum number of iterations allowed.\\n\\n    - `tol`: float, default=1e-3\\n      Convergence tolerance: threshold to consider the system at steady\\n      state.\\n\\n    - `n_jobs`: int, default=None\\n        The number of parallel jobs to run.\\n        ``None`` means 1 unless in a `joblib.parallel_backend` context.\\n        ``-1`` means using all processors. See `Glossary <n_jobs>`\\n        for more details.\\n\\n    Attributes\\n    ----------\\n    - `X_`: ndarray of shape (n_samples, n_features)\\n        Input array.\\n\\n    - `classes_`: ndarray of shape (n_classes,)\\n        The distinct labels used in classifying instances.\\n\\n    - `label_distributions_`: ndarray of shape (n_samples, n_classes)\\n        Categorical distribution for each item.\\n\\n    - `transduction_`: ndarray of shape (n_samples,)\\n        Label assigned to each item via the transduction.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    - `n_iter_`: int\\n        Number of iterations run.\\n\\n    See Also\\n    --------\\n    - `LabelPropagation`: Unregularized graph based semi-supervised learning.\\n\\n    References\\n    ----------\\n    Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston,\\n    Bernhard Schoelkopf. Learning with local and global consistency (2004)\\n    http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.115.3219\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> from sklearn import datasets\\n    >>> from sklearn.semi_supervised import LabelSpreading\\n    >>> label_prop_model = LabelSpreading()\\n    >>> iris = datasets.load_iris()\\n    >>> rng = np.random.RandomState(42)\\n    >>> random_unlabeled_points = rng.rand(len(iris.target)) < 0.3\\n    >>> labels = np.copy(iris.target)\\n    >>> labels[random_unlabeled_points] = -1\\n    >>> label_prop_model.fit(iris.data, labels)\\n    LabelSpreading(...)\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.classification/linear-discriminant-analysis\"} \":sklearn.classification/linear-discriminant-analysis\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [7 2]:\\n\\n|                 :name |  :default |\\n|-----------------------|-----------|\\n| :covariance-estimator |           |\\n|         :n-components |           |\\n|               :priors |           |\\n|            :shrinkage |           |\\n|               :solver |       svd |\\n|     :store-covariance |     false |\\n|                  :tol | 0.0001000 |\\n\"]]] [:span [:p/markdown \"Linear Discriminant Analysis.\\n\\n    A classifier with a linear decision boundary, generated by fitting class\\n    conditional densities to the data and using Bayes' rule.\\n\\n    The model fits a Gaussian density to each class, assuming that all classes\\n    share the same covariance matrix.\\n\\n    The fitted model can also be used to reduce the dimensionality of the input\\n    by projecting it to the most discriminative directions, using the\\n    `transform` method.\\n\\n    *Added in 0.17*\\n       *LinearDiscriminantAnalysis*.\\n\\n    Read more in the User Guide: `lda_qda`.\\n\\n    Parameters\\n    ----------\\n    - `solver`: {'svd', 'lsqr', 'eigen'}, default='svd'\\n        Solver to use, possible values:\\n          - 'svd': Singular value decomposition (default).\\n            Does not compute the covariance matrix, therefore this solver is\\n            recommended for data with a large number of features.\\n          - 'lsqr': Least squares solution.\\n            Can be combined with shrinkage or custom covariance estimator.\\n          - 'eigen': Eigenvalue decomposition.\\n            Can be combined with shrinkage or custom covariance estimator.\\n\\n    - `shrinkage`: 'auto' or float, default=None\\n        Shrinkage parameter, possible values:\\n          - None: no shrinkage (default).\\n          - 'auto': automatic shrinkage using the Ledoit-Wolf lemma.\\n          - float between 0 and 1: fixed shrinkage parameter.\\n\\n        This should be left to None if `covariance_estimator` is used.\\n        Note that shrinkage works only with 'lsqr' and 'eigen' solvers.\\n\\n    - `priors`: array-like of shape (n_classes,), default=None\\n        The class prior probabilities. By default, the class proportions are\\n        inferred from the training data.\\n\\n    - `n_components`: int, default=None\\n        Number of components (<= min(n_classes - 1, n_features)) for\\n        dimensionality reduction. If None, will be set to\\n        min(n_classes - 1, n_features). This parameter only affects the\\n        `transform` method.\\n\\n    - `store_covariance`: bool, default=False\\n        If True, explicitly compute the weighted within-class covariance\\n        matrix when solver is 'svd'. The matrix is always computed\\n        and stored for the other solvers.\\n\\n        *Added in 0.17*\\n\\n    - `tol`: float, default=1.0e-4\\n        Absolute threshold for a singular value of X to be considered\\n        significant, used to estimate the rank of X. Dimensions whose\\n        singular values are non-significant are discarded. Only used if\\n        solver is 'svd'.\\n\\n        *Added in 0.17*\\n\\n    - `covariance_estimator`: covariance estimator, default=None\\n        If not None, `covariance_estimator` is used to estimate\\n        the covariance matrices instead of relying on the empirical\\n        covariance estimator (with potential shrinkage).\\n        The object should have a fit method and a ``covariance_`` attribute\\n        like the estimators in `sklearn.covariance`.\\n        if None the shrinkage parameter drives the estimate.\\n\\n        This should be left to None if `shrinkage` is used.\\n        Note that `covariance_estimator` works only with 'lsqr' and 'eigen'\\n        solvers.\\n\\n        *Added in 0.24*\\n\\n    Attributes\\n    ----------\\n    - `coef_`: ndarray of shape (n_features,) or (n_classes, n_features)\\n        Weight vector(s).\\n\\n    - `intercept_`: ndarray of shape (n_classes,)\\n        Intercept term.\\n\\n    - `covariance_`: array-like of shape (n_features, n_features)\\n        Weighted within-class covariance matrix. It corresponds to\\n        `sum_k prior_k * C_k` where `C_k` is the covariance matrix of the\\n        samples in class `k`. The `C_k` are estimated using the (potentially\\n        shrunk) biased estimator of covariance. If solver is 'svd', only\\n        exists when `store_covariance` is True.\\n\\n    - `explained_variance_ratio_`: ndarray of shape (n_components,)\\n        Percentage of variance explained by each of the selected components.\\n        If ``n_components`` is not set then all components are stored and the\\n        sum of explained variances is equal to 1.0. Only available when eigen\\n        or svd solver is used.\\n\\n    - `means_`: array-like of shape (n_classes, n_features)\\n        Class-wise means.\\n\\n    - `priors_`: array-like of shape (n_classes,)\\n        Class priors (sum to 1).\\n\\n    - `scalings_`: array-like of shape (rank, n_classes - 1)\\n        Scaling of the features in the space spanned by the class centroids.\\n        Only available for 'svd' and 'eigen' solvers.\\n\\n    - `xbar_`: array-like of shape (n_features,)\\n        Overall mean. Only present if solver is 'svd'.\\n\\n    - `classes_`: array-like of shape (n_classes,)\\n        Unique class labels.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    See Also\\n    --------\\n    - `QuadraticDiscriminantAnalysis`: Quadratic Discriminant Analysis.\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\\n    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\\n    >>> y = np.array([1, 1, 1, 2, 2, 2])\\n    >>> clf = LinearDiscriminantAnalysis()\\n    >>> clf.fit(X, y)\\n    LinearDiscriminantAnalysis()\\n    >>> print(clf.predict([[-0.8, -1]]))\\n    [1]\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.classification/linear-svc\"} \":sklearn.classification/linear-svc\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [12 2]:\\n\\n|              :name |      :default |\\n|--------------------|---------------|\\n|               :tol |     0.0001000 |\\n| :intercept-scaling |         1.000 |\\n|       :multi-class |           ovr |\\n|           :penalty |            l2 |\\n|                 :c |         1.000 |\\n|          :max-iter |          1000 |\\n|      :random-state |               |\\n|              :dual |          true |\\n|     :fit-intercept |          true |\\n|      :class-weight |               |\\n|              :loss | squared_hinge |\\n|           :verbose |             0 |\\n\"]]] [:span [:p/markdown \"Linear Support Vector Classification.\\n\\n    Similar to SVC with parameter kernel='linear', but implemented in terms of\\n    liblinear rather than libsvm, so it has more flexibility in the choice of\\n    penalties and loss functions and should scale better to large numbers of\\n    samples.\\n\\n    This class supports both dense and sparse input and the multiclass support\\n    is handled according to a one-vs-the-rest scheme.\\n\\n    Read more in the User Guide: `svm_classification`.\\n\\n    Parameters\\n    ----------\\n    - `penalty`: {'l1', 'l2'}, default='l2'\\n        Specifies the norm used in the penalization. The 'l2'\\n        penalty is the standard used in SVC. The 'l1' leads to ``coef_``\\n        vectors that are sparse.\\n\\n    - `loss`: {'hinge', 'squared_hinge'}, default='squared_hinge'\\n        Specifies the loss function. 'hinge' is the standard SVM loss\\n        (used e.g. by the SVC class) while 'squared_hinge' is the\\n        square of the hinge loss. The combination of ``penalty='l1'``\\n        and ``loss='hinge'`` is not supported.\\n\\n    - `dual`: bool, default=True\\n        Select the algorithm to either solve the dual or primal\\n        optimization problem. Prefer dual=False when n_samples > n_features.\\n\\n    - `tol`: float, default=1e-4\\n        Tolerance for stopping criteria.\\n\\n    - `C`: float, default=1.0\\n        Regularization parameter. The strength of the regularization is\\n        inversely proportional to C. Must be strictly positive.\\n\\n    - `multi_class`: {'ovr', 'crammer_singer'}, default='ovr'\\n        Determines the multi-class strategy if `y` contains more than\\n        two classes.\\n        ``\\\"ovr\\\"`` trains n_classes one-vs-rest classifiers, while\\n        ``\\\"crammer_singer\\\"`` optimizes a joint objective over all classes.\\n        While `crammer_singer` is interesting from a theoretical perspective\\n        as it is consistent, it is seldom used in practice as it rarely leads\\n        to better accuracy and is more expensive to compute.\\n        If ``\\\"crammer_singer\\\"`` is chosen, the options loss, penalty and dual\\n        will be ignored.\\n\\n    - `fit_intercept`: bool, default=True\\n        Whether to calculate the intercept for this model. If set\\n        to false, no intercept will be used in calculations\\n        (i.e. data is expected to be already centered).\\n\\n    - `intercept_scaling`: float, default=1\\n        When self.fit_intercept is True, instance vector x becomes\\n        ``[x, self.intercept_scaling]``,\\n        i.e. a \\\"synthetic\\\" feature with constant value equals to\\n        intercept_scaling is appended to the instance vector.\\n        The intercept becomes intercept_scaling * synthetic feature weight\\n        Note! the synthetic feature weight is subject to l1/l2 regularization\\n        as all other features.\\n        To lessen the effect of regularization on synthetic feature weight\\n        (and therefore on the intercept) intercept_scaling has to be increased.\\n\\n    - `class_weight`: dict or 'balanced', default=None\\n        Set the parameter C of class i to ``class_weight[i]*C`` for\\n        SVC. If not given, all classes are supposed to have\\n        weight one.\\n        The \\\"balanced\\\" mode uses the values of y to automatically adjust\\n        weights inversely proportional to class frequencies in the input data\\n        as ``n_samples / (n_classes * np.bincount(y))``.\\n\\n    - `verbose`: int, default=0\\n        Enable verbose output. Note that this setting takes advantage of a\\n        per-process runtime setting in liblinear that, if enabled, may not work\\n        properly in a multithreaded context.\\n\\n    - `random_state`: int, RandomState instance or None, default=None\\n        Controls the pseudo random number generation for shuffling the data for\\n        the dual coordinate descent (if ``dual=True``). When ``dual=False`` the\\n        underlying implementation of `LinearSVC` is not random and\\n        ``random_state`` has no effect on the results.\\n        Pass an int for reproducible output across multiple function calls.\\n        See `Glossary <random_state>`.\\n\\n    - `max_iter`: int, default=1000\\n        The maximum number of iterations to be run.\\n\\n    Attributes\\n    ----------\\n    - `coef_`: ndarray of shape (1, n_features) if n_classes == 2             else (n_classes, n_features)\\n        Weights assigned to the features (coefficients in the primal\\n        problem).\\n\\n        ``coef_`` is a readonly property derived from ``raw_coef_`` that\\n        follows the internal memory layout of liblinear.\\n\\n    - `intercept_`: ndarray of shape (1,) if n_classes == 2 else (n_classes,)\\n        Constants in decision function.\\n\\n    - `classes_`: ndarray of shape (n_classes,)\\n        The unique classes labels.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    - `n_iter_`: int\\n        Maximum number of iterations run across all classes.\\n\\n    See Also\\n    --------\\n    - `SVC`: Implementation of Support Vector Machine classifier using libsvm:\\n        the kernel can be non-linear but its SMO algorithm does not\\n        scale to large number of samples as LinearSVC does.\\n\\n        Furthermore SVC multi-class mode is implemented using one\\n        vs one scheme while LinearSVC uses one vs the rest. It is\\n        possible to implement one vs the rest with SVC by using the\\n        `~sklearn.multiclass.OneVsRestClassifier` wrapper.\\n\\n        Finally SVC can fit dense data without memory copy if the input\\n        is C-contiguous. Sparse data will still incur memory copy though.\\n\\n    - `sklearn.linear_model.SGDClassifier`: SGDClassifier can optimize the same\\n        cost function as LinearSVC\\n        by adjusting the penalty and loss parameters. In addition it requires\\n        less memory, allows incremental (online) learning, and implements\\n        various loss functions and regularization regimes.\\n\\n    Notes\\n    -----\\n    The underlying C implementation uses a random number generator to\\n    select features when fitting the model. It is thus not uncommon\\n    to have slightly different results for the same input data. If\\n    that happens, try with a smaller ``tol`` parameter.\\n\\n    The underlying implementation, liblinear, uses a sparse internal\\n    representation for the data that will incur a memory copy.\\n\\n    Predict output may not match that of standalone liblinear in certain\\n    cases. See differences from liblinear: `liblinear_differences`\\n    in the narrative documentation.\\n\\n    References\\n    ----------\\n    [LIBLINEAR: A Library for Large Linear Classification\\n    ](https://www.csie.ntu.edu.tw/~cjlin/liblinear/)\\n\\n    Examples\\n    --------\\n    >>> from sklearn.svm import LinearSVC\\n    >>> from sklearn.pipeline import make_pipeline\\n    >>> from sklearn.preprocessing import StandardScaler\\n    >>> from sklearn.datasets import make_classification\\n    >>> X, y = make_classification(n_features=4, random_state=0)\\n    >>> clf = make_pipeline(StandardScaler(),\\n    ...                     LinearSVC(random_state=0, tol=1e-5))\\n    >>> clf.fit(X, y)\\n    Pipeline(steps=[('standardscaler', StandardScaler()),\\n                    ('linearsvc', LinearSVC(random_state=0, tol=1e-05))])\\n\\n    >>> print(clf.named_steps['linearsvc'].coef_)\\n    [[0.141...   0.526... 0.679... 0.493...]]\\n\\n    >>> print(clf.named_steps['linearsvc'].intercept_)\\n    [0.1693...]\\n    >>> print(clf.predict([[0, 0, 0, 0]]))\\n    [1]\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.classification/logistic-regression\"} \":sklearn.classification/logistic-regression\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [15 2]:\\n\\n|              :name |  :default |\\n|--------------------|-----------|\\n|               :tol | 0.0001000 |\\n| :intercept-scaling |     1.000 |\\n|       :multi-class |      auto |\\n|            :solver |     lbfgs |\\n|           :penalty |        l2 |\\n|                 :c |     1.000 |\\n|          :max-iter |       100 |\\n|            :n-jobs |           |\\n|      :random-state |           |\\n|              :dual |     false |\\n|     :fit-intercept |      true |\\n|        :warm-start |     false |\\n|         :l-1-ratio |           |\\n|      :class-weight |           |\\n|           :verbose |         0 |\\n\"]]] [:span [:p/markdown \"\\n    Logistic Regression (aka logit, MaxEnt) classifier.\\n\\n    In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\\n    scheme if the 'multi_class' option is set to 'ovr', and uses the\\n    cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\\n    (Currently the 'multinomial' option is supported only by the 'lbfgs',\\n    'sag', 'saga' and 'newton-cg' solvers.)\\n\\n    This class implements regularized logistic regression using the\\n    'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\\n    that regularization is applied by default**. It can handle both dense\\n    and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\\n    floats for optimal performance; any other input format will be converted\\n    (and copied).\\n\\n    The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\\n    with primal formulation, or no regularization. The 'liblinear' solver\\n    supports both L1 and L2 regularization, with a dual formulation only for\\n    the L2 penalty. The Elastic-Net regularization is only supported by the\\n    'saga' solver.\\n\\n    Read more in the User Guide: `logistic_regression`.\\n\\n    Parameters\\n    ----------\\n    - `penalty`: {'l1', 'l2', 'elasticnet', 'none'}, default='l2'\\n        Specify the norm of the penalty:\\n\\n        - `'none'`: no penalty is added;\\n        - `'l2'`: add a L2 penalty term and it is the default choice;\\n        - `'l1'`: add a L1 penalty term;\\n        - `'elasticnet'`: both L1 and L2 penalty terms are added.\\n\\n\\n---\\n**Warning**\\n\\nSome penalties may not work with some solvers. See the parameter\\n`solver` below, to know the compatibility between the penalty and\\nsolver.\\n\\nversionadded:: 0.19\\nl1 penalty with SAGA solver (allowing 'multinomial' + L1)\\n\\nbool, default=False\\nl or primal formulation. Dual formulation is only implemented for\\npenalty with liblinear solver. Prefer dual=False when\\namples > n_features.\\n\\nloat, default=1e-4\\nerance for stopping criteria.\\n\\nat, default=1.0\\nerse of regularization strength; must be a positive float.\\ne in support vector machines, smaller values specify stronger\\nularization.\\n\\nercept : bool, default=True\\ncifies if a constant (a.k.a. bias or intercept) should be\\ned to the decision function.\\n\\npt_scaling : float, default=1\\nful only when the solver 'liblinear' is used\\n self.fit_intercept is set to True. In this case, x becomes\\n self.intercept_scaling],\\n. a \\\"synthetic\\\" feature with constant value equal to\\nercept_scaling is appended to the instance vector.\\n intercept becomes ``intercept_scaling * synthetic_feature_weight``.\\n\\ne! the synthetic feature weight is subject to l1/l2 regularization\\nall other features.\\nlessen the effect of regularization on synthetic feature weight\\nd therefore on the intercept) intercept_scaling has to be increased.\\n\\neight : dict or 'balanced', default=None\\nghts associated with classes in the form ``{class_label: weight}``.\\nnot given, all classes are supposed to have weight one.\\n\\n \\\"balanced\\\" mode uses the values of y to automatically adjust\\nghts inversely proportional to class frequencies in the input data\\n``n_samples / (n_classes * np.bincount(y))``.\\n\\ne that these weights will be multiplied with sample_weight (passed\\nough the fit method) if sample_weight is specified.\\n\\nversionadded:: 0.17\\n*class_weight='balanced'*\\n\\nstate : int, RandomState instance, default=None\\nd when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\\na. See :term:`Glossary <random_state>` for details.\\n\\n: {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},             default='lbfgs'\\n\\norithm to use in the optimization problem. Default is 'lbfgs'.\\nchoose a solver, you might want to consider the following aspects:\\n\\n - For small datasets, 'liblinear' is a good choice, whereas 'sag'\\n   and 'saga' are faster for large ones;\\n - For multiclass problems, only 'newton-cg', 'sag', 'saga' and\\n   'lbfgs' handle multinomial loss;\\n - 'liblinear' is limited to one-versus-rest schemes.\\n\\nwarning::\\nThe choice of the algorithm depends on the penalty chosen:\\nSupported penalties by solver:\\n\\n- 'newton-cg'   -   ['l2', 'none']\\n- 'lbfgs'       -   ['l2', 'none']\\n- 'liblinear'   -   ['l1', 'l2']\\n- 'sag'         -   ['l2', 'none']\\n- 'saga'        -   ['elasticnet', 'l1', 'l2', 'none']\\n\\nnote::\\n'sag' and 'saga' fast convergence is only guaranteed on\\nfeatures with approximately the same scale. You can\\npreprocess the data with a scaler from :mod:`sklearn.preprocessing`.\\n\\nseealso::\\nRefer to the User Guide for more information regarding\\n:class:`LogisticRegression` and more specifically the\\n`Table <https://scikit-learn.org/dev/modules/linear_model.html#logistic-regression>`_\\nsummarazing solver/penalty supports.\\n\\nversionadded:: 0.17\\nStochastic Average Gradient descent solver.\\nversionadded:: 0.19\\nSAGA solver.\\nversionchanged:: 0.22\\n The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\\n\\nr : int, default=100\\nimum number of iterations taken for the solvers to converge.\\n\\nlass : {'auto', 'ovr', 'multinomial'}, default='auto'\\nthe option chosen is 'ovr', then a binary problem is fit for each\\nel. For 'multinomial' the loss minimised is the multinomial loss fit\\noss the entire probability distribution, *even when the data is\\nary*. 'multinomial' is unavailable when solver='liblinear'.\\nto' selects 'ovr' if the data is binary, or if solver='liblinear',\\n otherwise selects 'multinomial'.\\n\\nversionadded:: 0.18\\nStochastic Average Gradient descent solver for 'multinomial' case.\\nversionchanged:: 0.22\\n Default changed from 'ovr' to 'auto' in 0.22.\\n\\n : int, default=0\\n the liblinear and lbfgs solvers set verbose to any positive\\nber for verbosity.\\n\\nart : bool, default=False\\nn set to True, reuse the solution of the previous call to fit as\\ntialization, otherwise, just erase the previous solution.\\nless for liblinear solver. See :term:`the Glossary <warm_start>`.\\n\\nversionadded:: 0.17\\n*warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\\n\\n: int, default=None\\nber of CPU cores used when parallelizing over classes if\\nti_class='ovr'\\\". This parameter is ignored when the ``solver`` is\\n to 'liblinear' regardless of whether 'multi_class' is specified or\\n. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\\ntext. ``-1`` means using all processors.\\n :term:`Glossary <n_jobs>` for more details.\\n\\no : float, default=None\\n Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\\nd if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\\nusing ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\\nusing ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\\nbination of L1 and L2.\\n\\ntes\\n---\\n\\n_ : ndarray of shape (n_classes, )\\nist of class labels known to the classifier.\\n\\n ndarray of shape (1, n_features) or (n_classes, n_features)\\nfficient of the features in the decision function.\\n\\nef_` is of shape (1, n_features) when the given problem is binary.\\nparticular, when `multi_class='multinomial'`, `coef_` corresponds\\noutcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\\n\\npt_ : ndarray of shape (1,) or (n_classes,)\\nercept (a.k.a. bias) added to the decision function.\\n\\n`fit_intercept` is set to False, the intercept is set to zero.\\ntercept_` is of shape (1,) when the given problem is binary.\\nparticular, when `multi_class='multinomial'`, `intercept_`\\nresponds to outcome 1 (True) and `-intercept_` corresponds to\\ncome 0 (False).\\n\\nres_in_ : int\\nber of features seen during :term:`fit`.\\n\\nversionadded:: 0.24\\n\\n_names_in_ : ndarray of shape (`n_features_in_`,)\\nes of features seen during :term:`fit`. Defined only when `X`\\n feature names that are all strings.\\n\\nversionadded:: 1.0\\n\\n : ndarray of shape (n_classes,) or (1, )\\nual number of iterations for all classes. If binary or multinomial,\\nreturns only 1 element. For liblinear solver, only the maximum\\nber of iteration across all classes is given.\\n\\nversionchanged:: 0.20\\n\\n In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\\n ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\\n\\no\\n-\\nsifier : Incrementally trained logistic regression (when given\\n parameter ``loss=\\\"log\\\"``).\\ncRegressionCV : Logistic regression with built-in cross validation.\\n\\n\\n\\nerlying C implementation uses a random number generator to\\nfeatures when fitting the model. It is thus not uncommon,\\n slightly different results for the same input data. If\\nppens, try with a smaller tol parameter.\\n\\n output may not match that of standalone liblinear in certain\\nSee :ref:`differences from liblinear <liblinear_differences>`\\nnarrative documentation.\\n\\nces\\n---\\n\\nB -- Software for Large-scale Bound-constrained Optimization\\nou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\\np://users.iems.northwestern.edu/~nocedal/lbfgsb.html\\n\\nAR -- A Library for Large Linear Classification\\nps://www.csie.ntu.edu.tw/~cjlin/liblinear/\\n\\nMark Schmidt, Nicolas Le Roux, and Francis Bach\\nimizing Finite Sums with the Stochastic Average Gradient\\nps://hal.inria.fr/hal-00860051/document\\n\\n Defazio, A., Bach F. & Lacoste-Julien S. (2014).\\n :arxiv:`\\\"SAGA: A Fast Incremental Gradient Method With Support\\n for Non-Strongly Convex Composite Objectives\\\" <1407.0202>`\\n\\nFu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\\nhods for logistic regression and maximum entropy models.\\nhine Learning 85(1-2):41-75.\\nps://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\\n\\ns\\n-\\nm sklearn.datasets import load_iris\\nm sklearn.linear_model import LogisticRegression\\ny = load_iris(return_X_y=True)\\n = LogisticRegression(random_state=0).fit(X, y)\\n.predict(X[:2, :])\\n0, 0])\\n.predict_proba(X[:2, :])\\n[9.8...e-01, 1.8...e-02, 1.4...e-08],\\n[9.7...e-01, 2.8...e-02, ...e-08]])\\n.score(X, y)\\n\\n\\n---\\n\"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.classification/logistic-regression-cv\"} \":sklearn.classification/logistic-regression-cv\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [17 2]:\\n\\n|              :name |  :default |\\n|--------------------|-----------|\\n|             :refit |      true |\\n|           :scoring |           |\\n|               :tol | 0.0001000 |\\n| :intercept-scaling |     1.000 |\\n|       :multi-class |      auto |\\n|            :solver |     lbfgs |\\n|           :penalty |        l2 |\\n|          :max-iter |       100 |\\n|            :n-jobs |           |\\n|      :random-state |           |\\n|              :dual |     false |\\n|     :fit-intercept |      true |\\n|                :cv |           |\\n|                :cs |        10 |\\n|      :class-weight |           |\\n|           :verbose |         0 |\\n|        :l-1-ratios |           |\\n\"]]] [:span [:p/markdown \"Logistic Regression CV (aka logit, MaxEnt) classifier.\\n\\n    See glossary entry for `cross-validation estimator`.\\n\\n    This class implements logistic regression using liblinear, newton-cg, sag\\n    of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2\\n    regularization with primal formulation. The liblinear solver supports both\\n    L1 and L2 regularization, with a dual formulation only for the L2 penalty.\\n    Elastic-Net penalty is only supported by the saga solver.\\n\\n    For the grid of `Cs` values and `l1_ratios` values, the best hyperparameter\\n    is selected by the cross-validator\\n    `~sklearn.model_selection.StratifiedKFold`, but it can be changed\\n    using the `cv` parameter. The 'newton-cg', 'sag', 'saga' and 'lbfgs'\\n    solvers can warm-start the coefficients (see `Glossary<warm_start>`).\\n\\n    Read more in the User Guide: `logistic_regression`.\\n\\n    Parameters\\n    ----------\\n    - `Cs`: int or list of floats, default=10\\n        Each of the values in Cs describes the inverse of regularization\\n        strength. If Cs is as an int, then a grid of Cs values are chosen\\n        in a logarithmic scale between 1e-4 and 1e4.\\n        Like in support vector machines, smaller values specify stronger\\n        regularization.\\n\\n    - `fit_intercept`: bool, default=True\\n        Specifies if a constant (a.k.a. bias or intercept) should be\\n        added to the decision function.\\n\\n    - `cv`: int or cross-validation generator, default=None\\n        The default cross-validation generator used is Stratified K-Folds.\\n        If an integer is provided, then it is the number of folds used.\\n        See the module `sklearn.model_selection` module for the\\n        list of possible cross-validation objects.\\n\\n        *Changed in 0.22*\\n            ``cv`` default value if None changed from 3-fold to 5-fold.\\n\\n    - `dual`: bool, default=False\\n        Dual or primal formulation. Dual formulation is only implemented for\\n        l2 penalty with liblinear solver. Prefer dual=False when\\n        n_samples > n_features.\\n\\n    - `penalty`: {'l1', 'l2', 'elasticnet'}, default='l2'\\n        Specify the norm of the penalty:\\n\\n        - `'l2'`: add a L2 penalty term (used by default);\\n        - `'l1'`: add a L1 penalty term;\\n        - `'elasticnet'`: both L1 and L2 penalty terms are added.\\n\\n\\n---\\n**Warning**\\n\\nSome penalties may not work with some solvers. See the parameter\\n`solver` below, to know the compatibility between the penalty and\\nsolver.\\n\\n : str or callable, default=None\\ntring (see model evaluation documentation) or\\ncorer callable object / function with signature\\ncorer(estimator, X, y)``. For a list of scoring functions\\nt can be used, look at :mod:`sklearn.metrics`. The\\nault scoring option used is 'accuracy'.\\n\\n: {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},             default='lbfgs'\\n\\norithm to use in the optimization problem. Default is 'lbfgs'.\\nchoose a solver, you might want to consider the following aspects:\\n\\n - For small datasets, 'liblinear' is a good choice, whereas 'sag'\\n   and 'saga' are faster for large ones;\\n - For multiclass problems, only 'newton-cg', 'sag', 'saga' and\\n   'lbfgs' handle multinomial loss;\\n - 'liblinear' might be slower in :class:`LogisticRegressionCV`\\n   because it does not handle warm-starting. 'liblinear' is\\n   limited to one-versus-rest schemes.\\n\\nwarning::\\nThe choice of the algorithm depends on the penalty chosen:\\n\\n- 'newton-cg'   -   ['l2']\\n- 'lbfgs'       -   ['l2']\\n- 'liblinear'   -   ['l1', 'l2']\\n- 'sag'         -   ['l2']\\n- 'saga'        -   ['elasticnet', 'l1', 'l2']\\n\\nnote::\\n'sag' and 'saga' fast convergence is only guaranteed on features\\nwith approximately the same scale. You can preprocess the data with\\na scaler from :mod:`sklearn.preprocessing`.\\n\\nversionadded:: 0.17\\nStochastic Average Gradient descent solver.\\nversionadded:: 0.19\\nSAGA solver.\\n\\nloat, default=1e-4\\nerance for stopping criteria.\\n\\nr : int, default=100\\nimum number of iterations of the optimization algorithm.\\n\\neight : dict or 'balanced', default=None\\nghts associated with classes in the form ``{class_label: weight}``.\\nnot given, all classes are supposed to have weight one.\\n\\n \\\"balanced\\\" mode uses the values of y to automatically adjust\\nghts inversely proportional to class frequencies in the input data\\n``n_samples / (n_classes * np.bincount(y))``.\\n\\ne that these weights will be multiplied with sample_weight (passed\\nough the fit method) if sample_weight is specified.\\n\\nversionadded:: 0.17\\nclass_weight == 'balanced'\\n\\n: int, default=None\\nber of CPU cores used during the cross-validation loop.\\none`` means 1 unless in a :obj:`joblib.parallel_backend` context.\\n1`` means using all processors. See :term:`Glossary <n_jobs>`\\n more details.\\n\\n : int, default=0\\n the 'liblinear', 'sag' and 'lbfgs' solvers set verbose to any\\nitive number for verbosity.\\n\\n bool, default=True\\nset to True, the scores are averaged across all folds, and the\\nfs and the C that corresponds to the best score is taken, and a\\nal refit is done using these parameters.\\nerwise the coefs, intercepts and C that correspond to the\\nt scores across folds are averaged.\\n\\npt_scaling : float, default=1\\nful only when the solver 'liblinear' is used\\n self.fit_intercept is set to True. In this case, x becomes\\n self.intercept_scaling],\\n. a \\\"synthetic\\\" feature with constant value equal to\\nercept_scaling is appended to the instance vector.\\n intercept becomes ``intercept_scaling * synthetic_feature_weight``.\\n\\ne! the synthetic feature weight is subject to l1/l2 regularization\\nall other features.\\nlessen the effect of regularization on synthetic feature weight\\nd therefore on the intercept) intercept_scaling has to be increased.\\n\\nlass : {'auto, 'ovr', 'multinomial'}, default='auto'\\nthe option chosen is 'ovr', then a binary problem is fit for each\\nel. For 'multinomial' the loss minimised is the multinomial loss fit\\noss the entire probability distribution, *even when the data is\\nary*. 'multinomial' is unavailable when solver='liblinear'.\\nto' selects 'ovr' if the data is binary, or if solver='liblinear',\\n otherwise selects 'multinomial'.\\n\\nversionadded:: 0.18\\nStochastic Average Gradient descent solver for 'multinomial' case.\\nversionchanged:: 0.22\\n Default changed from 'ovr' to 'auto' in 0.22.\\n\\nstate : int, RandomState instance, default=None\\nd when `solver='sag'`, 'saga' or 'liblinear' to shuffle the data.\\ne that this only applies to the solver and not the cross-validation\\nerator. See :term:`Glossary <random_state>` for details.\\n\\nos : list of float, default=None\\n list of Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``.\\ny used if ``penalty='elasticnet'``. A value of 0 is equivalent to\\nng ``penalty='l2'``, while 1 is equivalent to using\\nenalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a combination\\nL1 and L2.\\n\\ntes\\n---\\n_ : ndarray of shape (n_classes, )\\nist of class labels known to the classifier.\\n\\n ndarray of shape (1, n_features) or (n_classes, n_features)\\nfficient of the features in the decision function.\\n\\nef_` is of shape (1, n_features) when the given problem\\nbinary.\\n\\npt_ : ndarray of shape (1,) or (n_classes,)\\nercept (a.k.a. bias) added to the decision function.\\n\\n`fit_intercept` is set to False, the intercept is set to zero.\\ntercept_` is of shape(1,) when the problem is binary.\\n\\ndarray of shape (n_cs)\\nay of C i.e. inverse of regularization parameter values used\\n cross-validation.\\n\\nos_ : ndarray of shape (n_l1_ratios)\\nay of l1_ratios used for cross-validation. If no l1_ratio is used\\ne. penalty is not 'elasticnet'), this is set to ``[None]``\\n\\naths_ : ndarray of shape (n_folds, n_cs, n_features) or                    (n_folds, n_cs, n_features + 1)\\nt with classes as the keys, and the path of coefficients obtained\\ning cross-validating across each fold and then across each Cs\\ner doing an OvR for the corresponding class as values.\\nthe 'multi_class' option is set to 'multinomial', then\\n coefs_paths are the coefficients corresponding to each class.\\nh dict value has shape ``(n_folds, n_cs, n_features)`` or\\nn_folds, n_cs, n_features + 1)`` depending on whether the\\nercept is fit or not. If ``penalty='elasticnet'``, the shape is\\nn_folds, n_cs, n_l1_ratios_, n_features)`` or\\nn_folds, n_cs, n_l1_ratios_, n_features + 1)``.\\n\\n : dict\\nt with classes as the keys, and the values as the\\nd of scores obtained during cross-validating each fold, after doing\\nOvR for the corresponding class. If the 'multi_class' option\\nen is 'multinomial' then the same scores are repeated across\\n classes, since this is the multinomial class. Each dict value\\n shape ``(n_folds, n_cs`` or ``(n_folds, n_cs, n_l1_ratios)`` if\\nenalty='elasticnet'``.\\n\\narray of shape (n_classes,) or (n_classes - 1,)\\nay of C that maps to the best scores across every class. If refit is\\n to False, then for each class, the best C is the average of the\\n that correspond to the best scores for each fold.\\n` is of shape(n_classes,) when the problem is binary.\\n\\no_ : ndarray of shape (n_classes,) or (n_classes - 1,)\\nay of l1_ratio that maps to the best scores across every class. If\\nit is set to False, then for each class, the best l1_ratio is the\\nrage of the l1_ratio's that correspond to the best scores for each\\nd.  `l1_ratio_` is of shape(n_classes,) when the problem is binary.\\n\\n : ndarray of shape (n_classes, n_folds, n_cs) or (1, n_folds, n_cs)\\nual number of iterations for all classes, folds and Cs.\\nthe binary or multinomial cases, the first dimension is equal to 1.\\n``penalty='elasticnet'``, the shape is ``(n_classes, n_folds,\\ns, n_l1_ratios)`` or ``(1, n_folds, n_cs, n_l1_ratios)``.\\n\\nres_in_ : int\\nber of features seen during :term:`fit`.\\n\\nversionadded:: 0.24\\n\\n_names_in_ : ndarray of shape (`n_features_in_`,)\\nes of features seen during :term:`fit`. Defined only when `X`\\n feature names that are all strings.\\n\\nversionadded:: 1.0\\n\\no\\n-\\ncRegression : Logistic regression without tuning the\\nerparameter `C`.\\n\\ns\\n-\\nm sklearn.datasets import load_iris\\nm sklearn.linear_model import LogisticRegressionCV\\ny = load_iris(return_X_y=True)\\n = LogisticRegressionCV(cv=5, random_state=0).fit(X, y)\\n.predict(X[:2, :])\\n0, 0])\\n.predict_proba(X[:2, :]).shape\\n\\n.score(X, y)\\n\\n\\n---\\n\"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.classification/mlp-classifier\"} \":sklearn.classification/mlp-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [23 2]:\\n\\n|                :name |  :default |\\n|----------------------|-----------|\\n|    :n-iter-no-change |        10 |\\n|       :learning-rate |  constant |\\n|          :activation |      relu |\\n|  :hidden-layer-sizes |     [100] |\\n|                 :tol | 0.0001000 |\\n|              :beta-2 |    0.9990 |\\n|      :early-stopping |     false |\\n|  :nesterovs-momentum |      true |\\n|          :batch-size |      auto |\\n|              :solver |      adam |\\n|                  ... |       ... |\\n|             :max-fun |     15000 |\\n|              :beta-1 |    0.9000 |\\n|            :max-iter |       200 |\\n|        :random-state |           |\\n|            :momentum |    0.9000 |\\n|  :learning-rate-init |  0.001000 |\\n|               :alpha | 0.0001000 |\\n|          :warm-start |     false |\\n| :validation-fraction |    0.1000 |\\n|             :verbose |     false |\\n|             :epsilon | 1.000E-08 |\\n\"]]] [:span [:p/markdown \"Multi-layer Perceptron classifier.\\n\\n    This model optimizes the log-loss function using LBFGS or stochastic\\n    gradient descent.\\n\\n    *Added in 0.18*\\n\\n    Parameters\\n    ----------\\n    - `hidden_layer_sizes`: tuple, length = n_layers - 2, default=(100,)\\n        The ith element represents the number of neurons in the ith\\n        hidden layer.\\n\\n    - `activation`: {'identity', 'logistic', 'tanh', 'relu'}, default='relu'\\n        Activation function for the hidden layer.\\n\\n        - 'identity', no-op activation, useful to implement linear bottleneck,\\n          returns f(x) = x\\n\\n        - 'logistic', the logistic sigmoid function,\\n          returns f(x) = 1 / (1 + exp(-x)).\\n\\n        - 'tanh', the hyperbolic tan function,\\n          returns f(x) = tanh(x).\\n\\n        - 'relu', the rectified linear unit function,\\n          returns f(x) = max(0, x)\\n\\n    - `solver`: {'lbfgs', 'sgd', 'adam'}, default='adam'\\n        The solver for weight optimization.\\n\\n        - 'lbfgs' is an optimizer in the family of quasi-Newton methods.\\n\\n        - 'sgd' refers to stochastic gradient descent.\\n\\n        - 'adam' refers to a stochastic gradient-based optimizer proposed\\n          by Kingma, Diederik, and Jimmy Ba\\n\\n        Note: The default solver 'adam' works pretty well on relatively\\n        large datasets (with thousands of training samples or more) in terms of\\n        both training time and validation score.\\n        For small datasets, however, 'lbfgs' can converge faster and perform\\n        better.\\n\\n    - `alpha`: float, default=0.0001\\n        Strength of the L2 regularization term. The L2 regularization term\\n        is divided by the sample size when added to the loss.\\n\\n    - `batch_size`: int, default='auto'\\n        Size of minibatches for stochastic optimizers.\\n        If the solver is 'lbfgs', the classifier will not use minibatch.\\n        When set to \\\"auto\\\", `batch_size=min(200, n_samples)`.\\n\\n    - `learning_rate`: {'constant', 'invscaling', 'adaptive'}, default='constant'\\n        Learning rate schedule for weight updates.\\n\\n        - 'constant' is a constant learning rate given by\\n          'learning_rate_init'.\\n\\n        - 'invscaling' gradually decreases the learning rate at each\\n          time step 't' using an inverse scaling exponent of 'power_t'.\\n          effective_learning_rate = learning_rate_init / pow(t, power_t)\\n\\n        - 'adaptive' keeps the learning rate constant to\\n          'learning_rate_init' as long as training loss keeps decreasing.\\n          Each time two consecutive epochs fail to decrease training loss by at\\n          least tol, or fail to increase validation score by at least tol if\\n          'early_stopping' is on, the current learning rate is divided by 5.\\n\\n        Only used when ``solver='sgd'``.\\n\\n    - `learning_rate_init`: float, default=0.001\\n        The initial learning rate used. It controls the step-size\\n        in updating the weights. Only used when solver='sgd' or 'adam'.\\n\\n    - `power_t`: float, default=0.5\\n        The exponent for inverse scaling learning rate.\\n        It is used in updating effective learning rate when the learning_rate\\n        is set to 'invscaling'. Only used when solver='sgd'.\\n\\n    - `max_iter`: int, default=200\\n        Maximum number of iterations. The solver iterates until convergence\\n        (determined by 'tol') or this number of iterations. For stochastic\\n        solvers ('sgd', 'adam'), note that this determines the number of epochs\\n        (how many times each data point will be used), not the number of\\n        gradient steps.\\n\\n    - `shuffle`: bool, default=True\\n        Whether to shuffle samples in each iteration. Only used when\\n        solver='sgd' or 'adam'.\\n\\n    - `random_state`: int, RandomState instance, default=None\\n        Determines random number generation for weights and bias\\n        initialization, train-test split if early stopping is used, and batch\\n        sampling when solver='sgd' or 'adam'.\\n        Pass an int for reproducible results across multiple function calls.\\n        See `Glossary <random_state>`.\\n\\n    - `tol`: float, default=1e-4\\n        Tolerance for the optimization. When the loss or score is not improving\\n        by at least ``tol`` for ``n_iter_no_change`` consecutive iterations,\\n        unless ``learning_rate`` is set to 'adaptive', convergence is\\n        considered to be reached and training stops.\\n\\n    - `verbose`: bool, default=False\\n        Whether to print progress messages to stdout.\\n\\n    - `warm_start`: bool, default=False\\n        When set to True, reuse the solution of the previous\\n        call to fit as initialization, otherwise, just erase the\\n        previous solution. See `the Glossary <warm_start>`.\\n\\n    - `momentum`: float, default=0.9\\n        Momentum for gradient descent update. Should be between 0 and 1. Only\\n        used when solver='sgd'.\\n\\n    - `nesterovs_momentum`: bool, default=True\\n        Whether to use Nesterov's momentum. Only used when solver='sgd' and\\n        momentum > 0.\\n\\n    - `early_stopping`: bool, default=False\\n        Whether to use early stopping to terminate training when validation\\n        score is not improving. If set to true, it will automatically set\\n        aside 10% of training data as validation and terminate training when\\n        validation score is not improving by at least tol for\\n        ``n_iter_no_change`` consecutive epochs. The split is stratified,\\n        except in a multilabel setting.\\n        If early stopping is False, then the training stops when the training\\n        loss does not improve by more than tol for n_iter_no_change consecutive\\n        passes over the training set.\\n        Only effective when solver='sgd' or 'adam'.\\n\\n    - `validation_fraction`: float, default=0.1\\n        The proportion of training data to set aside as validation set for\\n        early stopping. Must be between 0 and 1.\\n        Only used if early_stopping is True.\\n\\n    - `beta_1`: float, default=0.9\\n        Exponential decay rate for estimates of first moment vector in adam,\\n        should be in [0, 1). Only used when solver='adam'.\\n\\n    - `beta_2`: float, default=0.999\\n        Exponential decay rate for estimates of second moment vector in adam,\\n        should be in [0, 1). Only used when solver='adam'.\\n\\n    - `epsilon`: float, default=1e-8\\n        Value for numerical stability in adam. Only used when solver='adam'.\\n\\n    - `n_iter_no_change`: int, default=10\\n        Maximum number of epochs to not meet ``tol`` improvement.\\n        Only effective when solver='sgd' or 'adam'.\\n\\n        *Added in 0.20*\\n\\n    - `max_fun`: int, default=15000\\n        Only used when solver='lbfgs'. Maximum number of loss function calls.\\n        The solver iterates until convergence (determined by 'tol'), number\\n        of iterations reaches max_iter, or this number of loss function calls.\\n        Note that number of loss function calls will be greater than or equal\\n        to the number of iterations for the `MLPClassifier`.\\n\\n        *Added in 0.22*\\n\\n    Attributes\\n    ----------\\n    - `classes_`: ndarray or list of ndarray of shape (n_classes,)\\n        Class labels for each output.\\n\\n    - `loss_`: float\\n        The current loss computed with the loss function.\\n\\n    - `best_loss_`: float\\n        The minimum loss reached by the solver throughout fitting.\\n\\n    - `loss_curve_`: list of shape (`n_iter_`,)\\n        The ith element in the list represents the loss at the ith iteration.\\n\\n    - `t_`: int\\n        The number of training samples seen by the solver during fitting.\\n\\n    - `coefs_`: list of shape (n_layers - 1,)\\n        The ith element in the list represents the weight matrix corresponding\\n        to layer i.\\n\\n    - `intercepts_`: list of shape (n_layers - 1,)\\n        The ith element in the list represents the bias vector corresponding to\\n        layer i + 1.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    - `n_iter_`: int\\n        The number of iterations the solver has run.\\n\\n    - `n_layers_`: int\\n        Number of layers.\\n\\n    - `n_outputs_`: int\\n        Number of outputs.\\n\\n    - `out_activation_`: str\\n        Name of the output activation function.\\n\\n    See Also\\n    --------\\n    - `MLPRegressor`: Multi-layer Perceptron regressor.\\n    - `BernoulliRBM`: Bernoulli Restricted Boltzmann Machine (RBM).\\n\\n    Notes\\n    -----\\n    MLPClassifier trains iteratively since at each time step\\n    the partial derivatives of the loss function with respect to the model\\n    parameters are computed to update the parameters.\\n\\n    It can also have a regularization term added to the loss function\\n    that shrinks model parameters to prevent overfitting.\\n\\n    This implementation works with data represented as dense numpy arrays or\\n    sparse scipy arrays of floating point values.\\n\\n    References\\n    ----------\\n    Hinton, Geoffrey E. \\\"Connectionist learning procedures.\\\"\\n    Artificial intelligence 40.1 (1989): 185-234.\\n\\n    Glorot, Xavier, and Yoshua Bengio.\\n    \\\"Understanding the difficulty of training deep feedforward neural networks.\\\"\\n    International Conference on Artificial Intelligence and Statistics. 2010.\\n\\n    :arxiv:`He, Kaiming, et al (2015). \\\"Delving deep into rectifiers:\\n    Surpassing human-level performance on imagenet classification.\\\" <1502.01852>`\\n\\n    :arxiv:`Kingma, Diederik, and Jimmy Ba (2014)\\n    \\\"Adam: A method for stochastic optimization.\\\" <1412.6980>`\\n\\n    Examples\\n    --------\\n    >>> from sklearn.neural_network import MLPClassifier\\n    >>> from sklearn.datasets import make_classification\\n    >>> from sklearn.model_selection import train_test_split\\n    >>> X, y = make_classification(n_samples=100, random_state=1)\\n    >>> X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\\n    ...                                                     random_state=1)\\n    >>> clf = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)\\n    >>> clf.predict_proba(X_test[:1])\\n    array([[0.038..., 0.961...]])\\n    >>> clf.predict(X_test[:5, :])\\n    array([1, 0, 1, 0, 1])\\n    >>> clf.score(X_test, y_test)\\n    0.8...\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.classification/multinomial-nb\"} \":sklearn.classification/multinomial-nb\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [3 2]:\\n\\n|        :name | :default |\\n|--------------|----------|\\n|       :alpha |    1.000 |\\n| :class-prior |          |\\n|   :fit-prior |     true |\\n\"]]] [:span [:p/markdown \"\\n    Naive Bayes classifier for multinomial models.\\n\\n    The multinomial Naive Bayes classifier is suitable for classification with\\n    discrete features (e.g., word counts for text classification). The\\n    multinomial distribution normally requires integer feature counts. However,\\n    in practice, fractional counts such as tf-idf may also work.\\n\\n    Read more in the User Guide: `multinomial_naive_bayes`.\\n\\n    Parameters\\n    ----------\\n    - `alpha`: float, default=1.0\\n        Additive (Laplace/Lidstone) smoothing parameter\\n        (0 for no smoothing).\\n\\n    - `fit_prior`: bool, default=True\\n        Whether to learn class prior probabilities or not.\\n        If false, a uniform prior will be used.\\n\\n    - `class_prior`: array-like of shape (n_classes,), default=None\\n        Prior probabilities of the classes. If specified, the priors are not\\n        adjusted according to the data.\\n\\n    Attributes\\n    ----------\\n    - `class_count_`: ndarray of shape (n_classes,)\\n        Number of samples encountered for each class during fitting. This\\n        value is weighted by the sample weight when provided.\\n\\n    - `class_log_prior_`: ndarray of shape (n_classes,)\\n        Smoothed empirical log probability for each class.\\n\\n    - `classes_`: ndarray of shape (n_classes,)\\n        Class labels known to the classifier\\n\\n    - `feature_count_`: ndarray of shape (n_classes, n_features)\\n        Number of samples encountered for each (class, feature)\\n        during fitting. This value is weighted by the sample weight when\\n        provided.\\n\\n    - `feature_log_prob_`: ndarray of shape (n_classes, n_features)\\n        Empirical log probability of features\\n        given a class, ``P(x_i|y)``.\\n\\n    - `n_features_`: int\\n        Number of features of each sample.\\n\\n        *Deprecated since 1.0*\\n            Attribute `n_features_` was deprecated in version 1.0 and will be\\n            removed in 1.2. Use `n_features_in_` instead.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    See Also\\n    --------\\n    - `BernoulliNB`: Naive Bayes classifier for multivariate Bernoulli models.\\n    - `CategoricalNB`: Naive Bayes classifier for categorical features.\\n    - `ComplementNB`: Complement Naive Bayes classifier.\\n    - `GaussianNB`: Gaussian Naive Bayes.\\n\\n    References\\n    ----------\\n    C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\\n    Information Retrieval. Cambridge University Press, pp. 234-265.\\n    https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> rng = np.random.RandomState(1)\\n    >>> X = rng.randint(5, size=(6, 100))\\n    >>> y = np.array([1, 2, 3, 4, 5, 6])\\n    >>> from sklearn.naive_bayes import MultinomialNB\\n    >>> clf = MultinomialNB()\\n    >>> clf.fit(X, y)\\n    MultinomialNB()\\n    >>> print(clf.predict(X[2:3]))\\n    [3]\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.classification/nearest-centroid\"} \":sklearn.classification/nearest-centroid\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [2 2]:\\n\\n|             :name |  :default |\\n|-------------------|-----------|\\n|           :metric | euclidean |\\n| :shrink-threshold |           |\\n\"]]] [:span [:p/markdown \"Nearest centroid classifier.\\n\\n    Each class is represented by its centroid, with test samples classified to\\n    the class with the nearest centroid.\\n\\n    Read more in the User Guide: `nearest_centroid_classifier`.\\n\\n    Parameters\\n    ----------\\n    - `metric`: str or callable, default=\\\"euclidean\\\"\\n        The metric to use when calculating distance between instances in a\\n        feature array. If metric is a string or callable, it must be one of\\n        the options allowed by\\n        `~sklearn.metrics.pairwise_distances` for its metric\\n        parameter. The centroids for the samples corresponding to each class is\\n        the point from which the sum of the distances (according to the metric)\\n        of all samples that belong to that particular class are minimized.\\n        If the `\\\"manhattan\\\"` metric is provided, this centroid is the median\\n        and for all other metrics, the centroid is now set to be the mean.\\n\\n        *Changed in 0.19*\\n            `metric='precomputed'` was deprecated and now raises an error\\n\\n    - `shrink_threshold`: float, default=None\\n        Threshold for shrinking centroids to remove features.\\n\\n    Attributes\\n    ----------\\n    - `centroids_`: array-like of shape (n_classes, n_features)\\n        Centroid of each class.\\n\\n    - `classes_`: array of shape (n_classes,)\\n        The unique classes labels.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    See Also\\n    --------\\n    - `KNeighborsClassifier`: Nearest neighbors classifier.\\n\\n    Notes\\n    -----\\n    When used for text classification with tf-idf vectors, this classifier is\\n    also known as the Rocchio classifier.\\n\\n    References\\n    ----------\\n    Tibshirani, R., Hastie, T., Narasimhan, B., & Chu, G. (2002). Diagnosis of\\n    multiple cancer types by shrunken centroids of gene expression. Proceedings\\n    of the National Academy of Sciences of the United States of America,\\n    99(10), 6567-6572. The National Academy of Sciences.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.neighbors import NearestCentroid\\n    >>> import numpy as np\\n    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\\n    >>> y = np.array([1, 1, 1, 2, 2, 2])\\n    >>> clf = NearestCentroid()\\n    >>> clf.fit(X, y)\\n    NearestCentroid()\\n    >>> print(clf.predict([[-0.8, -1]]))\\n    [1]\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.classification/nu-svc\"} \":sklearn.classification/nu-svc\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [15 2]:\\n\\n|                    :name | :default |\\n|--------------------------|----------|\\n|              :break-ties |    false |\\n|                  :kernel |      rbf |\\n|                   :gamma |    scale |\\n|                  :degree |        3 |\\n| :decision-function-shape |      ovr |\\n|             :probability |    false |\\n|                     :tol | 0.001000 |\\n|                      :nu |   0.5000 |\\n|               :shrinking |     true |\\n|                :max-iter |       -1 |\\n|            :random-state |          |\\n|                  :coef-0 |    0.000 |\\n|            :class-weight |          |\\n|              :cache-size |      200 |\\n|                 :verbose |    false |\\n\"]]] [:span [:p/markdown \"Nu-Support Vector Classification.\\n\\n    Similar to SVC but uses a parameter to control the number of support\\n    vectors.\\n\\n    The implementation is based on libsvm.\\n\\n    Read more in the User Guide: `svm_classification`.\\n\\n    Parameters\\n    ----------\\n    - `nu`: float, default=0.5\\n        An upper bound on the fraction of margin errors (see User Guide: `nu_svc`) and a lower bound of the fraction of support vectors.\\n        Should be in the interval (0, 1].\\n\\n    - `kernel`: {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'} or callable,          default='rbf'\\n         Specifies the kernel type to be used in the algorithm.\\n         If none is given, 'rbf' will be used. If a callable is given it is\\n         used to precompute the kernel matrix.\\n\\n    - `degree`: int, default=3\\n        Degree of the polynomial kernel function ('poly').\\n        Ignored by all other kernels.\\n\\n    - `gamma`: {'scale', 'auto'} or float, default='scale'\\n        Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\\n\\n        - if ``gamma='scale'`` (default) is passed then it uses\\n          1 / (n_features * X.var()) as value of gamma,\\n        - if 'auto', uses 1 / n_features.\\n\\n        *Changed in 0.22*\\n           The default value of ``gamma`` changed from 'auto' to 'scale'.\\n\\n    - `coef0`: float, default=0.0\\n        Independent term in kernel function.\\n        It is only significant in 'poly' and 'sigmoid'.\\n\\n    - `shrinking`: bool, default=True\\n        Whether to use the shrinking heuristic.\\n        See the User Guide: `shrinking_svm`.\\n\\n    - `probability`: bool, default=False\\n        Whether to enable probability estimates. This must be enabled prior\\n        to calling `fit`, will slow down that method as it internally uses\\n        5-fold cross-validation, and `predict_proba` may be inconsistent with\\n        `predict`. Read more in the User Guide: `scores_probabilities`.\\n\\n    - `tol`: float, default=1e-3\\n        Tolerance for stopping criterion.\\n\\n    - `cache_size`: float, default=200\\n        Specify the size of the kernel cache (in MB).\\n\\n    - `class_weight`: {dict, 'balanced'}, default=None\\n        Set the parameter C of class i to class_weight[i]*C for\\n        SVC. If not given, all classes are supposed to have\\n        weight one. The \\\"balanced\\\" mode uses the values of y to automatically\\n        adjust weights inversely proportional to class frequencies as\\n        ``n_samples / (n_classes * np.bincount(y))``.\\n\\n    - `verbose`: bool, default=False\\n        Enable verbose output. Note that this setting takes advantage of a\\n        per-process runtime setting in libsvm that, if enabled, may not work\\n        properly in a multithreaded context.\\n\\n    - `max_iter`: int, default=-1\\n        Hard limit on iterations within solver, or -1 for no limit.\\n\\n    - `decision_function_shape`: {'ovo', 'ovr'}, default='ovr'\\n        Whether to return a one-vs-rest ('ovr') decision function of shape\\n        (n_samples, n_classes) as all other classifiers, or the original\\n        one-vs-one ('ovo') decision function of libsvm which has shape\\n        (n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one\\n        ('ovo') is always used as multi-class strategy. The parameter is\\n        ignored for binary classification.\\n\\n        *Changed in 0.19*\\n            decision_function_shape is 'ovr' by default.\\n\\n        *Added in 0.17*\\n           *decision_function_shape='ovr'* is recommended.\\n\\n        *Changed in 0.17*\\n           Deprecated *decision_function_shape='ovo' and None*.\\n\\n    - `break_ties`: bool, default=False\\n        If true, ``decision_function_shape='ovr'``, and number of classes > 2,\\n        `predict` will break ties according to the confidence values of\\n        `decision_function`; otherwise the first class among the tied\\n        classes is returned. Please note that breaking ties comes at a\\n        relatively high computational cost compared to a simple predict.\\n\\n        *Added in 0.22*\\n\\n    - `random_state`: int, RandomState instance or None, default=None\\n        Controls the pseudo random number generation for shuffling the data for\\n        probability estimates. Ignored when `probability` is False.\\n        Pass an int for reproducible output across multiple function calls.\\n        See `Glossary <random_state>`.\\n\\n    Attributes\\n    ----------\\n    - `class_weight_`: ndarray of shape (n_classes,)\\n        Multipliers of parameter C of each class.\\n        Computed based on the ``class_weight`` parameter.\\n\\n    - `classes_`: ndarray of shape (n_classes,)\\n        The unique classes labels.\\n\\n    - `coef_`: ndarray of shape (n_classes * (n_classes -1) / 2, n_features)\\n        Weights assigned to the features (coefficients in the primal\\n        problem). This is only available in the case of a linear kernel.\\n\\n        `coef_` is readonly property derived from `dual_coef_` and\\n        `support_vectors_`.\\n\\n    - `dual_coef_`: ndarray of shape (n_classes - 1, n_SV)\\n        Dual coefficients of the support vector in the decision\\n        function (see :ref:`sgd_mathematical_formulation`), multiplied by\\n        their targets.\\n        For multiclass, coefficient for all 1-vs-1 classifiers.\\n        The layout of the coefficients in the multiclass case is somewhat\\n        non-trivial. See the multi-class section of the User Guide: `svm_multi_class` for details.\\n\\n    - `fit_status_`: int\\n        0 if correctly fitted, 1 if the algorithm did not converge.\\n\\n    - `intercept_`: ndarray of shape (n_classes * (n_classes - 1) / 2,)\\n        Constants in decision function.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    - `n_iter_`: ndarray of shape (n_classes * (n_classes - 1) // 2,)\\n        Number of iterations run by the optimization routine to fit the model.\\n        The shape of this attribute depends on the number of models optimized\\n        which in turn depends on the number of classes.\\n\\n        *Added in 1.1*\\n\\n    - `support_`: ndarray of shape (n_SV,)\\n        Indices of support vectors.\\n\\n    - `support_vectors_`: ndarray of shape (n_SV, n_features)\\n        Support vectors.\\n\\n    - `n_support_`: ndarray of shape (n_classes,), dtype=int32\\n        Number of support vectors for each class.\\n\\n    - `fit_status_`: int\\n        0 if correctly fitted, 1 if the algorithm did not converge.\\n\\n    - `probA_`: ndarray of shape (n_classes * (n_classes - 1) / 2,)\\n    - `probB_`: ndarray of shape (n_classes * (n_classes - 1) / 2,)\\n        If `probability=True`, it corresponds to the parameters learned in\\n        Platt scaling to produce probability estimates from decision values.\\n        If `probability=False`, it's an empty array. Platt scaling uses the\\n        logistic function\\n        ``1 / (1 + exp(decision_value * probA_ + probB_))``\\n        where ``probA_`` and ``probB_`` are learned from the dataset [2]_. For\\n        more information on the multiclass case and training procedure see\\n        section 8 of [1]_.\\n\\n    - `shape_fit_`: tuple of int of shape (n_dimensions_of_X,)\\n        Array dimensions of training vector ``X``.\\n\\n    See Also\\n    --------\\n    - `SVC`: Support Vector Machine for classification using libsvm.\\n\\n    - `LinearSVC`: Scalable linear Support Vector Machine for classification using\\n        liblinear.\\n\\n    References\\n    ----------\\n - [1] [LIBSVM: A Library for Support Vector Machines\\n        ](http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf)\\n\\n - [2] [Platt, John (1999). \\\"Probabilistic outputs for support vector\\n        machines and comparison to regularizedlikelihood methods.\\\"\\n        ](http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639)\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\\n    >>> y = np.array([1, 1, 2, 2])\\n    >>> from sklearn.pipeline import make_pipeline\\n    >>> from sklearn.preprocessing import StandardScaler\\n    >>> from sklearn.svm import NuSVC\\n    >>> clf = make_pipeline(StandardScaler(), NuSVC())\\n    >>> clf.fit(X, y)\\n    Pipeline(steps=[('standardscaler', StandardScaler()), ('nusvc', NuSVC())])\\n    >>> print(clf.predict([[-0.8, -1]]))\\n    [1]\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.classification/passive-aggressive-classifier\"} \":sklearn.classification/passive-aggressive-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [15 2]:\\n\\n|                :name | :default |\\n|----------------------|----------|\\n|    :n-iter-no-change |        5 |\\n|             :average |    false |\\n|                 :tol | 0.001000 |\\n|      :early-stopping |    false |\\n|             :shuffle |     true |\\n|                   :c |    1.000 |\\n|            :max-iter |     1000 |\\n|              :n-jobs |          |\\n|        :random-state |          |\\n|       :fit-intercept |     true |\\n|          :warm-start |    false |\\n| :validation-fraction |   0.1000 |\\n|        :class-weight |          |\\n|                :loss |    hinge |\\n|             :verbose |        0 |\\n\"]]] [:span [:p/markdown \"Passive Aggressive Classifier.\\n\\n    Read more in the User Guide: `passive_aggressive`.\\n\\n    Parameters\\n    ----------\\n    - `C`: float, default=1.0\\n        Maximum step size (regularization). Defaults to 1.0.\\n\\n    - `fit_intercept`: bool, default=True\\n        Whether the intercept should be estimated or not. If False, the\\n        data is assumed to be already centered.\\n\\n    - `max_iter`: int, default=1000\\n        The maximum number of passes over the training data (aka epochs).\\n        It only impacts the behavior in the ``fit`` method, and not the\\n        `partial_fit` method.\\n\\n        *Added in 0.19*\\n\\n    - `tol`: float or None, default=1e-3\\n        The stopping criterion. If it is not None, the iterations will stop\\n        when (loss > previous_loss - tol).\\n\\n        *Added in 0.19*\\n\\n    - `early_stopping`: bool, default=False\\n        Whether to use early stopping to terminate training when validation.\\n        score is not improving. If set to True, it will automatically set aside\\n        a stratified fraction of training data as validation and terminate\\n        training when validation score is not improving by at least tol for\\n        n_iter_no_change consecutive epochs.\\n\\n        *Added in 0.20*\\n\\n    - `validation_fraction`: float, default=0.1\\n        The proportion of training data to set aside as validation set for\\n        early stopping. Must be between 0 and 1.\\n        Only used if early_stopping is True.\\n\\n        *Added in 0.20*\\n\\n    - `n_iter_no_change`: int, default=5\\n        Number of iterations with no improvement to wait before early stopping.\\n\\n        *Added in 0.20*\\n\\n    - `shuffle`: bool, default=True\\n        Whether or not the training data should be shuffled after each epoch.\\n\\n    - `verbose`: int, default=0\\n        The verbosity level.\\n\\n    - `loss`: str, default=\\\"hinge\\\"\\n        The loss function to be used:\\n        hinge: equivalent to PA-I in the reference paper.\\n        squared_hinge: equivalent to PA-II in the reference paper.\\n\\n    - `n_jobs`: int or None, default=None\\n        The number of CPUs to use to do the OVA (One Versus All, for\\n        multi-class problems) computation.\\n        ``None`` means 1 unless in a `joblib.parallel_backend` context.\\n        ``-1`` means using all processors. See `Glossary <n_jobs>`\\n        for more details.\\n\\n    - `random_state`: int, RandomState instance, default=None\\n        Used to shuffle the training data, when ``shuffle`` is set to\\n        ``True``. Pass an int for reproducible output across multiple\\n        function calls.\\n        See `Glossary <random_state>`.\\n\\n    - `warm_start`: bool, default=False\\n        When set to True, reuse the solution of the previous call to fit as\\n        initialization, otherwise, just erase the previous solution.\\n        See `the Glossary <warm_start>`.\\n\\n        Repeatedly calling fit or partial_fit when warm_start is True can\\n        result in a different solution than when calling fit a single time\\n        because of the way the data is shuffled.\\n\\n    - `class_weight`: dict, {class_label: weight} or \\\"balanced\\\" or None,             default=None\\n        Preset for the class_weight fit parameter.\\n\\n        Weights associated with classes. If not given, all classes\\n        are supposed to have weight one.\\n\\n        The \\\"balanced\\\" mode uses the values of y to automatically adjust\\n        weights inversely proportional to class frequencies in the input data\\n        as ``n_samples / (n_classes * np.bincount(y))``.\\n\\n        *Added in 0.17*\\n           parameter *class_weight* to automatically weight samples.\\n\\n    - `average`: bool or int, default=False\\n        When set to True, computes the averaged SGD weights and stores the\\n        result in the ``coef_`` attribute. If set to an int greater than 1,\\n        averaging will begin once the total number of samples seen reaches\\n        average. So average=10 will begin averaging after seeing 10 samples.\\n\\n        *Added in 0.19*\\n           parameter *average* to use weights averaging in SGD.\\n\\n    Attributes\\n    ----------\\n    - `coef_`: ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)\\n        Weights assigned to the features.\\n\\n    - `intercept_`: ndarray of shape (1,) if n_classes == 2 else (n_classes,)\\n        Constants in decision function.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    - `n_iter_`: int\\n        The actual number of iterations to reach the stopping criterion.\\n        For multiclass fits, it is the maximum over every binary fit.\\n\\n    - `classes_`: ndarray of shape (n_classes,)\\n        The unique classes labels.\\n\\n    - `t_`: int\\n        Number of weight updates performed during training.\\n        Same as ``(n_iter_ * n_samples)``.\\n\\n    - `loss_function_`: callable\\n        Loss function used by the algorithm.\\n\\n    See Also\\n    --------\\n    - `SGDClassifier`: Incrementally trained logistic regression.\\n    - `Perceptron`: Linear perceptron classifier.\\n\\n    References\\n    ----------\\n    Online Passive-Aggressive Algorithms\\n    <http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf>\\n    K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)\\n\\n    Examples\\n    --------\\n    >>> from sklearn.linear_model import PassiveAggressiveClassifier\\n    >>> from sklearn.datasets import make_classification\\n    >>> X, y = make_classification(n_features=4, random_state=0)\\n    >>> clf = PassiveAggressiveClassifier(max_iter=1000, random_state=0,\\n    ... tol=1e-3)\\n    >>> clf.fit(X, y)\\n    PassiveAggressiveClassifier(random_state=0)\\n    >>> print(clf.coef_)\\n    [[0.26642044 0.45070924 0.67251877 0.64185414]]\\n    >>> print(clf.intercept_)\\n    [1.84127814]\\n    >>> print(clf.predict([[0, 0, 0, 0]]))\\n    [1]\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.classification/perceptron\"} \":sklearn.classification/perceptron\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [16 2]:\\n\\n|                :name |  :default |\\n|----------------------|-----------|\\n|    :n-iter-no-change |     5.000 |\\n|                 :tol |  0.001000 |\\n|      :early-stopping |     false |\\n|               :eta-0 |     1.000 |\\n|             :shuffle |      true |\\n|             :penalty |           |\\n|            :max-iter |      1000 |\\n|              :n-jobs |           |\\n|        :random-state |         0 |\\n|       :fit-intercept |      true |\\n|               :alpha | 0.0001000 |\\n|          :warm-start |     false |\\n|           :l-1-ratio |    0.1500 |\\n| :validation-fraction |    0.1000 |\\n|        :class-weight |           |\\n|             :verbose |         0 |\\n\"]]] [:span [:p/markdown \"Linear perceptron classifier.\\n\\n    Read more in the User Guide: `perceptron`.\\n\\n    Parameters\\n    ----------\\n\\n    - `penalty`: {'l2','l1','elasticnet'}, default=None\\n        The penalty (aka regularization term) to be used.\\n\\n    - `alpha`: float, default=0.0001\\n        Constant that multiplies the regularization term if regularization is\\n        used.\\n\\n    - `l1_ratio`: float, default=0.15\\n        The Elastic Net mixing parameter, with `0 <= l1_ratio <= 1`.\\n        `l1_ratio=0` corresponds to L2 penalty, `l1_ratio=1` to L1.\\n        Only used if `penalty='elasticnet'`.\\n\\n        *Added in 0.24*\\n\\n    - `fit_intercept`: bool, default=True\\n        Whether the intercept should be estimated or not. If False, the\\n        data is assumed to be already centered.\\n\\n    - `max_iter`: int, default=1000\\n        The maximum number of passes over the training data (aka epochs).\\n        It only impacts the behavior in the ``fit`` method, and not the\\n        `partial_fit` method.\\n\\n        *Added in 0.19*\\n\\n    - `tol`: float, default=1e-3\\n        The stopping criterion. If it is not None, the iterations will stop\\n        when (loss > previous_loss - tol).\\n\\n        *Added in 0.19*\\n\\n    - `shuffle`: bool, default=True\\n        Whether or not the training data should be shuffled after each epoch.\\n\\n    - `verbose`: int, default=0\\n        The verbosity level.\\n\\n    - `eta0`: float, default=1\\n        Constant by which the updates are multiplied.\\n\\n    - `n_jobs`: int, default=None\\n        The number of CPUs to use to do the OVA (One Versus All, for\\n        multi-class problems) computation.\\n        ``None`` means 1 unless in a `joblib.parallel_backend` context.\\n        ``-1`` means using all processors. See `Glossary <n_jobs>`\\n        for more details.\\n\\n    - `random_state`: int, RandomState instance or None, default=0\\n        Used to shuffle the training data, when ``shuffle`` is set to\\n        ``True``. Pass an int for reproducible output across multiple\\n        function calls.\\n        See `Glossary <random_state>`.\\n\\n    - `early_stopping`: bool, default=False\\n        Whether to use early stopping to terminate training when validation.\\n        score is not improving. If set to True, it will automatically set aside\\n        a stratified fraction of training data as validation and terminate\\n        training when validation score is not improving by at least tol for\\n        n_iter_no_change consecutive epochs.\\n\\n        *Added in 0.20*\\n\\n    - `validation_fraction`: float, default=0.1\\n        The proportion of training data to set aside as validation set for\\n        early stopping. Must be between 0 and 1.\\n        Only used if early_stopping is True.\\n\\n        *Added in 0.20*\\n\\n    - `n_iter_no_change`: int, default=5\\n        Number of iterations with no improvement to wait before early stopping.\\n\\n        *Added in 0.20*\\n\\n    - `class_weight`: dict, {class_label: weight} or \\\"balanced\\\", default=None\\n        Preset for the class_weight fit parameter.\\n\\n        Weights associated with classes. If not given, all classes\\n        are supposed to have weight one.\\n\\n        The \\\"balanced\\\" mode uses the values of y to automatically adjust\\n        weights inversely proportional to class frequencies in the input data\\n        as ``n_samples / (n_classes * np.bincount(y))``.\\n\\n    - `warm_start`: bool, default=False\\n        When set to True, reuse the solution of the previous call to fit as\\n        initialization, otherwise, just erase the previous solution. See\\n        `the Glossary <warm_start>`.\\n\\n    Attributes\\n    ----------\\n    - `classes_`: ndarray of shape (n_classes,)\\n        The unique classes labels.\\n\\n    - `coef_`: ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)\\n        Weights assigned to the features.\\n\\n    - `intercept_`: ndarray of shape (1,) if n_classes == 2 else (n_classes,)\\n        Constants in decision function.\\n\\n    - `loss_function_`: concreteLossFunction\\n        The function that determines the loss, or difference between the\\n        output of the algorithm and the target values.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    - `n_iter_`: int\\n        The actual number of iterations to reach the stopping criterion.\\n        For multiclass fits, it is the maximum over every binary fit.\\n\\n    - `t_`: int\\n        Number of weight updates performed during training.\\n        Same as ``(n_iter_ * n_samples)``.\\n\\n    See Also\\n    --------\\n    - `sklearn.linear_model.SGDClassifier`: Linear classifiers\\n        (SVM, logistic regression, etc.) with SGD training.\\n\\n    Notes\\n    -----\\n    ``Perceptron`` is a classification algorithm which shares the same\\n    underlying implementation with ``SGDClassifier``. In fact,\\n    ``Perceptron()`` is equivalent to `SGDClassifier(loss=\\\"perceptron\\\",\\n    eta0=1, learning_rate=\\\"constant\\\", penalty=None)`.\\n\\n    References\\n    ----------\\n    https://en.wikipedia.org/wiki/Perceptron and references therein.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.datasets import load_digits\\n    >>> from sklearn.linear_model import Perceptron\\n    >>> X, y = load_digits(return_X_y=True)\\n    >>> clf = Perceptron(tol=1e-3, random_state=0)\\n    >>> clf.fit(X, y)\\n    Perceptron()\\n    >>> clf.score(X, y)\\n    0.939...\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.classification/quadratic-discriminant-analysis\"} \":sklearn.classification/quadratic-discriminant-analysis\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [4 2]:\\n\\n|             :name |  :default |\\n|-------------------|-----------|\\n|           :priors |           |\\n|        :reg-param |     0.000 |\\n| :store-covariance |     false |\\n|              :tol | 0.0001000 |\\n\"]]] [:span [:p/markdown \"Quadratic Discriminant Analysis.\\n\\n    A classifier with a quadratic decision boundary, generated\\n    by fitting class conditional densities to the data\\n    and using Bayes' rule.\\n\\n    The model fits a Gaussian density to each class.\\n\\n    *Added in 0.17*\\n       *QuadraticDiscriminantAnalysis*\\n\\n    Read more in the User Guide: `lda_qda`.\\n\\n    Parameters\\n    ----------\\n    - `priors`: ndarray of shape (n_classes,), default=None\\n        Class priors. By default, the class proportions are inferred from the\\n        training data.\\n\\n    - `reg_param`: float, default=0.0\\n        Regularizes the per-class covariance estimates by transforming S2 as\\n        ``S2 = (1 - reg_param) * S2 + reg_param * np.eye(n_features)``,\\n        where S2 corresponds to the `scaling_` attribute of a given class.\\n\\n    - `store_covariance`: bool, default=False\\n        If True, the class covariance matrices are explicitly computed and\\n        stored in the `self.covariance_` attribute.\\n\\n        *Added in 0.17*\\n\\n    - `tol`: float, default=1.0e-4\\n        Absolute threshold for a singular value to be considered significant,\\n        used to estimate the rank of `Xk` where `Xk` is the centered matrix\\n        of samples in class k. This parameter does not affect the\\n        predictions. It only controls a warning that is raised when features\\n        are considered to be colinear.\\n\\n        *Added in 0.17*\\n\\n    Attributes\\n    ----------\\n    - `covariance_`: list of len n_classes of ndarray             of shape (n_features, n_features)\\n        For each class, gives the covariance matrix estimated using the\\n        samples of that class. The estimations are unbiased. Only present if\\n        `store_covariance` is True.\\n\\n    - `means_`: array-like of shape (n_classes, n_features)\\n        Class-wise means.\\n\\n    - `priors_`: array-like of shape (n_classes,)\\n        Class priors (sum to 1).\\n\\n    - `rotations_`: list of len n_classes of ndarray of shape (n_features, n_k)\\n        For each class k an array of shape (n_features, n_k), where\\n        ``n_k = min(n_features, number of elements in class k)``\\n        It is the rotation of the Gaussian distribution, i.e. its\\n        principal axis. It corresponds to `V`, the matrix of eigenvectors\\n        coming from the SVD of `Xk = U S Vt` where `Xk` is the centered\\n        matrix of samples from class k.\\n\\n    - `scalings_`: list of len n_classes of ndarray of shape (n_k,)\\n        For each class, contains the scaling of\\n        the Gaussian distributions along its principal axes, i.e. the\\n        variance in the rotated coordinate system. It corresponds to `S^2 /\\n        (n_samples - 1)`, where `S` is the diagonal matrix of singular values\\n        from the SVD of `Xk`, where `Xk` is the centered matrix of samples\\n        from class k.\\n\\n    - `classes_`: ndarray of shape (n_classes,)\\n        Unique class labels.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    See Also\\n    --------\\n    - `LinearDiscriminantAnalysis`: Linear Discriminant Analysis.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\\n    >>> import numpy as np\\n    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\\n    >>> y = np.array([1, 1, 1, 2, 2, 2])\\n    >>> clf = QuadraticDiscriminantAnalysis()\\n    >>> clf.fit(X, y)\\n    QuadraticDiscriminantAnalysis()\\n    >>> print(clf.predict([[-0.8, -1]]))\\n    [1]\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.classification/radius-neighbors-classifier\"} \":sklearn.classification/radius-neighbors-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [9 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|       :weights |   uniform |\\n|             :p |         2 |\\n|     :leaf-size |        30 |\\n| :metric-params |           |\\n|        :radius |     1.000 |\\n| :outlier-label |           |\\n|     :algorithm |      auto |\\n|        :n-jobs |           |\\n|        :metric | minkowski |\\n\"]]] [:span [:p/markdown \"Classifier implementing a vote among neighbors within a given radius.\\n\\n    Read more in the User Guide: `classification`.\\n\\n    Parameters\\n    ----------\\n    - `radius`: float, default=1.0\\n        Range of parameter space to use by default for `radius_neighbors`\\n        queries.\\n\\n    - `weights`: {'uniform', 'distance'} or callable, default='uniform'\\n        Weight function used in prediction.  Possible values:\\n\\n        - 'uniform' : uniform weights.  All points in each neighborhood\\n          are weighted equally.\\n        - 'distance' : weight points by the inverse of their distance.\\n          in this case, closer neighbors of a query point will have a\\n          greater influence than neighbors which are further away.\\n        - [callable] : a user-defined function which accepts an\\n          array of distances, and returns an array of the same shape\\n          containing the weights.\\n\\n        Uniform weights are used by default.\\n\\n    - `algorithm`: {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\\n        Algorithm used to compute the nearest neighbors:\\n\\n        - 'ball_tree' will use `BallTree`\\n        - 'kd_tree' will use `KDTree`\\n        - 'brute' will use a brute-force search.\\n        - 'auto' will attempt to decide the most appropriate algorithm\\n          based on the values passed to `fit` method.\\n\\n        Note: fitting on sparse input will override the setting of\\n        this parameter, using brute force.\\n\\n    - `leaf_size`: int, default=30\\n        Leaf size passed to BallTree or KDTree.  This can affect the\\n        speed of the construction and query, as well as the memory\\n        required to store the tree.  The optimal value depends on the\\n        nature of the problem.\\n\\n    - `p`: int, default=2\\n        Power parameter for the Minkowski metric. When p = 1, this is\\n        equivalent to using manhattan_distance (l1), and euclidean_distance\\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\\n\\n    - `metric`: str or callable, default='minkowski'\\n        Distance metric to use for the tree.  The default metric is\\n        minkowski, and with p=2 is equivalent to the standard Euclidean\\n        metric. For a list of available metrics, see the documentation of\\n        `~sklearn.metrics.DistanceMetric`.\\n        If metric is \\\"precomputed\\\", X is assumed to be a distance matrix and\\n        must be square during fit. X may be a `sparse graph`,\\n        in which case only \\\"nonzero\\\" elements may be considered neighbors.\\n\\n    - `outlier_label`: {manual label, 'most_frequent'}, default=None\\n        Label for outlier samples (samples with no neighbors in given radius).\\n\\n        - manual label: str or int label (should be the same type as y)\\n          or list of manual labels if multi-output is used.\\n        - 'most_frequent' : assign the most frequent label of y to outliers.\\n        - None : when any outlier is detected, ValueError will be raised.\\n\\n    - `metric_params`: dict, default=None\\n        Additional keyword arguments for the metric function.\\n\\n    - `n_jobs`: int, default=None\\n        The number of parallel jobs to run for neighbors search.\\n        ``None`` means 1 unless in a `joblib.parallel_backend` context.\\n        ``-1`` means using all processors. See `Glossary <n_jobs>`\\n        for more details.\\n\\n    - `**kwargs`: dict\\n        Additional keyword arguments passed to the constructor.\\n\\n        *Deprecated since 1.0*\\n            The RadiusNeighborsClassifier class will not longer accept extra\\n            keyword parameters in 1.2 since they are unused.\\n\\n    Attributes\\n    ----------\\n    - `classes_`: ndarray of shape (n_classes,)\\n        Class labels known to the classifier.\\n\\n    - `effective_metric_`: str or callable\\n        The distance metric used. It will be same as the `metric` parameter\\n        or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\\n        'minkowski' and `p` parameter set to 2.\\n\\n    - `effective_metric_params_`: dict\\n        Additional keyword arguments for the metric function. For most metrics\\n        will be same with `metric_params` parameter, but may also contain the\\n        `p` parameter value if the `effective_metric_` attribute is set to\\n        'minkowski'.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    - `n_samples_fit_`: int\\n        Number of samples in the fitted data.\\n\\n    - `outlier_label_`: int or array-like of shape (n_class,)\\n        Label which is given for outlier samples (samples with no neighbors\\n        on given radius).\\n\\n    - `outputs_2d_`: bool\\n        False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit\\n        otherwise True.\\n\\n    See Also\\n    --------\\n    - `KNeighborsClassifier`: Classifier implementing the k-nearest neighbors\\n        vote.\\n    - `RadiusNeighborsRegressor`: Regression based on neighbors within a\\n        fixed radius.\\n    - `KNeighborsRegressor`: Regression based on k-nearest neighbors.\\n    - `NearestNeighbors`: Unsupervised learner for implementing neighbor\\n        searches.\\n\\n    Notes\\n    -----\\n    See Nearest Neighbors: `neighbors` in the online documentation\\n    for a discussion of the choice of ``algorithm`` and ``leaf_size``.\\n\\n    https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\\n\\n    Examples\\n    --------\\n    >>> X = [[0], [1], [2], [3]]\\n    >>> y = [0, 0, 1, 1]\\n    >>> from sklearn.neighbors import RadiusNeighborsClassifier\\n    >>> neigh = RadiusNeighborsClassifier(radius=1.0)\\n    >>> neigh.fit(X, y)\\n    RadiusNeighborsClassifier(...)\\n    >>> print(neigh.predict([[1.5]]))\\n    [0]\\n    >>> print(neigh.predict_proba([[1.0]]))\\n    [[0.66666667 0.33333333]]\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.classification/random-forest-classifier\"} \":sklearn.classification/random-forest-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [18 2]:\\n\\n|                     :name | :default |\\n|---------------------------|----------|\\n| :min-weight-fraction-leaf |    0.000 |\\n|           :max-leaf-nodes |          |\\n|    :min-impurity-decrease |    0.000 |\\n|        :min-samples-split |    2.000 |\\n|                :bootstrap |     true |\\n|                :ccp-alpha |    0.000 |\\n|                   :n-jobs |          |\\n|             :random-state |          |\\n|                :oob-score |    false |\\n|         :min-samples-leaf |        1 |\\n|             :max-features |     sqrt |\\n|               :warm-start |    false |\\n|                :max-depth |          |\\n|             :class-weight |          |\\n|             :n-estimators |      100 |\\n|              :max-samples |          |\\n|                :criterion |     gini |\\n|                  :verbose |        0 |\\n\"]]] [:span [:p/markdown \"\\n    A random forest classifier.\\n\\n    A random forest is a meta estimator that fits a number of decision tree\\n    classifiers on various sub-samples of the dataset and uses averaging to\\n    improve the predictive accuracy and control over-fitting.\\n    The sub-sample size is controlled with the `max_samples` parameter if\\n    `bootstrap=True` (default), otherwise the whole dataset is used to build\\n    each tree.\\n\\n    Read more in the User Guide: `forest`.\\n\\n    Parameters\\n    ----------\\n    - `n_estimators`: int, default=100\\n        The number of trees in the forest.\\n\\n        *Changed in 0.22*\\n           The default value of ``n_estimators`` changed from 10 to 100\\n           in 0.22.\\n\\n    - `criterion`: {\\\"gini\\\", \\\"entropy\\\", \\\"log_loss\\\"}, default=\\\"gini\\\"\\n        The function to measure the quality of a split. Supported criteria are\\n        \\\"gini\\\" for the Gini impurity and \\\"log_loss\\\" and \\\"entropy\\\" both for the\\n        Shannon information gain, see :ref:`tree_mathematical_formulation`.\\n        Note: This parameter is tree-specific.\\n\\n    - `max_depth`: int, default=None\\n        The maximum depth of the tree. If None, then nodes are expanded until\\n        all leaves are pure or until all leaves contain less than\\n        min_samples_split samples.\\n\\n    - `min_samples_split`: int or float, default=2\\n        The minimum number of samples required to split an internal node:\\n\\n        - If int, then consider `min_samples_split` as the minimum number.\\n        - If float, then `min_samples_split` is a fraction and\\n          `ceil(min_samples_split * n_samples)` are the minimum\\n          number of samples for each split.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n    - `min_samples_leaf`: int or float, default=1\\n        The minimum number of samples required to be at a leaf node.\\n        A split point at any depth will only be considered if it leaves at\\n        least ``min_samples_leaf`` training samples in each of the left and\\n        right branches.  This may have the effect of smoothing the model,\\n        especially in regression.\\n\\n        - If int, then consider `min_samples_leaf` as the minimum number.\\n        - If float, then `min_samples_leaf` is a fraction and\\n          `ceil(min_samples_leaf * n_samples)` are the minimum\\n          number of samples for each node.\\n\\n        *Changed in 0.18*\\n           Added float values for fractions.\\n\\n    - `min_weight_fraction_leaf`: float, default=0.0\\n        The minimum weighted fraction of the sum total of weights (of all\\n        the input samples) required to be at a leaf node. Samples have\\n        equal weight when sample_weight is not provided.\\n\\n    - `max_features`: {\\\"sqrt\\\", \\\"log2\\\", None}, int or float, default=\\\"sqrt\\\"\\n        The number of features to consider when looking for the best split:\\n\\n        - If int, then consider `max_features` features at each split.\\n        - If float, then `max_features` is a fraction and\\n          `round(max_features * n_features)` features are considered at each\\n          split.\\n        - If \\\"auto\\\", then `max_features=sqrt(n_features)`.\\n        - If \\\"sqrt\\\", then `max_features=sqrt(n_features)`.\\n        - If \\\"log2\\\", then `max_features=log2(n_features)`.\\n        - If None, then `max_features=n_features`.\\n\\n        *Changed in 1.1*\\n            The default of `max_features` changed from `\\\"auto\\\"` to `\\\"sqrt\\\"`.\\n\\n        *Deprecated since 1.1*\\n            The `\\\"auto\\\"` option was deprecated in 1.1 and will be removed\\n            in 1.3.\\n\\n        Note: the search for a split does not stop until at least one\\n        valid partition of the node samples is found, even if it requires to\\n        effectively inspect more than ``max_features`` features.\\n\\n    - `max_leaf_nodes`: int, default=None\\n        Grow trees with ``max_leaf_nodes`` in best-first fashion.\\n        Best nodes are defined as relative reduction in impurity.\\n        If None then unlimited number of leaf nodes.\\n\\n    - `min_impurity_decrease`: float, default=0.0\\n        A node will be split if this split induces a decrease of the impurity\\n        greater than or equal to this value.\\n\\n        The weighted impurity decrease equation is the following\\n\\n```python\\nN_t / N * (impurity - N_t_R / N_t * right_impurity\\n                    - N_t_L / N_t * left_impurity)\\n\\ne ``N`` is the total number of samples, ``N_t`` is the number of\\nles at the current node, ``N_t_L`` is the number of samples in the\\n child, and ``N_t_R`` is the number of samples in the right child.\\n\\n`, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\\n`sample_weight`` is passed.\\n\\nersionadded:: 0.19\\n\\np : bool, default=True\\nher bootstrap samples are used when building trees. If False, the\\ne dataset is used to build each tree.\\n\\ne : bool, default=False\\nher to use out-of-bag samples to estimate the generalization score.\\n available if bootstrap=True.\\n\\n int, default=None\\nnumber of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\\nh:`decision_path` and :meth:`apply` are all parallelized over the\\ns. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\\next. ``-1`` means using all processors. See :term:`Glossary\\nobs>` for more details.\\n\\ntate : int, RandomState instance or None, default=None\\nrols both the randomness of the bootstrapping of the samples used\\n building trees (if ``bootstrap=True``) and the sampling of the\\nures to consider when looking for the best split at each node\\n``max_features < n_features``).\\n:term:`Glossary <random_state>` for details.\\n\\n: int, default=0\\nrols the verbosity when fitting and predicting.\\n\\nrt : bool, default=False\\n set to ``True``, reuse the solution of the previous call to fit\\nadd more estimators to the ensemble, otherwise, just fit a whole\\nforest. See :term:`the Glossary <warm_start>`.\\n\\night : {\\\"balanced\\\", \\\"balanced_subsample\\\"}, dict or list of dicts,             default=None\\nhts associated with classes in the form ``{class_label: weight}``.\\not given, all classes are supposed to have weight one. For\\ni-output problems, a list of dicts can be provided in the same\\nr as the columns of y.\\n\\n that for multioutput (including multilabel) weights should be\\nned for each class of every column in its own dict. For example,\\nfour-class multilabel classification weights should be\\n 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\\n1}, {2:5}, {3:1}, {4:1}].\\n\\n\\\"balanced\\\" mode uses the values of y to automatically adjust\\nhts inversely proportional to class frequencies in the input data\\n`n_samples / (n_classes * np.bincount(y))``\\n\\n\\\"balanced_subsample\\\" mode is the same as \\\"balanced\\\" except that\\nhts are computed based on the bootstrap sample for every tree\\nn.\\n\\nmulti-output, the weights of each column of y will be multiplied.\\n\\n that these weights will be multiplied with sample_weight (passed\\nugh the fit method) if sample_weight is specified.\\n\\na : non-negative float, default=0.0\\nlexity parameter used for Minimal Cost-Complexity Pruning. The\\nree with the largest cost complexity that is smaller than\\np_alpha`` will be chosen. By default, no pruning is performed. See\\n:`minimal_cost_complexity_pruning` for details.\\n\\nersionadded:: 0.22\\n\\nles : int or float, default=None\\nootstrap is True, the number of samples to draw from X\\nrain each base estimator.\\n\\n None (default), then draw `X.shape[0]` samples.\\n int, then draw `max_samples` samples.\\n float, then draw `max_samples * X.shape[0]` samples. Thus,\\nax_samples` should be in the interval `(0.0, 1.0]`.\\n\\nersionadded:: 0.22\\n\\nes\\n--\\nimator_ : DecisionTreeClassifier\\nchild estimator template used to create the collection of fitted\\nestimators.\\n\\nrs_ : list of DecisionTreeClassifier\\ncollection of fitted sub-estimators.\\n\\n : ndarray of shape (n_classes,) or a list of such arrays\\nclasses labels (single output problem), or a list of arrays of\\ns labels (multi-output problem).\\n\\ns_ : int or list\\nnumber of classes (single output problem), or a list containing the\\ner of classes for each output (multi-output problem).\\n\\nes_ : int\\nnumber of features when ``fit`` is performed.\\n\\neprecated:: 1.0\\nAttribute `n_features_` was deprecated in version 1.0 and will be\\nremoved in 1.2. Use `n_features_in_` instead.\\n\\nes_in_ : int\\ner of features seen during :term:`fit`.\\n\\nersionadded:: 0.24\\n\\nnames_in_ : ndarray of shape (`n_features_in_`,)\\ns of features seen during :term:`fit`. Defined only when `X`\\nfeature names that are all strings.\\n\\nersionadded:: 1.0\\n\\ns_ : int\\nnumber of outputs when ``fit`` is performed.\\n\\nimportances_ : ndarray of shape (n_features,)\\nimpurity-based feature importances.\\nhigher, the more important the feature.\\nimportance of a feature is computed as the (normalized)\\nl reduction of the criterion brought by that feature.  It is also\\nn as the Gini importance.\\n\\ning: impurity-based feature importances can be misleading for\\n cardinality features (many unique values). See\\nc:`sklearn.inspection.permutation_importance` as an alternative.\\n\\ne_ : float\\ne of the training dataset obtained using an out-of-bag estimate.\\n attribute exists only when ``oob_score`` is True.\\n\\nsion_function_ : ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)\\nsion function computed with out-of-bag estimate on the training\\n If n_estimators is small it might be possible that a data point\\nnever left out during the bootstrap. In this case,\\n_decision_function_` might contain NaN. This attribute exists\\n when ``oob_score`` is True.\\n\\n\\n\\ntree.DecisionTreeClassifier : A decision tree classifier.\\nensemble.ExtraTreesClassifier : Ensemble of extremely randomized\\n classifiers.\\n\\n\\n\\nult values for the parameters controlling the size of the trees\\nmax_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\\n trees which can potentially be very large on some data sets. To\\nemory consumption, the complexity and size of the trees should be\\ned by setting those parameter values.\\n\\nures are always randomly permuted at each split. Therefore,\\n found split may vary, even with the same training data,\\natures=n_features`` and ``bootstrap=False``, if the improvement\\nriterion is identical for several splits enumerated during the\\nf the best split. To obtain a deterministic behaviour during\\n ``random_state`` has to be fixed.\\n\\nes\\n--\\n. Breiman, \\\"Random Forests\\\", Machine Learning, 45(1), 5-32, 2001.\\n\\n\\n\\n sklearn.ensemble import RandomForestClassifier\\n sklearn.datasets import make_classification\\n = make_classification(n_samples=1000, n_features=4,\\n                       n_informative=2, n_redundant=0,\\n                       random_state=0, shuffle=False)\\n= RandomForestClassifier(max_depth=2, random_state=0)\\nfit(X, y)\\nrestClassifier(...)\\nt(clf.predict([[0, 0, 0, 0]]))\\n\\n```\\n\"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.classification/ridge-classifier\"} \":sklearn.classification/ridge-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [10 2]:\\n\\n|          :name |   :default |\\n|----------------|------------|\\n|     :normalize | deprecated |\\n|      :positive |      false |\\n|           :tol |   0.001000 |\\n|        :solver |       auto |\\n|      :max-iter |            |\\n|  :random-state |            |\\n|        :copy-x |       true |\\n| :fit-intercept |       true |\\n|         :alpha |      1.000 |\\n|  :class-weight |            |\\n\"]]] [:span [:p/markdown \"Classifier using Ridge regression.\\n\\n    This classifier first converts the target values into ``{-1, 1}`` and\\n    then treats the problem as a regression task (multi-output regression in\\n    the multiclass case).\\n\\n    Read more in the User Guide: `ridge_regression`.\\n\\n    Parameters\\n    ----------\\n    - `alpha`: float, default=1.0\\n        Regularization strength; must be a positive float. Regularization\\n        improves the conditioning of the problem and reduces the variance of\\n        the estimates. Larger values specify stronger regularization.\\n        Alpha corresponds to ``1 / (2C)`` in other linear models such as\\n        `~sklearn.linear_model.LogisticRegression` or\\n        `~sklearn.svm.LinearSVC`.\\n\\n    - `fit_intercept`: bool, default=True\\n        Whether to calculate the intercept for this model. If set to false, no\\n        intercept will be used in calculations (e.g. data is expected to be\\n        already centered).\\n\\n    - `normalize`: bool, default=False\\n        This parameter is ignored when ``fit_intercept`` is set to False.\\n        If True, the regressors X will be normalized before regression by\\n        subtracting the mean and dividing by the l2-norm.\\n        If you wish to standardize, please use\\n        `~sklearn.preprocessing.StandardScaler` before calling ``fit``\\n        on an estimator with ``normalize=False``.\\n\\n        *Deprecated since 1.0*\\n            ``normalize`` was deprecated in version 1.0 and\\n            will be removed in 1.2.\\n\\n    - `copy_X`: bool, default=True\\n        If True, X will be copied; else, it may be overwritten.\\n\\n    - `max_iter`: int, default=None\\n        Maximum number of iterations for conjugate gradient solver.\\n        The default value is determined by scipy.sparse.linalg.\\n\\n    - `tol`: float, default=1e-3\\n        Precision of the solution.\\n\\n    - `class_weight`: dict or 'balanced', default=None\\n        Weights associated with classes in the form ``{class_label: weight}``.\\n        If not given, all classes are supposed to have weight one.\\n\\n        The \\\"balanced\\\" mode uses the values of y to automatically adjust\\n        weights inversely proportional to class frequencies in the input data\\n        as ``n_samples / (n_classes * np.bincount(y))``.\\n\\n    - `solver`: {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg',             'sag', 'saga', 'lbfgs'}, default='auto'\\n        Solver to use in the computational routines:\\n\\n        - 'auto' chooses the solver automatically based on the type of data.\\n\\n        - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\\n          coefficients. It is the most stable solver, in particular more stable\\n          for singular matrices than 'cholesky' at the cost of being slower.\\n\\n        - 'cholesky' uses the standard scipy.linalg.solve function to\\n          obtain a closed-form solution.\\n\\n        - 'sparse_cg' uses the conjugate gradient solver as found in\\n          scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\\n          more appropriate than 'cholesky' for large-scale data\\n          (possibility to set `tol` and `max_iter`).\\n\\n        - 'lsqr' uses the dedicated regularized least-squares routine\\n          scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\\n          procedure.\\n\\n        - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\\n          its unbiased and more flexible version named SAGA. Both methods\\n          use an iterative procedure, and are often faster than other solvers\\n          when both n_samples and n_features are large. Note that 'sag' and\\n          'saga' fast convergence is only guaranteed on features with\\n          approximately the same scale. You can preprocess the data with a\\n          scaler from sklearn.preprocessing.\\n\\n          *Added in 0.17*\\n             Stochastic Average Gradient descent solver.\\n          *Added in 0.19*\\n             SAGA solver.\\n\\n        - 'lbfgs' uses L-BFGS-B algorithm implemented in\\n          `scipy.optimize.minimize`. It can be used only when `positive`\\n          is True.\\n\\n    - `positive`: bool, default=False\\n        When set to ``True``, forces the coefficients to be positive.\\n        Only 'lbfgs' solver is supported in this case.\\n\\n    - `random_state`: int, RandomState instance, default=None\\n        Used when ``solver`` == 'sag' or 'saga' to shuffle the data.\\n        See `Glossary <random_state>` for details.\\n\\n    Attributes\\n    ----------\\n    - `coef_`: ndarray of shape (1, n_features) or (n_classes, n_features)\\n        Coefficient of the features in the decision function.\\n\\n        ``coef_`` is of shape (1, n_features) when the given problem is binary.\\n\\n    - `intercept_`: float or ndarray of shape (n_targets,)\\n        Independent term in decision function. Set to 0.0 if\\n        ``fit_intercept = False``.\\n\\n    - `n_iter_`: None or ndarray of shape (n_targets,)\\n        Actual number of iterations for each target. Available only for\\n        sag and lsqr solvers. Other solvers will return None.\\n\\n    - `classes_`: ndarray of shape (n_classes,)\\n        The classes labels.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    See Also\\n    --------\\n    - `Ridge`: Ridge regression.\\n    - `RidgeClassifierCV`:  Ridge classifier with built-in cross validation.\\n\\n    Notes\\n    -----\\n    For multi-class classification, n_class classifiers are trained in\\n    a one-versus-all approach. Concretely, this is implemented by taking\\n    advantage of the multi-variate response support in Ridge.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.datasets import load_breast_cancer\\n    >>> from sklearn.linear_model import RidgeClassifier\\n    >>> X, y = load_breast_cancer(return_X_y=True)\\n    >>> clf = RidgeClassifier().fit(X, y)\\n    >>> clf.score(X, y)\\n    0.9595...\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.classification/ridge-classifier-cv\"} \":sklearn.classification/ridge-classifier-cv\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [7 2]:\\n\\n|            :name |       :default |\\n|------------------|----------------|\\n|          :alphas | [0.1 1.0 10.0] |\\n|    :class-weight |                |\\n|              :cv |                |\\n|   :fit-intercept |           true |\\n|       :normalize |     deprecated |\\n|         :scoring |                |\\n| :store-cv-values |          false |\\n\"]]] [:span [:p/markdown \"Ridge classifier with built-in cross-validation.\\n\\n    See glossary entry for `cross-validation estimator`.\\n\\n    By default, it performs Leave-One-Out Cross-Validation. Currently,\\n    only the n_features > n_samples case is handled efficiently.\\n\\n    Read more in the User Guide: `ridge_regression`.\\n\\n    Parameters\\n    ----------\\n    - `alphas`: ndarray of shape (n_alphas,), default=(0.1, 1.0, 10.0)\\n        Array of alpha values to try.\\n        Regularization strength; must be a positive float. Regularization\\n        improves the conditioning of the problem and reduces the variance of\\n        the estimates. Larger values specify stronger regularization.\\n        Alpha corresponds to ``1 / (2C)`` in other linear models such as\\n        `~sklearn.linear_model.LogisticRegression` or\\n        `~sklearn.svm.LinearSVC`.\\n\\n    - `fit_intercept`: bool, default=True\\n        Whether to calculate the intercept for this model. If set\\n        to false, no intercept will be used in calculations\\n        (i.e. data is expected to be centered).\\n\\n    - `normalize`: bool, default=False\\n        This parameter is ignored when ``fit_intercept`` is set to False.\\n        If True, the regressors X will be normalized before regression by\\n        subtracting the mean and dividing by the l2-norm.\\n        If you wish to standardize, please use\\n        `~sklearn.preprocessing.StandardScaler` before calling ``fit``\\n        on an estimator with ``normalize=False``.\\n\\n        *Deprecated since 1.0*\\n            ``normalize`` was deprecated in version 1.0 and\\n            will be removed in 1.2.\\n\\n    - `scoring`: str, callable, default=None\\n        A string (see model evaluation documentation) or\\n        a scorer callable object / function with signature\\n        ``scorer(estimator, X, y)``.\\n\\n    - `cv`: int, cross-validation generator or an iterable, default=None\\n        Determines the cross-validation splitting strategy.\\n        Possible inputs for cv are:\\n\\n        - None, to use the efficient Leave-One-Out cross-validation\\n        - integer, to specify the number of folds.\\n        - `CV splitter`,\\n        - An iterable yielding (train, test) splits as arrays of indices.\\n\\n        Refer User Guide: `cross_validation` for the various\\n        cross-validation strategies that can be used here.\\n\\n    - `class_weight`: dict or 'balanced', default=None\\n        Weights associated with classes in the form ``{class_label: weight}``.\\n        If not given, all classes are supposed to have weight one.\\n\\n        The \\\"balanced\\\" mode uses the values of y to automatically adjust\\n        weights inversely proportional to class frequencies in the input data\\n        as ``n_samples / (n_classes * np.bincount(y))``.\\n\\n    - `store_cv_values`: bool, default=False\\n        Flag indicating if the cross-validation values corresponding to\\n        each alpha should be stored in the ``cv_values_`` attribute (see\\n        below). This flag is only compatible with ``cv=None`` (i.e. using\\n        Leave-One-Out Cross-Validation).\\n\\n    Attributes\\n    ----------\\n    - `cv_values_`: ndarray of shape (n_samples, n_targets, n_alphas), optional\\n        Cross-validation values for each alpha (only if ``store_cv_values=True`` and\\n        ``cv=None``). After ``fit()`` has been called, this attribute will\\n        contain the mean squared errors if `scoring is None` otherwise it\\n        will contain standardized per point prediction values.\\n\\n    - `coef_`: ndarray of shape (1, n_features) or (n_targets, n_features)\\n        Coefficient of the features in the decision function.\\n\\n        ``coef_`` is of shape (1, n_features) when the given problem is binary.\\n\\n    - `intercept_`: float or ndarray of shape (n_targets,)\\n        Independent term in decision function. Set to 0.0 if\\n        ``fit_intercept = False``.\\n\\n    - `alpha_`: float\\n        Estimated regularization parameter.\\n\\n    - `best_score_`: float\\n        Score of base estimator with best alpha.\\n\\n        *Added in 0.23*\\n\\n    - `classes_`: ndarray of shape (n_classes,)\\n        The classes labels.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    See Also\\n    --------\\n    - `Ridge`: Ridge regression.\\n    - `RidgeClassifier`: Ridge classifier.\\n    - `RidgeCV`: Ridge regression with built-in cross validation.\\n\\n    Notes\\n    -----\\n    For multi-class classification, n_class classifiers are trained in\\n    a one-versus-all approach. Concretely, this is implemented by taking\\n    advantage of the multi-variate response support in Ridge.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.datasets import load_breast_cancer\\n    >>> from sklearn.linear_model import RidgeClassifierCV\\n    >>> X, y = load_breast_cancer(return_X_y=True)\\n    >>> clf = RidgeClassifierCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\\n    >>> clf.score(X, y)\\n    0.9630...\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.classification/sgd-classifier\"} \":sklearn.classification/sgd-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [21 2]:\\n\\n|                :name |  :default |\\n|----------------------|-----------|\\n|    :n-iter-no-change |         5 |\\n|       :learning-rate |   optimal |\\n|             :average |     false |\\n|                 :tol |  0.001000 |\\n|      :early-stopping |     false |\\n|               :eta-0 |     0.000 |\\n|             :shuffle |      true |\\n|             :penalty |        l2 |\\n|             :power-t |    0.5000 |\\n|            :max-iter |      1000 |\\n|                  ... |       ... |\\n|              :n-jobs |           |\\n|        :random-state |           |\\n|       :fit-intercept |      true |\\n|               :alpha | 0.0001000 |\\n|          :warm-start |     false |\\n|           :l-1-ratio |    0.1500 |\\n| :validation-fraction |    0.1000 |\\n|        :class-weight |           |\\n|                :loss |     hinge |\\n|             :verbose |         0 |\\n|             :epsilon |    0.1000 |\\n\"]]] [:span [:p/markdown \"Linear classifiers (SVM, logistic regression, etc.) with SGD training.\\n\\n    This estimator implements regularized linear models with stochastic\\n    gradient descent (SGD) learning: the gradient of the loss is estimated\\n    each sample at a time and the model is updated along the way with a\\n    decreasing strength schedule (aka learning rate). SGD allows minibatch\\n    (online/out-of-core) learning via the `partial_fit` method.\\n    For best results using the default learning rate schedule, the data should\\n    have zero mean and unit variance.\\n\\n    This implementation works with data represented as dense or sparse arrays\\n    of floating point values for the features. The model it fits can be\\n    controlled with the loss parameter; by default, it fits a linear support\\n    vector machine (SVM).\\n\\n    The regularizer is a penalty added to the loss function that shrinks model\\n    parameters towards the zero vector using either the squared euclidean norm\\n    L2 or the absolute norm L1 or a combination of both (Elastic Net). If the\\n    parameter update crosses the 0.0 value because of the regularizer, the\\n    update is truncated to 0.0 to allow for learning sparse models and achieve\\n    online feature selection.\\n\\n    Read more in the User Guide: `sgd`.\\n\\n    Parameters\\n    ----------\\n    - `loss`: {'hinge', 'log_loss', 'log', 'modified_huber', 'squared_hinge',        'perceptron', 'squared_error', 'huber', 'epsilon_insensitive',        'squared_epsilon_insensitive'}, default='hinge'\\n        The loss function to be used.\\n\\n        - 'hinge' gives a linear SVM.\\n        - 'log_loss' gives logistic regression, a probabilistic classifier.\\n        - 'modified_huber' is another smooth loss that brings tolerance to\\n           outliers as well as probability estimates.\\n        - 'squared_hinge' is like hinge but is quadratically penalized.\\n        - 'perceptron' is the linear loss used by the perceptron algorithm.\\n        - The other losses, 'squared_error', 'huber', 'epsilon_insensitive' and\\n          'squared_epsilon_insensitive' are designed for regression but can be useful\\n          in classification as well; see\\n          `~sklearn.linear_model.SGDRegressor` for a description.\\n\\n        More details about the losses formulas can be found in the\\n        User Guide: `sgd_mathematical_formulation`.\\n\\n        *Deprecated since 1.0*\\n            The loss 'squared_loss' was deprecated in v1.0 and will be removed\\n            in version 1.2. Use `loss='squared_error'` which is equivalent.\\n\\n        *Deprecated since 1.1*\\n            The loss 'log' was deprecated in v1.1 and will be removed\\n            in version 1.3. Use `loss='log_loss'` which is equivalent.\\n\\n    - `penalty`: {'l2', 'l1', 'elasticnet'}, default='l2'\\n        The penalty (aka regularization term) to be used. Defaults to 'l2'\\n        which is the standard regularizer for linear SVM models. 'l1' and\\n        'elasticnet' might bring sparsity to the model (feature selection)\\n        not achievable with 'l2'.\\n\\n    - `alpha`: float, default=0.0001\\n        Constant that multiplies the regularization term. The higher the\\n        value, the stronger the regularization.\\n        Also used to compute the learning rate when set to `learning_rate` is\\n        set to 'optimal'.\\n        Values must be in the range `[0.0, inf)`.\\n\\n    - `l1_ratio`: float, default=0.15\\n        The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\\n        l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\\n        Only used if `penalty` is 'elasticnet'.\\n        Values must be in the range `[0.0, 1.0]`.\\n\\n    - `fit_intercept`: bool, default=True\\n        Whether the intercept should be estimated or not. If False, the\\n        data is assumed to be already centered.\\n\\n    - `max_iter`: int, default=1000\\n        The maximum number of passes over the training data (aka epochs).\\n        It only impacts the behavior in the ``fit`` method, and not the\\n        `partial_fit` method.\\n        Values must be in the range `[1, inf)`.\\n\\n        *Added in 0.19*\\n\\n    - `tol`: float, default=1e-3\\n        The stopping criterion. If it is not None, training will stop\\n        when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive\\n        epochs.\\n        Convergence is checked against the training loss or the\\n        validation loss depending on the `early_stopping` parameter.\\n        Values must be in the range `[0.0, inf)`.\\n\\n        *Added in 0.19*\\n\\n    - `shuffle`: bool, default=True\\n        Whether or not the training data should be shuffled after each epoch.\\n\\n    - `verbose`: int, default=0\\n        The verbosity level.\\n        Values must be in the range `[0, inf)`.\\n\\n    - `epsilon`: float, default=0.1\\n        Epsilon in the epsilon-insensitive loss functions; only if `loss` is\\n        'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\\n        For 'huber', determines the threshold at which it becomes less\\n        important to get the prediction exactly right.\\n        For epsilon-insensitive, any differences between the current prediction\\n        and the correct label are ignored if they are less than this threshold.\\n        Values must be in the range `[0.0, inf)`.\\n\\n    - `n_jobs`: int, default=None\\n        The number of CPUs to use to do the OVA (One Versus All, for\\n        multi-class problems) computation.\\n        ``None`` means 1 unless in a `joblib.parallel_backend` context.\\n        ``-1`` means using all processors. See `Glossary <n_jobs>`\\n        for more details.\\n\\n    - `random_state`: int, RandomState instance, default=None\\n        Used for shuffling the data, when ``shuffle`` is set to ``True``.\\n        Pass an int for reproducible output across multiple function calls.\\n        See `Glossary <random_state>`.\\n        Integer values must be in the range `[0, 2**32 - 1]`.\\n\\n    - `learning_rate`: str, default='optimal'\\n        The learning rate schedule:\\n\\n        - 'constant': `eta = eta0`\\n        - 'optimal': `eta = 1.0 / (alpha * (t + t0))`\\n          where `t0` is chosen by a heuristic proposed by Leon Bottou.\\n        - 'invscaling': `eta = eta0 / pow(t, power_t)`\\n        - 'adaptive': `eta = eta0`, as long as the training keeps decreasing.\\n          Each time n_iter_no_change consecutive epochs fail to decrease the\\n          training loss by tol or fail to increase validation score by tol if\\n          `early_stopping` is `True`, the current learning rate is divided by 5.\\n\\n            *Added in 0.20*\\n                Added 'adaptive' option\\n\\n    - `eta0`: float, default=0.0\\n        The initial learning rate for the 'constant', 'invscaling' or\\n        'adaptive' schedules. The default value is 0.0 as eta0 is not used by\\n        the default schedule 'optimal'.\\n        Values must be in the range `(0.0, inf)`.\\n\\n    - `power_t`: float, default=0.5\\n        The exponent for inverse scaling learning rate [default 0.5].\\n        Values must be in the range `(-inf, inf)`.\\n\\n    - `early_stopping`: bool, default=False\\n        Whether to use early stopping to terminate training when validation\\n        score is not improving. If set to `True`, it will automatically set aside\\n        a stratified fraction of training data as validation and terminate\\n        training when validation score returned by the `score` method is not\\n        improving by at least tol for n_iter_no_change consecutive epochs.\\n\\n        *Added in 0.20*\\n            Added 'early_stopping' option\\n\\n    - `validation_fraction`: float, default=0.1\\n        The proportion of training data to set aside as validation set for\\n        early stopping. Must be between 0 and 1.\\n        Only used if `early_stopping` is True.\\n        Values must be in the range `(0.0, 1.0)`.\\n\\n        *Added in 0.20*\\n            Added 'validation_fraction' option\\n\\n    - `n_iter_no_change`: int, default=5\\n        Number of iterations with no improvement to wait before stopping\\n        fitting.\\n        Convergence is checked against the training loss or the\\n        validation loss depending on the `early_stopping` parameter.\\n        Integer values must be in the range `[1, max_iter)`.\\n\\n        *Added in 0.20*\\n            Added 'n_iter_no_change' option\\n\\n    - `class_weight`: dict, {class_label: weight} or \\\"balanced\\\", default=None\\n        Preset for the class_weight fit parameter.\\n\\n        Weights associated with classes. If not given, all classes\\n        are supposed to have weight one.\\n\\n        The \\\"balanced\\\" mode uses the values of y to automatically adjust\\n        weights inversely proportional to class frequencies in the input data\\n        as ``n_samples / (n_classes * np.bincount(y))``.\\n\\n    - `warm_start`: bool, default=False\\n        When set to True, reuse the solution of the previous call to fit as\\n        initialization, otherwise, just erase the previous solution.\\n        See `the Glossary <warm_start>`.\\n\\n        Repeatedly calling fit or partial_fit when warm_start is True can\\n        result in a different solution than when calling fit a single time\\n        because of the way the data is shuffled.\\n        If a dynamic learning rate is used, the learning rate is adapted\\n        depending on the number of samples already seen. Calling ``fit`` resets\\n        this counter, while ``partial_fit`` will result in increasing the\\n        existing counter.\\n\\n    - `average`: bool or int, default=False\\n        When set to `True`, computes the averaged SGD weights across all\\n        updates and stores the result in the ``coef_`` attribute. If set to\\n        an int greater than 1, averaging will begin once the total number of\\n        samples seen reaches `average`. So ``average=10`` will begin\\n        averaging after seeing 10 samples.\\n        Integer values must be in the range `[1, n_samples]`.\\n\\n    Attributes\\n    ----------\\n    - `coef_`: ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)\\n        Weights assigned to the features.\\n\\n    - `intercept_`: ndarray of shape (1,) if n_classes == 2 else (n_classes,)\\n        Constants in decision function.\\n\\n    - `n_iter_`: int\\n        The actual number of iterations before reaching the stopping criterion.\\n        For multiclass fits, it is the maximum over every binary fit.\\n\\n    - `loss_function_`: concrete ``LossFunction``\\n\\n    - `classes_`: array of shape (n_classes,)\\n\\n    - `t_`: int\\n        Number of weight updates performed during training.\\n        Same as ``(n_iter_ * n_samples)``.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    See Also\\n    --------\\n    - `sklearn.svm.LinearSVC`: Linear support vector classification.\\n    - `LogisticRegression`: Logistic regression.\\n    - `Perceptron`: Inherits from SGDClassifier. ``Perceptron()`` is equivalent to\\n        ``SGDClassifier(loss=\\\"perceptron\\\", eta0=1, learning_rate=\\\"constant\\\",\\n        penalty=None)``.\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> from sklearn.linear_model import SGDClassifier\\n    >>> from sklearn.preprocessing import StandardScaler\\n    >>> from sklearn.pipeline import make_pipeline\\n    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\\n    >>> Y = np.array([1, 1, 2, 2])\\n    >>> # Always scale the input. The most convenient way is to use a pipeline.\\n    >>> clf = make_pipeline(StandardScaler(),\\n    ...                     SGDClassifier(max_iter=1000, tol=1e-3))\\n    >>> clf.fit(X, Y)\\n    Pipeline(steps=[('standardscaler', StandardScaler()),\\n                    ('sgdclassifier', SGDClassifier())])\\n    >>> print(clf.predict([[-0.8, -1]]))\\n    [1]\\n    \"]] [:hr] [:hr]] [:div [:h3 {:id \":sklearn.classification/svc\"} \":sklearn.classification/svc\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\"} [:p/markdown \"_unnamed [15 2]:\\n\\n|                    :name | :default |\\n|--------------------------|----------|\\n|              :break-ties |    false |\\n|                  :kernel |      rbf |\\n|                   :gamma |    scale |\\n|                  :degree |        3 |\\n| :decision-function-shape |      ovr |\\n|             :probability |    false |\\n|                     :tol | 0.001000 |\\n|               :shrinking |     true |\\n|                       :c |    1.000 |\\n|                :max-iter |       -1 |\\n|            :random-state |          |\\n|                  :coef-0 |    0.000 |\\n|            :class-weight |          |\\n|              :cache-size |      200 |\\n|                 :verbose |    false |\\n\"]]] [:span [:p/markdown \"C-Support Vector Classification.\\n\\n    The implementation is based on libsvm. The fit time scales at least\\n    quadratically with the number of samples and may be impractical\\n    beyond tens of thousands of samples. For large datasets\\n    consider using `~sklearn.svm.LinearSVC` or\\n    `~sklearn.linear_model.SGDClassifier` instead, possibly after a\\n    `~sklearn.kernel_approximation.Nystroem` transformer.\\n\\n    The multiclass support is handled according to a one-vs-one scheme.\\n\\n    For details on the precise mathematical formulation of the provided\\n    kernel functions and how `gamma`, `coef0` and `degree` affect each\\n    other, see the corresponding section in the narrative documentation:\\n    :ref:`svm_kernels`.\\n\\n    Read more in the User Guide: `svm_classification`.\\n\\n    Parameters\\n    ----------\\n    - `C`: float, default=1.0\\n        Regularization parameter. The strength of the regularization is\\n        inversely proportional to C. Must be strictly positive. The penalty\\n        is a squared l2 penalty.\\n\\n    - `kernel`: {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'} or callable,          default='rbf'\\n        Specifies the kernel type to be used in the algorithm.\\n        If none is given, 'rbf' will be used. If a callable is given it is\\n        used to pre-compute the kernel matrix from data matrices; that matrix\\n        should be an array of shape ``(n_samples, n_samples)``.\\n\\n    - `degree`: int, default=3\\n        Degree of the polynomial kernel function ('poly').\\n        Ignored by all other kernels.\\n\\n    - `gamma`: {'scale', 'auto'} or float, default='scale'\\n        Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\\n\\n        - if ``gamma='scale'`` (default) is passed then it uses\\n          1 / (n_features * X.var()) as value of gamma,\\n        - if 'auto', uses 1 / n_features.\\n\\n        *Changed in 0.22*\\n           The default value of ``gamma`` changed from 'auto' to 'scale'.\\n\\n    - `coef0`: float, default=0.0\\n        Independent term in kernel function.\\n        It is only significant in 'poly' and 'sigmoid'.\\n\\n    - `shrinking`: bool, default=True\\n        Whether to use the shrinking heuristic.\\n        See the User Guide: `shrinking_svm`.\\n\\n    - `probability`: bool, default=False\\n        Whether to enable probability estimates. This must be enabled prior\\n        to calling `fit`, will slow down that method as it internally uses\\n        5-fold cross-validation, and `predict_proba` may be inconsistent with\\n        `predict`. Read more in the User Guide: `scores_probabilities`.\\n\\n    - `tol`: float, default=1e-3\\n        Tolerance for stopping criterion.\\n\\n    - `cache_size`: float, default=200\\n        Specify the size of the kernel cache (in MB).\\n\\n    - `class_weight`: dict or 'balanced', default=None\\n        Set the parameter C of class i to class_weight[i]*C for\\n        SVC. If not given, all classes are supposed to have\\n        weight one.\\n        The \\\"balanced\\\" mode uses the values of y to automatically adjust\\n        weights inversely proportional to class frequencies in the input data\\n        as ``n_samples / (n_classes * np.bincount(y))``.\\n\\n    - `verbose`: bool, default=False\\n        Enable verbose output. Note that this setting takes advantage of a\\n        per-process runtime setting in libsvm that, if enabled, may not work\\n        properly in a multithreaded context.\\n\\n    - `max_iter`: int, default=-1\\n        Hard limit on iterations within solver, or -1 for no limit.\\n\\n    - `decision_function_shape`: {'ovo', 'ovr'}, default='ovr'\\n        Whether to return a one-vs-rest ('ovr') decision function of shape\\n        (n_samples, n_classes) as all other classifiers, or the original\\n        one-vs-one ('ovo') decision function of libsvm which has shape\\n        (n_samples, n_classes * (n_classes - 1) / 2). However, note that\\n        internally, one-vs-one ('ovo') is always used as a multi-class strategy\\n        to train models; an ovr matrix is only constructed from the ovo matrix.\\n        The parameter is ignored for binary classification.\\n\\n        *Changed in 0.19*\\n            decision_function_shape is 'ovr' by default.\\n\\n        *Added in 0.17*\\n           *decision_function_shape='ovr'* is recommended.\\n\\n        *Changed in 0.17*\\n           Deprecated *decision_function_shape='ovo' and None*.\\n\\n    - `break_ties`: bool, default=False\\n        If true, ``decision_function_shape='ovr'``, and number of classes > 2,\\n        `predict` will break ties according to the confidence values of\\n        `decision_function`; otherwise the first class among the tied\\n        classes is returned. Please note that breaking ties comes at a\\n        relatively high computational cost compared to a simple predict.\\n\\n        *Added in 0.22*\\n\\n    - `random_state`: int, RandomState instance or None, default=None\\n        Controls the pseudo random number generation for shuffling the data for\\n        probability estimates. Ignored when `probability` is False.\\n        Pass an int for reproducible output across multiple function calls.\\n        See `Glossary <random_state>`.\\n\\n    Attributes\\n    ----------\\n    - `class_weight_`: ndarray of shape (n_classes,)\\n        Multipliers of parameter C for each class.\\n        Computed based on the ``class_weight`` parameter.\\n\\n    - `classes_`: ndarray of shape (n_classes,)\\n        The classes labels.\\n\\n    - `coef_`: ndarray of shape (n_classes * (n_classes - 1) / 2, n_features)\\n        Weights assigned to the features (coefficients in the primal\\n        problem). This is only available in the case of a linear kernel.\\n\\n        `coef_` is a readonly property derived from `dual_coef_` and\\n        `support_vectors_`.\\n\\n    - `dual_coef_`: ndarray of shape (n_classes -1, n_SV)\\n        Dual coefficients of the support vector in the decision\\n        function (see :ref:`sgd_mathematical_formulation`), multiplied by\\n        their targets.\\n        For multiclass, coefficient for all 1-vs-1 classifiers.\\n        The layout of the coefficients in the multiclass case is somewhat\\n        non-trivial. See the multi-class section of the User Guide: `svm_multi_class` for details.\\n\\n    - `fit_status_`: int\\n        0 if correctly fitted, 1 otherwise (will raise warning)\\n\\n    - `intercept_`: ndarray of shape (n_classes * (n_classes - 1) / 2,)\\n        Constants in decision function.\\n\\n    - `n_features_in_`: int\\n        Number of features seen during `fit`.\\n\\n        *Added in 0.24*\\n\\n    - `feature_names_in_`: ndarray of shape (`n_features_in_`,)\\n        Names of features seen during `fit`. Defined only when `X`\\n        has feature names that are all strings.\\n\\n        *Added in 1.0*\\n\\n    - `n_iter_`: ndarray of shape (n_classes * (n_classes - 1) // 2,)\\n        Number of iterations run by the optimization routine to fit the model.\\n        The shape of this attribute depends on the number of models optimized\\n        which in turn depends on the number of classes.\\n\\n        *Added in 1.1*\\n\\n    - `support_`: ndarray of shape (n_SV)\\n        Indices of support vectors.\\n\\n    - `support_vectors_`: ndarray of shape (n_SV, n_features)\\n        Support vectors.\\n\\n    - `n_support_`: ndarray of shape (n_classes,), dtype=int32\\n        Number of support vectors for each class.\\n\\n    - `probA_`: ndarray of shape (n_classes * (n_classes - 1) / 2)\\n    - `probB_`: ndarray of shape (n_classes * (n_classes - 1) / 2)\\n        If `probability=True`, it corresponds to the parameters learned in\\n        Platt scaling to produce probability estimates from decision values.\\n        If `probability=False`, it's an empty array. Platt scaling uses the\\n        logistic function\\n        ``1 / (1 + exp(decision_value * probA_ + probB_))``\\n        where ``probA_`` and ``probB_`` are learned from the dataset [2]_. For\\n        more information on the multiclass case and training procedure see\\n        section 8 of [1]_.\\n\\n    - `shape_fit_`: tuple of int of shape (n_dimensions_of_X,)\\n        Array dimensions of training vector ``X``.\\n\\n    See Also\\n    --------\\n    - `SVR`: Support Vector Machine for Regression implemented using libsvm.\\n\\n    - `LinearSVC`: Scalable Linear Support Vector Machine for classification\\n        implemented using liblinear. Check the See Also section of\\n        LinearSVC for more comparison element.\\n\\n    References\\n    ----------\\n - [1] [LIBSVM: A Library for Support Vector Machines\\n        ](http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf)\\n\\n - [2] [Platt, John (1999). \\\"Probabilistic outputs for support vector\\n        machines and comparison to regularizedlikelihood methods.\\\"\\n        ](http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639)\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> from sklearn.pipeline import make_pipeline\\n    >>> from sklearn.preprocessing import StandardScaler\\n    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\\n    >>> y = np.array([1, 1, 2, 2])\\n    >>> from sklearn.svm import SVC\\n    >>> clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\\n    >>> clf.fit(X, y)\\n    Pipeline(steps=[('standardscaler', StandardScaler()),\\n                    ('svc', SVC(gamma='auto'))])\\n\\n    >>> print(clf.predict([[-0.8, -1]]))\\n    [1]\\n    \"]] [:hr] [:hr]])]}}"</script>
    <script src="gorilla-notes/js/compiled/main.js"></script>
</html>
